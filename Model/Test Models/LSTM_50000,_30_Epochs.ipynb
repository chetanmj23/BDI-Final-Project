{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYGC82oqII7m"
   },
   "source": [
    "\n",
    "\n",
    "<center><font color=darkblue><h1>*Language Translation Using LSTM and GRU*</h1></font></center>\n",
    "***\n",
    "\n",
    "![Music](https://mondrian.mashable.com/2012%252F12%252F04%252F46%252F7languagetr.bTr.jpg%252F950x534__filters%253Aquality%252890%2529.jpg?signature=sBWpGjGRzxhm5G6BGJBqALyuLKY=)\n",
    "<div>\n",
    "<center><font color=darkblue>\n",
    "    <h3>A PROJECT BY:</h3></font>\n",
    "        <h2><a href='https://www.linkedin.com/in/chetanmjadhav/'> Chetan M Jadhav (001836501)</a></h2>\n",
    "    ![test116](https://upload.wikimedia.org/wikipedia/en/thumb/b/bd/Northeastern_University_seal.svg/586px-Northeastern_University_seal.svg.png)\n",
    "    <h3>Under the guidance of</h3></font>\n",
    "        <h2>Nick Bear Brown</h2>\n",
    "\n",
    " </center>\n",
    "</div>\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZi5NUKPII7n"
   },
   "source": [
    "<div>\n",
    "    <h2>&#10024; Table of Content: </h2>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Introduction](#Introduction) </h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Imports](#Imports)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Data Processing](#Data_Processing)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Tokenization](#Tokenization)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Build the Model](#Build_The_Model)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Train the Model](#Model_Train)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Prediction](#Prediction)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Evaluation](#Merging)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Conclusion](#Conclusion)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Contribution](#Contribution)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [References](#References)</h3>\n",
    "        <h3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#10163; [Licence](#Licence)</h3>\n",
    "</div>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyHAS_FBII7o"
   },
   "source": [
    "<h3><a id=\"Introduction\">&#9997; Introduction</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkxE5XpqII7p"
   },
   "source": [
    "Recurrent Neural Networks (RNN) have revealed promising outcomes in several machine learning tasks, specifically when input or output are of flexible length. More recently its been stated that recurrent neural networks can accomplish well in building the machine translation model.\n",
    "As we know that recurrent neural networks are doing well in machine translation field. We are more concerned in building a machine translation model using two closely related variants which are LSTM and GRU. These two variants of works well with seq2seq framework where the input is fed sequentially, and output is led out sequentially. The draw back which vanilla recurrent neural network had of vanishing gradient problem is resolved by using these two variants of RNN as these two can memorize more and can omit if the data is not needed in its memory and can work on longer sentences which was an issue using vanilla RNN.\n",
    "Based on our couple of experiments we can say that LSTM and GRU performs well in translating the language but GRU’s efficiency is little bit more compared to that of LSTM, but training time is much more than that of LSTM.\n",
    "\n",
    "**Recurrent Neural Network:**\n",
    "\n",
    "A recurrent neural network (RNN) is an extension of the conventional feedforward neural network, which can handle variable length sequence input. The core idea behind the RNN is to make use of sequential data. In traditional neural network we adopt that all the inputs are autonomous of each other. But many of tasks doesn’t work this way and the traditional neural network was of no use when the model must forecast what’s going to come next in the sentence as it needs to remember the previous couple of words, that’s when the Recurrent Neural Network came to existence. This is called recurrent because of the network achieves the same task for every element of a sequence, with output depending on the previous calculations. Further way of explaining the recurrent term is remembering the old computation in the memory.\n",
    "![test](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "Traditional Recurrent Neural Network\n",
    "\n",
    "The above picture shows an RNN being unfolded into a full network where the network will be unfolded the times of the input. For example, if there is 3 words input then the RNN will unfold 3 times to train the model and to provide the output i.e., one layer for each word.\n",
    "While using RNN, they came across the issue called vanishing gradients as the RNN is very old and this issue emerged as the major issue to recurrent neural network’s performance. For recurrent neural networks we want to have extended memories so that system can join data relationships at significance at times but as we had more time steps, the more chance we have back-propogation gradients either hoarding and exploding or vanishing down to nothing. \n",
    "The above obstacle of vanishing gradients is solved by the long short term memory and the gated recurrent unit, we will discuss about them in our next section.\n",
    "\n",
    "**Long Short Term Memory**\n",
    "\n",
    "Long short term memory was developed as the solution to the traditional recurrent neural network and this work and have internal gates that regulates the data flow to the next layer. These gates can learn which data in a sequence is important to keep or to forget, by this it can keep only the data which is relevant and which is helpful in predecting the next word. And about the LSTM gates we going to discuss in brief in this section so we have better understanding on the same.\n",
    " \n",
    "![test2](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/10131302/13-768x295.png)\n",
    "\n",
    "\n",
    "The typical LSTM network have different memory blocks which is termed by the name “Cells” and there are two states that are being shifted to the next cell called cell state and the hidden state. The memory blocks are accountable for remembering things and operations to this memory and is done through the mechanism called gates.\n",
    "\n",
    "**Forget Gate:**\n",
    "Forget gate is accountable for removing or throwing away the information that is not needed and this gate have two inputs ht-1 and xt, ht-1 is the hidden state from the previus cell and xt is the input of the current time step.\n",
    "\n",
    "![test3](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/10131319/14.png)\n",
    "\n",
    "Both the inputs ht-1 and xt are multipied before passing to the sigmoid function where the sigmoid function outputs vector of 0 to 1. Basically its sigmoid who decides whether to keep the data which is there in this gate or to throw away. For example, if the sigmoid output is 0 then it means the data which is passed to the gate needs to be forgotten and if the value of sigmoid is 1 then the information is passed to the next layer.\n",
    "\n",
    "**Input Gate:**\n",
    "\n",
    "The input gate is accountable for addition of the information to the cell state and the process is expained with the diagram below\n",
    "\n",
    "![test4](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/10131330/16.png)\n",
    "\n",
    "Input gate takes the two inputs hidden state from previous cell ht-1 and current input xt .\n",
    "Both the inputs multiplied and passed it to the sigmoid and tanh function before again passing it to the multiplier where tanh function outputs in the range of -1 to 1 this value passes the useful information to the cell state.\n",
    "\n",
    "\n",
    "**Output Gate:**\n",
    "\n",
    "The output gate is accountable for the selecting the useful information from the current cell state and showing it as an output to the next layer.\n",
    "\n",
    "![test5](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/10131340/18.png)\n",
    " \n",
    "\n",
    "Output gate  takes the two inputs hidden state from previous cell ht-1 and current input xt .\n",
    "Both the inputs multiplied and passed it to the sigmoid function and passing it to the tanh function where the tanh function sacles the output in the range of -1 to 1 and multiplying the value of this filet to the vector created by tanh function will be the oupput to the next layer which acts as ht-1 to the next layer.\n",
    "\n",
    "LSTM’s are very promising but its very difficult in training them and lot of computation is needed to train the model.\n",
    "\n",
    "**Gated Recurrent Unit:**\n",
    "\n",
    "Gated Recurrent Unit was introduced by the K.Cho in 2014 and this is almost similar to LSTM and can be called enhanced LSTM. As LSTM even GRU has the gate concept and has 2 gates instead of 3 which we will discuss in detail in this section. Below is the diagram of the GRU.\n",
    "\n",
    "![test6](https://cdn-images-1.medium.com/max/1400/1*6eNTqLzQ08AABo-STFNiBw.png)\n",
    "\n",
    "And the notations are as mentioned below which will help to understand the equation\n",
    "\n",
    "![test7](https://cdn-images-1.medium.com/max/800/1*qx5uUSVgL_QCvsJ_yM2pMA.png)\n",
    "\n",
    "**Update Gate:**\n",
    "\n",
    "The Update gate has two inputs hidden state from previous cell ht-1 and current input xt . and this gate is accountable to check the piece of information and decide if it needs to update the cell\n",
    "\n",
    "![test8](https://cdn-images-1.medium.com/max/800/1*gSlR_JLNeuZBSCAKyjmAdA.png)\n",
    " \n",
    "Both the inputs will be passed to the plus operation which again will be passed to the sigmoid function which vectors the output in the range of 0 to 1 and each inputs carries its own weight and can be defined by the below formula\n",
    "\n",
    "Zt = σ(W(z)xt + U(z)ht-1)\n",
    "\n",
    "The input xt is multiplied by its own weight W(z) and input ht-1 is mulitplied by its own weight U(z) .\n",
    "This update gate helps th ecell to determine how much of the information needs to be passed along and which information needs to be dropped and not to passed to the next layer.\n",
    "\n",
    "**Reset Gate:**\n",
    "\n",
    "Reset gate is accountable for deciding what piece information needs to be forgotten and reset gate also takes two inputs hidden state from previous cell ht-1 and current input xt .\n",
    " \n",
    "![test9](https://cdn-images-1.medium.com/max/800/1*5M6LYj544UKKHkFkDmDQ8A.png)\n",
    "\n",
    "Both the inputs multiplied with their own weight and added together before passing it to the sigmoid function which vectors the output in the range of 0 to 1 and can be defined by the below formula\n",
    "\n",
    " ![test10](https://cdn-images-1.medium.com/max/800/1*j1j1mLIyTm97hCay4GRC_Q.png)\n",
    " \n",
    " The output of the sigmoid function determines if the information is passed or forgotten. for example, if the output is 1 then the data is passed and if the output is 0 then the piece of information is dropped.\n",
    " \n",
    "**Current Memory Content:**\n",
    "\n",
    "By making use of the reset gate’s output which will store the relevant information from the past and can be calculated by the below mentioned formula\n",
    "\n",
    "![test11](https://cdn-images-1.medium.com/max/800/1*CxQBMqy8dvgJNjeJcur6pQ.png)\n",
    " \n",
    "The input xt is multiplied by its own weight W and ht-1 with its own weight U. and compute the Hadamard product between reset gate’s output rt and Uht-1 and add it with the input Wxt before passing it to the nonlinear tanh function which will vector the output in the range of -1 to 1\n",
    "\n",
    " \n",
    "![test12](https://cdn-images-1.medium.com/max/800/1*AZObvZ2GXSDYkJ2iv28MaQ.png)\n",
    "\n",
    "\n",
    "**Final memory at current time step:**\n",
    "\n",
    "In this step, at the end it will hold the information which we pass to the next layer ht-1, so to do this we require the update gate’s output zt and the output can be computed by the below formula\n",
    "\n",
    "![test13](https://cdn-images-1.medium.com/max/800/1*zxSTnqedwLRoicgHKYKsVQ.png)\n",
    " \n",
    "Here we will multiply by Hadamard product function the hidden input with the update gate’s output and then Hadamard multiplier on (1-zt) and ht before passing it to the plus operator.\n",
    "\n",
    "![test14](https://cdn-images-1.medium.com/max/800/1*UxZ0pTQW8kofL9bzPVYV1w.png)\n",
    " \n",
    "\n",
    "This is how GRUs can store and update using reset and update gates which eliminates vanishing gradient problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3gZlVA0II7p"
   },
   "source": [
    "<h3><a id=\"Imports\">&#9997; Imports</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1HzFn9FII7q"
   },
   "source": [
    "**Import All the neccessary Libraries which we will be requiring to run this notebook and the project**\n",
    "\n",
    "I will be using seq2seq model with the LSTM so all the libraries related to it should be installed and also Keras's tokenizer function which is essential to the tekenize the text\n",
    "\n",
    "All the basic libraries such as numpy, pandas, string, matplotlib and etc also imported along with other helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c9i2PR3-II7r",
    "outputId": "6bb4b508-4782-4892-e34a-a1471f88b913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#Make plots bigger\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97-bGVeHII7x"
   },
   "source": [
    "Creating a function which will read the file, encode it and save it and then the funtion to_lines wil  split the data into Italian & English part seperately by '\\n' and build it in the form of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7JJQcTaII7y"
   },
   "outputs": [],
   "source": [
    "def Read_TextFile(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def to_lines(text):\n",
    "    sentences = text.strip().split('\\n')\n",
    "    sentences = [i.split('\\t') for i in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCMWxAs-II71"
   },
   "source": [
    "By using the function we wrote the text file is imported to the jupyter notebook. \n",
    "\n",
    "Once the notebook is opened and read using the function Read_TextFile we will pass it to the to_lines to split and to build the sentences of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2x-Jknh5II72"
   },
   "outputs": [],
   "source": [
    "data = Read_TextFile(\"ita.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "1S7LmC7SII74",
    "outputId": "a1a34574-b486-40cf-e7f9-28578c245279"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Ciao!'],\n",
       " ['Run!', 'Corri!'],\n",
       " ['Run!', 'Corra!'],\n",
       " ['Run!', 'Correte!'],\n",
       " ['Who?', 'Chi?'],\n",
       " ['Wow!', 'Wow!'],\n",
       " ['Jump!', 'Salta!'],\n",
       " ['Jump!', 'Salti!'],\n",
       " ['Jump!', 'Saltate!'],\n",
       " ['Jump.', 'Salta.'],\n",
       " ['Jump.', 'Salti.'],\n",
       " ['Jump.', 'Saltate.'],\n",
       " ['Stop!', 'Fermati!'],\n",
       " ['Stop!', 'Fermatevi!'],\n",
       " ['Stop!', 'Stop!'],\n",
       " ['Stop!', 'Si fermi!'],\n",
       " ['Wait!', 'Aspetta!'],\n",
       " ['Wait!', 'Aspettate!'],\n",
       " ['Wait!', 'Aspetti!'],\n",
       " ['Wait.', 'Aspetta.'],\n",
       " ['Wait.', 'Aspetti.'],\n",
       " ['Wait.', 'Aspettate.'],\n",
       " ['Do it.', 'Fallo.'],\n",
       " ['Do it.', 'Falla.'],\n",
       " ['Do it.', 'Lo faccia.'],\n",
       " ['Do it.', 'La faccia.'],\n",
       " ['Do it.', 'Fatelo.'],\n",
       " ['Do it.', 'Fatela.'],\n",
       " ['Go on.', 'Vai avanti.'],\n",
       " ['Go on.', 'Continua.'],\n",
       " ['Go on.', 'Continui.'],\n",
       " ['Go on.', 'Continuate.'],\n",
       " ['Go on.', 'Vada avanti.'],\n",
       " ['Go on.', 'Andate avanti.'],\n",
       " ['Hello!', 'Buongiorno!'],\n",
       " ['Hello!', 'Ciao!'],\n",
       " ['Hello!', 'Salve.'],\n",
       " ['I ran.', 'Ho corso.'],\n",
       " ['I ran.', 'Corsi.'],\n",
       " ['I see.', 'Capisco.'],\n",
       " ['I see.', 'Io capisco.'],\n",
       " ['I try.', 'Provo.'],\n",
       " ['I try.', 'Io provo.'],\n",
       " ['I won!', 'Ho vinto!'],\n",
       " ['I won.', 'Vinsi.'],\n",
       " ['Oh no!', 'Oh no!'],\n",
       " ['Relax.', 'Rilassati.'],\n",
       " ['Relax.', 'Si rilassi.'],\n",
       " ['Relax.', 'Rilassatevi.'],\n",
       " ['Shoot!', 'Spara!'],\n",
       " ['Shoot!', 'Spari!'],\n",
       " ['Shoot!', 'Sparate!'],\n",
       " ['Smile.', 'Sorridi.'],\n",
       " ['Smile.', 'Sorrida.'],\n",
       " ['Smile.', 'Sorridete.'],\n",
       " ['Attack!', 'Attacca!'],\n",
       " ['Attack!', 'Attaccate!'],\n",
       " ['Attack!', 'Attacchi!'],\n",
       " ['Cheers!', 'Alla vostra!'],\n",
       " ['Cheers!', 'Salute!'],\n",
       " ['Cheers!', 'Alla tua!'],\n",
       " ['Cheers!', 'Alla sua!'],\n",
       " ['Eat it.', 'Mangialo.'],\n",
       " ['Eat it.', 'Mangiala.'],\n",
       " ['Eat it.', 'Lo mangi.'],\n",
       " ['Eat it.', 'La mangi.'],\n",
       " ['Eat it.', 'Mangiatelo.'],\n",
       " ['Eat it.', 'Mangiatela.'],\n",
       " ['Eat up.', 'Finisci di mangiare.'],\n",
       " ['Eat up.', 'Finisca di mangiare.'],\n",
       " ['Eat up.', 'Finite di mangiare.'],\n",
       " ['Freeze!', 'Fermo!'],\n",
       " ['Freeze!', 'Ferma!'],\n",
       " ['Freeze!', 'Fermi!'],\n",
       " ['Freeze!', 'Ferme!'],\n",
       " ['Freeze!', 'Che nessuno si muova!'],\n",
       " ['Get up.', 'Alzati.'],\n",
       " ['Go now.', 'Vai ora.'],\n",
       " ['Go now.', 'Vai adesso.'],\n",
       " ['Go now.', 'Vada ora.'],\n",
       " ['Go now.', 'Vada adesso.'],\n",
       " ['Go now.', 'Andate ora.'],\n",
       " ['Go now.', 'Andate adesso.'],\n",
       " ['Got it!', 'Capito!'],\n",
       " ['Got it!', 'Capita!'],\n",
       " ['He ran.', 'Corse.'],\n",
       " ['Hop in.', 'Salta su.'],\n",
       " ['Hop in.', 'Saltate su.'],\n",
       " ['Hop in.', 'Salti su.'],\n",
       " ['Hug me.', 'Abbracciami.'],\n",
       " ['Hug me.', 'Mi abbracci.'],\n",
       " ['Hug me.', 'Abbracciatemi.'],\n",
       " ['I fell.', 'Sono caduta.'],\n",
       " ['I fell.', 'Sono caduto.'],\n",
       " ['I fell.', 'Io sono caduto.'],\n",
       " ['I fell.', 'Io sono caduta.'],\n",
       " ['I fell.', 'Caddi.'],\n",
       " ['I fell.', 'Io caddi.'],\n",
       " ['I knit.', 'Lavoro a maglia.'],\n",
       " ['I knit.', 'Io lavoro a maglia.'],\n",
       " ['I knit.', 'Lavoro ai ferri.'],\n",
       " ['I knit.', 'Io lavoro ai ferri.'],\n",
       " ['I know.', 'Lo so.'],\n",
       " ['I know.', 'Io lo so.'],\n",
       " ['I left.', 'Sono partito.'],\n",
       " ['I left.', 'Io sono partito.'],\n",
       " ['I left.', 'Sono partita.'],\n",
       " ['I left.', 'Io sono partita.'],\n",
       " ['I left.', 'Partii.'],\n",
       " ['I left.', 'Io partii.'],\n",
       " ['I left.', 'Me ne sono andato.'],\n",
       " ['I left.', 'Io me ne sono andato.'],\n",
       " ['I left.', 'Me ne sono andata.'],\n",
       " ['I left.', 'Io me ne sono andata.'],\n",
       " ['I left.', 'Me ne andai.'],\n",
       " ['I left.', 'Io me ne andai.'],\n",
       " ['I lied.', 'Ho mentito.'],\n",
       " ['I lied.', 'Io ho mentito.'],\n",
       " ['I lied.', 'Mentii.'],\n",
       " ['I lied.', 'Io mentii.'],\n",
       " ['I lost.', 'Ho perso.'],\n",
       " ['I lost.', 'Io ho perso.'],\n",
       " ['I lost.', 'Persi.'],\n",
       " ['I lost.', 'Io persi.'],\n",
       " ['I paid.', 'Ho pagato.'],\n",
       " ['I paid.', 'Pagai.'],\n",
       " ['I quit.', 'Mi licenzio.'],\n",
       " ['I swim.', 'Nuoto.'],\n",
       " ['I work.', 'Io lavoro.'],\n",
       " ['I work.', 'Lavoro.'],\n",
       " [\"I'm 19.\", 'Ho diciannove anni.'],\n",
       " [\"I'm 19.\", 'Ho 19 anni.'],\n",
       " [\"I'm OK.\", 'Sto bene.'],\n",
       " ['Listen.', 'Ascolta!'],\n",
       " ['Listen.', 'Ascoltate!'],\n",
       " ['Listen.', 'Ascolti!'],\n",
       " ['No way!', 'È impossibile.'],\n",
       " ['Really?', 'Veramente?'],\n",
       " ['Really?', 'Davvero?'],\n",
       " ['Thanks.', 'Grazie!'],\n",
       " ['Try it.', 'Provalo.'],\n",
       " ['Try it.', 'Provala.'],\n",
       " ['Try it.', 'Lo provi.'],\n",
       " ['Try it.', 'La provi.'],\n",
       " ['Try it.', 'Provatelo.'],\n",
       " ['Try it.', 'Provatela.'],\n",
       " ['We ate.', 'Abbiamo mangiato.'],\n",
       " ['We ate.', 'Noi abbiamo mangiato.'],\n",
       " ['We ate.', 'Mangiammo.'],\n",
       " ['We ate.', 'Noi mangiammo.'],\n",
       " ['We try.', 'Proviamo.'],\n",
       " ['We try.', 'Ci proviamo.'],\n",
       " ['We try.', 'Noi proviamo.'],\n",
       " ['We won.', 'Abbiamo vinto.'],\n",
       " ['We won.', 'Noi abbiamo vinto.'],\n",
       " ['We won.', 'Vincemmo.'],\n",
       " ['We won.', 'Noi vincemmo.'],\n",
       " ['Why me?', 'Perché io?'],\n",
       " ['Ask Tom.', 'Chiedi a Tom.'],\n",
       " ['Ask Tom.', 'Chiedete a Tom.'],\n",
       " ['Ask Tom.', 'Chieda a Tom.'],\n",
       " ['Ask Tom.', 'Chiedilo a Tom.'],\n",
       " ['Ask Tom.', 'Chiedetelo a Tom.'],\n",
       " ['Ask Tom.', 'Lo chieda a Tom.'],\n",
       " ['Awesome!', 'Fantastico!'],\n",
       " ['Awesome!', 'Spettacolare!'],\n",
       " ['Be calm.', 'Sii calmo.'],\n",
       " ['Be calm.', 'Sii calma.'],\n",
       " ['Be calm.', 'Sia calmo.'],\n",
       " ['Be calm.', 'Sia calma.'],\n",
       " ['Be calm.', 'Siate calmi.'],\n",
       " ['Be calm.', 'Siate calme.'],\n",
       " ['Be cool.', 'Atteggiati.'],\n",
       " ['Be fair.', 'Sii imparziale.'],\n",
       " ['Be fair.', 'Sia imparziale.'],\n",
       " ['Be fair.', 'Siate imparziali.'],\n",
       " ['Be kind.', 'Sii gentile.'],\n",
       " ['Be kind.', 'Sia gentile.'],\n",
       " ['Be kind.', 'Siate gentili.'],\n",
       " ['Be nice.', 'Sii gentile.'],\n",
       " ['Be nice.', 'Sia gentile.'],\n",
       " ['Be nice.', 'Siate gentili.'],\n",
       " ['Beat it.', 'Sparisci.'],\n",
       " ['Burn it.', 'Brucialo.'],\n",
       " ['Burn it.', 'Bruciala.'],\n",
       " ['Burn it.', 'Lo bruci.'],\n",
       " ['Burn it.', 'La bruci.'],\n",
       " ['Burn it.', 'Bruciatelo.'],\n",
       " ['Burn it.', 'Bruciatela.'],\n",
       " ['Call me.', 'Chiamami.'],\n",
       " ['Call me.', 'Chiamatemi.'],\n",
       " ['Call me.', 'Mi chiami.'],\n",
       " ['Call us.', 'Chiamaci.'],\n",
       " ['Call us.', 'Ci chiami.'],\n",
       " ['Call us.', 'Chiamateci.'],\n",
       " ['Come in.', 'Entrate!'],\n",
       " ['Come in.', 'Entra!'],\n",
       " ['Come in.', 'Entri!'],\n",
       " ['Come in.', 'Vieni dentro.'],\n",
       " ['Come in.', 'Venite dentro.'],\n",
       " ['Come in.', 'Venga dentro.'],\n",
       " ['Come in.', 'Entrate.'],\n",
       " ['Come on!', 'Forza!'],\n",
       " ['Come on!', 'Vada!'],\n",
       " ['Come on!', 'Andate!'],\n",
       " ['Drop it!', 'Mollalo!'],\n",
       " ['Drop it!', 'Mollala!'],\n",
       " ['Drop it!', 'Lo molli!'],\n",
       " ['Drop it!', 'La molli!'],\n",
       " ['Drop it!', 'Mollatelo!'],\n",
       " ['Drop it!', 'Mollatela!'],\n",
       " ['Get Tom.', 'Prendi Tom.'],\n",
       " ['Get Tom.', 'Prenda Tom.'],\n",
       " ['Get Tom.', 'Prendete Tom.'],\n",
       " ['Get out!', 'Vattene!'],\n",
       " ['Get out!', 'Andatevene!'],\n",
       " ['Get out!', 'Esci!'],\n",
       " ['Get out!', 'Se ne vada!'],\n",
       " ['Get out!', 'Vattene fuori!'],\n",
       " ['Get out!', 'Se ne vada fuori!'],\n",
       " ['Get out!', 'Andatevene fuori!'],\n",
       " ['Get out!', 'Uscite!'],\n",
       " ['Get out!', 'Esca!'],\n",
       " ['Get out.', 'Esci.'],\n",
       " ['Get out.', 'Vattene.'],\n",
       " ['Get out.', 'Uscite.'],\n",
       " ['Get out.', 'Esca.'],\n",
       " ['Go away!', 'Vattene!'],\n",
       " ['Go away!', 'Andatevene!'],\n",
       " ['Go away!', 'Vai via!'],\n",
       " ['Go away!', 'Se ne vada!'],\n",
       " ['Go away!', 'Vada via!'],\n",
       " ['Go away!', 'Andate via!'],\n",
       " ['Go away.', 'Vattene.'],\n",
       " ['Go away.', 'Se ne vada.'],\n",
       " ['Go away.', 'Andatevene.'],\n",
       " ['Go away.', 'Vai via.'],\n",
       " ['Go away.', 'Andate via.'],\n",
       " ['Go away.', 'Vada via.'],\n",
       " ['Go home.', 'Vai a casa.'],\n",
       " ['Go home.', 'Vada a casa.'],\n",
       " ['Go home.', 'Andate a casa.'],\n",
       " ['Go slow.', 'Vai lentamente.'],\n",
       " ['Go slow.', 'Vada lentamente.'],\n",
       " ['Go slow.', 'Andate lentamente.'],\n",
       " ['Goodbye!', 'Arrivederci.'],\n",
       " ['Goodbye!', 'Ciao!'],\n",
       " ['Hang on!', 'Aspetta!'],\n",
       " ['Hang on!', 'Aspettate!'],\n",
       " ['Hang on!', 'Aspetti!'],\n",
       " ['Hang on!', 'Attendi!'],\n",
       " ['Hang on!', 'Attenda!'],\n",
       " ['Hang on!', 'Attendete!'],\n",
       " ['Hang on.', 'Aspetta!'],\n",
       " ['Hang on.', 'Aspettate!'],\n",
       " ['Hang on.', 'Aspetti!'],\n",
       " ['Hang on.', 'Attendi!'],\n",
       " ['Hang on.', 'Attenda!'],\n",
       " ['Hang on.', 'Attendete!'],\n",
       " ['He came.', 'È venuto.'],\n",
       " ['He came.', 'Lui è venuto.'],\n",
       " ['He quit.', 'Ha rinunciato.'],\n",
       " ['He quit.', 'Lui ha rinunciato.'],\n",
       " ['He quit.', 'Rinunciò.'],\n",
       " ['He quit.', 'Lui rinunciò.'],\n",
       " ['He runs.', 'Corre.'],\n",
       " ['He runs.', 'Lui corre.'],\n",
       " ['Help me!', 'Aiutatemi!'],\n",
       " ['Help me!', 'Mi aiuti!'],\n",
       " ['Help me.', 'Aiutami.'],\n",
       " ['Help me.', 'Mi aiuti.'],\n",
       " ['Help me.', 'Aiutatemi.'],\n",
       " ['Help us.', 'Aiutateci.'],\n",
       " ['Help us.', 'Aiutaci.'],\n",
       " ['Help us.', 'Ci aiuti.'],\n",
       " ['Hit Tom.', 'Colpisci Tom.'],\n",
       " ['Hit Tom.', 'Colpisca Tom.'],\n",
       " ['Hit Tom.', 'Colpite Tom.'],\n",
       " ['Hold it!', 'Aspetta!'],\n",
       " ['Hold it!', 'Aspettate!'],\n",
       " ['Hold it!', 'Aspetti!'],\n",
       " ['Hold it!', 'Aspetta un attimo!'],\n",
       " ['Hold it!', 'Aspetti un attimo!'],\n",
       " ['Hold it!', 'Aspettate un attimo!'],\n",
       " ['Hold it!', 'Un attimo!'],\n",
       " ['Hug Tom.', 'Abbraccia Tom.'],\n",
       " ['Hug Tom.', 'Abbracci Tom.'],\n",
       " ['Hug Tom.', 'Abbracciate Tom.'],\n",
       " ['I agree.', \"Io sono d'accordo.\"],\n",
       " ['I bowed.', 'Mi sono inchinato.'],\n",
       " ['I bowed.', 'Mi sono inchinata.'],\n",
       " ['I bowed.', 'Mi inchinai.'],\n",
       " ['I cried.', 'Ho pianto.'],\n",
       " ['I cried.', 'Piansi.'],\n",
       " ['I dozed.', 'Ho dormicchiato.'],\n",
       " ['I dozed.', 'Dormicchiai.'],\n",
       " ['I dozed.', 'Ho sonnecchiato.'],\n",
       " ['I dozed.', 'Sonnecchiai.'],\n",
       " ['I drive.', 'Guido.'],\n",
       " ['I drive.', 'Io guido.'],\n",
       " ['I drove.', 'Ho guidato.'],\n",
       " ['I drove.', 'Guidai.'],\n",
       " ['I moved.', 'Mi sono mosso.'],\n",
       " ['I moved.', 'Mi sono mossa.'],\n",
       " ['I moved.', 'Mi sono trasferito.'],\n",
       " ['I moved.', 'Mi sono trasferita.'],\n",
       " ['I moved.', 'Mi mossi.'],\n",
       " ['I moved.', 'Mi trasferii.'],\n",
       " ['I slept.', 'Ho dormito.'],\n",
       " ['I slept.', 'Io ho dormito.'],\n",
       " ['I slept.', 'Dormii.'],\n",
       " ['I slept.', 'Io dormii.'],\n",
       " ['I smoke.', 'Io fumo.'],\n",
       " ['I smoke.', 'Fumo.'],\n",
       " ['I snore.', 'Russo.'],\n",
       " ['I snore.', 'Io russo.'],\n",
       " ['I stink.', 'Puzzo.'],\n",
       " ['I stink.', 'Io puzzo.'],\n",
       " ['I swore.', 'Ho giurato.'],\n",
       " ['I swore.', 'Giurai.'],\n",
       " [\"I'll go.\", 'Andrò.'],\n",
       " [\"I'll go.\", 'Io andrò.'],\n",
       " [\"I'm Tom.\", 'Sono Tom.'],\n",
       " [\"I'm Tom.\", 'Io sono Tom.'],\n",
       " [\"I'm fat.\", 'Sono grasso.'],\n",
       " [\"I'm fat.\", 'Io sono grasso.'],\n",
       " [\"I'm fat.\", 'Sono grassa.'],\n",
       " [\"I'm fat.\", 'Io sono grassa.'],\n",
       " [\"I'm fit.\", 'Sono in forma.'],\n",
       " [\"I'm fit.\", 'Io sono in forma.'],\n",
       " [\"I'm hit!\", 'Sono stato colpito!'],\n",
       " [\"I'm hit!\", 'Sono stata colpita!'],\n",
       " [\"I'm hot.\", 'Ho caldo.'],\n",
       " [\"I'm hot.\", 'Io ho caldo.'],\n",
       " [\"I'm ill.\", 'Sono malato.'],\n",
       " [\"I'm ill.\", 'Sono malata.'],\n",
       " [\"I'm ill.\", 'Sto male.'],\n",
       " [\"I'm ill.\", 'Sono ammalato.'],\n",
       " [\"I'm old.\", 'Sono vecchio.'],\n",
       " [\"I'm old.\", 'Sono vecchia.'],\n",
       " [\"I'm old.\", 'Io sono vecchio.'],\n",
       " [\"I'm old.\", 'Io sono vecchia.'],\n",
       " [\"I'm sad.\", 'Sono triste.'],\n",
       " [\"I'm sad.\", 'Io sono triste.'],\n",
       " [\"I'm shy.\", 'Sono timido.'],\n",
       " [\"I'm shy.\", 'Io sono timido.'],\n",
       " [\"I'm shy.\", 'Sono timida.'],\n",
       " [\"I'm shy.\", 'Io sono timida.'],\n",
       " [\"It's OK.\", 'Va bene.'],\n",
       " ['Join us.', 'Unisciti a noi.'],\n",
       " ['Keep it.', 'Tienilo.'],\n",
       " ['Keep it.', 'Lo tenga.'],\n",
       " ['Keep it.', 'Tenetelo.'],\n",
       " ['Keep it.', 'Tienila.'],\n",
       " ['Keep it.', 'La tenga.'],\n",
       " ['Keep it.', 'Tenetela.'],\n",
       " ['Kick it.', 'Calcialo.'],\n",
       " ['Kick it.', 'Calciala.'],\n",
       " ['Kick it.', 'Lo calci.'],\n",
       " ['Kick it.', 'La calci.'],\n",
       " ['Kick it.', 'Calciatelo.'],\n",
       " ['Kick it.', 'Calciatela.'],\n",
       " ['Kiss me.', 'Baciami.'],\n",
       " ['Kiss me.', 'Baciatemi.'],\n",
       " ['Kiss me.', 'Mi baci.'],\n",
       " ['Lock it.', 'Chiudilo a chiave.'],\n",
       " ['Lock it.', 'Chiudila a chiave.'],\n",
       " ['Lock it.', 'Lo chiuda a chiave.'],\n",
       " ['Lock it.', 'Chiudetelo a chiave.'],\n",
       " ['Lock it.', 'La chiuda a chiave.'],\n",
       " ['Lock it.', 'Chiudetela a chiave.'],\n",
       " ['Me, too.', \"Anch'io.\"],\n",
       " ['Me, too.', 'Anche io.'],\n",
       " ['Me, too.', 'Pure io.'],\n",
       " ['Open up.', 'Apriti.'],\n",
       " ['Open up.', 'Apritevi.'],\n",
       " ['Open up.', 'Si apra.'],\n",
       " ['Open up.', 'Apri.'],\n",
       " ['Open up.', 'Apra.'],\n",
       " ['Open up.', 'Aprite.'],\n",
       " ['Open up.', 'Confidati.'],\n",
       " ['Open up.', 'Si confidi.'],\n",
       " ['Open up.', 'Confidatevi.'],\n",
       " ['Perfect!', 'Perfetto!'],\n",
       " ['Pull it.', 'Tiralo.'],\n",
       " ['Pull it.', 'Tirala.'],\n",
       " ['Pull it.', 'Lo tiri.'],\n",
       " ['Pull it.', 'La tiri.'],\n",
       " ['Pull it.', 'Tiratelo.'],\n",
       " ['Pull it.', 'Tiratela.'],\n",
       " ['Push it.', 'Spingilo.'],\n",
       " ['Push it.', 'Spingila.'],\n",
       " ['Push it.', 'Lo spinga.'],\n",
       " ['Push it.', 'La spinga.'],\n",
       " ['Push it.', 'Spingetelo.'],\n",
       " ['Push it.', 'Spingetela.'],\n",
       " ['See you!', 'Arrivederci.'],\n",
       " ['See you!', 'Ci si vede!'],\n",
       " ['See you.', 'Arrivederci.'],\n",
       " ['See you.', 'Ci si vede!'],\n",
       " ['See you.', 'Ci si vede.'],\n",
       " ['See you.', 'Ci vediamo.'],\n",
       " ['Show me.', 'Fammi vedere.'],\n",
       " ['Show me.', 'Mostrami.'],\n",
       " ['Show me.', 'Fatemi vedere.'],\n",
       " ['Show me.', 'Mi faccia vedere.'],\n",
       " ['Show me.', 'Mostratemi.'],\n",
       " ['Show me.', 'Mi mostri.'],\n",
       " ['Shut up!', 'Taci!'],\n",
       " ['Shut up!', 'Stai zitto!'],\n",
       " ['Shut up!', 'Stai zitta!'],\n",
       " ['Shut up!', 'Stia zitto!'],\n",
       " ['Shut up!', 'Stia zitta!'],\n",
       " ['Shut up!', 'State zitti!'],\n",
       " ['Shut up!', 'State zitte!'],\n",
       " ['Shut up!', 'Silenzio!'],\n",
       " ['Shut up!', 'Tacete!'],\n",
       " ['Shut up!', 'Taccia!'],\n",
       " ['Shut up!', \"Sta' zitto!\"],\n",
       " ['Skip it.', 'Saltalo.'],\n",
       " ['Skip it.', 'Saltala.'],\n",
       " ['Skip it.', 'Lo salti.'],\n",
       " ['Skip it.', 'La salti.'],\n",
       " ['Skip it.', 'Saltatelo.'],\n",
       " ['Skip it.', 'Saltatela.'],\n",
       " ['So long.', 'A tra poco!'],\n",
       " ['Stop it.', 'Smettila!'],\n",
       " ['Stop it.', 'La smetta!'],\n",
       " ['Stop it.', 'Smettetela!'],\n",
       " ['Tom ate.', 'Tom ha mangiato.'],\n",
       " ['Tom ate.', 'Tom mangiò.'],\n",
       " ['Tom ran.', 'Tom ha corso.'],\n",
       " ['Tom ran.', 'Tom corse.'],\n",
       " ['Tom won.', 'Tom ha vinto.'],\n",
       " ['Tom won.', 'Tom vinse.'],\n",
       " ['Tom won.', 'Ha vinto Tom.'],\n",
       " ['Tom won.', 'Vinse Tom.'],\n",
       " ['Wait up.', 'Aspetta.'],\n",
       " ['Wait up.', 'Aspetti.'],\n",
       " ['Wait up.', 'Aspettate.'],\n",
       " ['Wake up!', 'Sveglia!'],\n",
       " ['Wake up!', 'Svegliati!'],\n",
       " ['Wake up!', 'Svegliatevi!'],\n",
       " ['Wake up!', 'Si svegli!'],\n",
       " ['Wake up.', 'Alzati.'],\n",
       " ['Wash up.', 'Lavati.'],\n",
       " ['Wash up.', 'Lavatevi.'],\n",
       " ['Wash up.', 'Si lavi.'],\n",
       " ['We care.', 'A noi importa.'],\n",
       " ['We know.', 'Lo sappiamo.'],\n",
       " ['We know.', 'Noi lo sappiamo.'],\n",
       " ['We know.', 'Sappiamo.'],\n",
       " ['We know.', 'Noi sappiamo.'],\n",
       " ['We lost.', 'Abbiamo perso.'],\n",
       " ['We lost.', 'Noi abbiamo perso.'],\n",
       " ['We lost.', 'Perdemmo.'],\n",
       " ['We lost.', 'Noi perdemmo.'],\n",
       " ['Welcome.', 'Benvenuto!'],\n",
       " ['Welcome.', 'Benvenuta!'],\n",
       " ['Welcome.', 'Benvenuti!'],\n",
       " ['Welcome.', 'Benvenute!'],\n",
       " ['Welcome.', 'Ben arrivato!'],\n",
       " ['Who ate?', 'Chi ha mangiato?'],\n",
       " ['Who ran?', 'Chi ha corso?'],\n",
       " ['Who won?', 'Chi ha vinto?'],\n",
       " ['Why not?', 'Perché no?'],\n",
       " ['You run.', 'Corri.'],\n",
       " ['You run.', 'Correte.'],\n",
       " ['You run.', 'Corra.'],\n",
       " ['You won.', 'Hai vinto.'],\n",
       " ['You won.', 'Ha vinto.'],\n",
       " ['You won.', 'Avete vinto.'],\n",
       " ['Am I fat?', 'Sono grasso?'],\n",
       " ['Am I fat?', 'Io sono grasso?'],\n",
       " ['Am I fat?', 'Sono grassa?'],\n",
       " ['Am I fat?', 'Io sono grassa?'],\n",
       " ['Ask them.', 'Chiedilo a loro.'],\n",
       " ['Ask them.', 'Lo chieda a loro.'],\n",
       " ['Ask them.', 'Chiedetelo a loro.'],\n",
       " ['Back off!', 'Stai indietro!'],\n",
       " ['Back off!', 'Stia indietro!'],\n",
       " ['Back off!', 'State indietro!'],\n",
       " ['Back off.', 'Indietreggia.'],\n",
       " ['Back off.', 'Indietreggi.'],\n",
       " ['Back off.', 'Indietreggiate.'],\n",
       " ['Back off.', 'Tirati indietro.'],\n",
       " ['Back off.', 'Si tiri indietro.'],\n",
       " ['Back off.', 'Tiratevi indietro.'],\n",
       " ['Be brave.', 'Sii coraggioso.'],\n",
       " ['Be brave.', 'Sii coraggiosa.'],\n",
       " ['Be brave.', 'Sia coraggioso.'],\n",
       " ['Be brave.', 'Sia coraggiosa.'],\n",
       " ['Be brave.', 'Siate coraggiosi.'],\n",
       " ['Be brave.', 'Siate coraggiose.'],\n",
       " ['Be brief.', 'Siate brevi.'],\n",
       " ['Be brief.', 'Sii breve.'],\n",
       " ['Be brief.', 'Sia breve.'],\n",
       " ['Be still.', 'Stai fermo.'],\n",
       " ['Be still.', 'Stai ferma.'],\n",
       " ['Be still.', 'Stia fermo.'],\n",
       " ['Be still.', 'Stia ferma.'],\n",
       " ['Be still.', 'State fermi.'],\n",
       " ['Be still.', 'State ferme.'],\n",
       " ['Buzz off.', 'Levati dai piedi.'],\n",
       " ['Buzz off.', 'Si levi dai piedi.'],\n",
       " ['Buzz off.', 'Levatevi dai piedi.'],\n",
       " ['Bye, Tom.', 'Arrivederci, Tom.'],\n",
       " ['Bye, Tom.', 'Ci vediamo, Tom.'],\n",
       " ['Bye, Tom.', 'Ci si vede, Tom.'],\n",
       " ['Call Tom.', 'Chiama Tom.'],\n",
       " ['Call Tom.', 'Chiami Tom.'],\n",
       " ['Call Tom.', 'Chiamate Tom.'],\n",
       " ['Can I go?', 'Posso andare?'],\n",
       " ['Cheer up!', 'Coraggio!'],\n",
       " ['Cheer up!', 'Su col morale!'],\n",
       " ['Cheer up!', 'Su con la vita!'],\n",
       " ['Cool off!', 'Calmati!'],\n",
       " ['Cool off!', 'Rilassati!'],\n",
       " ['Cool off!', 'Rilassatevi!'],\n",
       " ['Cool off!', 'Si rilassi!'],\n",
       " ['Cool off!', 'Si calmi!'],\n",
       " ['Cool off!', 'Calmatevi!'],\n",
       " ['Cuff him.', 'Ammanettalo.'],\n",
       " ['Cuff him.', 'Lo ammanetti.'],\n",
       " ['Cuff him.', 'Ammanettatelo.'],\n",
       " [\"Don't go.\", 'Non andare.'],\n",
       " [\"Don't go.\", 'Non andate.'],\n",
       " [\"Don't go.\", 'Non vada.'],\n",
       " ['Drive on.', 'Continua a guidare.'],\n",
       " ['Drive on.', 'Continui a guidare.'],\n",
       " ['Drive on.', 'Continuate a guidare.'],\n",
       " ['Find Tom.', 'Trova Tom.'],\n",
       " ['Find Tom.', 'Trovate Tom.'],\n",
       " ['Find Tom.', 'Trovi Tom.'],\n",
       " ['Fix this.', 'Ripara questo.'],\n",
       " ['Fix this.', 'Riparate questo.'],\n",
       " ['Fix this.', 'Ripari questo.'],\n",
       " ['Fix this.', 'Fissa questo.'],\n",
       " ['Fix this.', 'Fissate questo.'],\n",
       " ['Fix this.', 'Fissi questo.'],\n",
       " ['Get away!', 'Vattene!'],\n",
       " ['Get away!', 'Andatevene!'],\n",
       " ['Get away!', 'Vai via!'],\n",
       " ['Get away!', 'Vattene.'],\n",
       " ['Get away!', 'Se ne vada.'],\n",
       " ['Get away!', 'Andatevene.'],\n",
       " ['Get away!', 'Vai via.'],\n",
       " ['Get away!', 'Andate via.'],\n",
       " ['Get away!', 'Vada via.'],\n",
       " ['Get away!', 'Se ne vada!'],\n",
       " ['Get away!', 'Vada via!'],\n",
       " ['Get away!', 'Andate via!'],\n",
       " ['Get down!', 'Vieni giù!'],\n",
       " ['Get down!', 'Venite giù!'],\n",
       " ['Get down!', 'Venga giù!'],\n",
       " ['Get lost!', 'Smamma!'],\n",
       " ['Get lost!', 'Smammate!'],\n",
       " ['Get lost!', 'Smammi!'],\n",
       " ['Get lost.', 'Smamma.'],\n",
       " ['Get lost.', 'Smammi.'],\n",
       " ['Get lost.', 'Smammate.'],\n",
       " ['Get lost.', 'Sgomma.'],\n",
       " ['Get lost.', 'Sgommate.'],\n",
       " ['Get lost.', 'Sgommi.'],\n",
       " ['Get lost.', 'Vai al diavolo.'],\n",
       " ['Get lost.', 'Vada al diavolo.'],\n",
       " ['Get lost.', 'Andate al diavolo.'],\n",
       " ['Get real.', 'Sii realista.'],\n",
       " ['Get real.', 'Sia realista.'],\n",
       " ['Get real.', 'Siate realisti.'],\n",
       " ['Get real.', 'Siate realiste.'],\n",
       " ['Go ahead!', 'Vai pure!'],\n",
       " ['Go ahead!', 'Vada pure!'],\n",
       " ['Go ahead!', 'Andate pure!'],\n",
       " ['Go ahead.', 'Vai pure.'],\n",
       " ['Go ahead.', 'Vai avanti.'],\n",
       " ['Good job!', 'Buon lavoro!'],\n",
       " ['Grab Tom.', 'Agguanta Tom.'],\n",
       " ['Grab Tom.', 'Agguantate Tom.'],\n",
       " ['Grab Tom.', 'Agguanti Tom.'],\n",
       " ['Grab him.', 'Afferralo.'],\n",
       " ['Grab him.', 'Afferratelo.'],\n",
       " ['Grab him.', 'Lo afferri.'],\n",
       " ['Have fun.', 'Divertiti!'],\n",
       " ['Have fun.', 'Divertitevi!'],\n",
       " ['Have fun.', 'Si diverta!'],\n",
       " ['He spoke.', 'Ha parlato.'],\n",
       " ['He spoke.', 'Lui ha parlato.'],\n",
       " ['He spoke.', 'Parlò.'],\n",
       " ['He spoke.', 'Lui parlò.'],\n",
       " ['He tried.', 'Provò.'],\n",
       " ['He tried.', 'Lui provò.'],\n",
       " ['He tries.', 'Prova.'],\n",
       " ['He tries.', 'Lui prova.'],\n",
       " [\"He's wet.\", 'È bagnato.'],\n",
       " [\"He's wet.\", 'Lui è bagnato.'],\n",
       " ['Help Tom.', 'Aiuta Tom.'],\n",
       " ['Help Tom.', 'Aiutate Tom.'],\n",
       " ['Help Tom.', 'Aiuti Tom.'],\n",
       " ['Help him.', 'Aiutatelo.'],\n",
       " ['Help him.', 'Aiutalo.'],\n",
       " ['Help him.', 'Lo aiuti.'],\n",
       " ['How cute!', 'Che carino!'],\n",
       " ['How cute!', 'Che carina!'],\n",
       " ['How cute!', 'Che carini!'],\n",
       " ['How cute!', 'Che carine!'],\n",
       " ['How deep?', 'Quanto profondo?'],\n",
       " ['How deep?', 'Quanto profonda?'],\n",
       " ['How deep?', 'Quanto profondi?'],\n",
       " ['How deep?', 'Quanto profonde?'],\n",
       " ['How nice!', 'Che bella!'],\n",
       " ['How nice!', 'Che belli!'],\n",
       " ['How nice!', 'Che belle!'],\n",
       " ['Humor me.', 'Assecondami.'],\n",
       " ['Humor me.', 'Mi assecondi.'],\n",
       " ['Humor me.', 'Assecondatemi.'],\n",
       " ['Humor me.', 'Fammi ridere.'],\n",
       " ['Humor me.', 'Mi faccia ridere.'],\n",
       " ['Humor me.', 'Fatemi ridere.'],\n",
       " ['Hurry up.', 'Svelto!'],\n",
       " ['Hurry up.', 'Sbrigati!'],\n",
       " ['Hurry up.', 'Sbrigatevi!'],\n",
       " ['Hurry up.', 'Si sbrighi!'],\n",
       " ['I agreed.', \"Ero d'accordo.\"],\n",
       " ['I agreed.', \"Io ero d'accordo.\"],\n",
       " ['I am fat.', 'Sono grasso.'],\n",
       " ['I am fat.', 'Io sono grasso.'],\n",
       " ['I am fat.', 'Sono grassa.'],\n",
       " ['I am fat.', 'Io sono grassa.'],\n",
       " ['I am hot.', 'Ho caldo.'],\n",
       " ['I am hot.', 'Io ho caldo.'],\n",
       " ['I am old.', 'Sono vecchio.'],\n",
       " ['I am old.', 'Sono vecchia.'],\n",
       " ['I ate it.', \"L'ho mangiato.\"],\n",
       " ['I ate it.', \"L'ho mangiata.\"],\n",
       " ['I burped.', 'Ruttai.'],\n",
       " ['I danced.', 'Ballai.'],\n",
       " ['I danced.', 'Danzai.'],\n",
       " ['I failed.', 'Ho fallito.'],\n",
       " ['I gasped.', 'Ho rantolato.'],\n",
       " ['I gasped.', 'Rantolai.'],\n",
       " ['I gasped.', 'Ho ansimato.'],\n",
       " ['I gasped.', 'Ansimai.'],\n",
       " ['I got it.', 'Ho capito.'],\n",
       " ['I got it.', 'Ho compreso.'],\n",
       " ['I got it.', 'Io ho capito.'],\n",
       " ['I got it.', 'Io ho compreso.'],\n",
       " ['I helped.', 'Aiutai.'],\n",
       " ['I jumped.', 'Ho saltato.'],\n",
       " ['I jumped.', 'Saltai.'],\n",
       " ['I looked.', 'Ho guardato.'],\n",
       " ['I looked.', 'Guardai.'],\n",
       " ['I moaned.', 'Ho gemuto.'],\n",
       " ['I moaned.', 'Gemetti.'],\n",
       " ['I moaned.', 'Ho frignato.'],\n",
       " ['I moaned.', 'Frignai.'],\n",
       " ['I moaned.', 'Gemettei.'],\n",
       " ['I nodded.', 'Ho annuito.'],\n",
       " ['I nodded.', 'Annuii.'],\n",
       " ['I obeyed.', 'Ho ubbidito.'],\n",
       " ['I obeyed.', 'Ho obbedito.'],\n",
       " ['I paused.', 'Mi sono fermato.'],\n",
       " ['I paused.', 'Mi sono fermata.'],\n",
       " ['I paused.', 'Mi fermai.'],\n",
       " ['I paused.', 'Mi sono interrotto.'],\n",
       " ['I paused.', 'Mi sono interrotta.'],\n",
       " ['I phoned.', 'Ho telefonato.'],\n",
       " ['I phoned.', 'Io ho telefonato.'],\n",
       " ['I phoned.', 'Telefonai.'],\n",
       " ['I phoned.', 'Io telefonai.'],\n",
       " ['I prayed.', 'Pregai.'],\n",
       " ['I refuse.', 'Mi rifiuto.'],\n",
       " ['I refuse.', 'Io mi rifiuto.'],\n",
       " ['I rested.', 'Mi sono riposato.'],\n",
       " ['I rested.', 'Mi sono riposata.'],\n",
       " ['I rested.', 'Mi riposai.'],\n",
       " ['I shaved.', 'Mi sono rasato.'],\n",
       " ['I shaved.', 'Mi sono rasata.'],\n",
       " ['I shaved.', 'Mi rasai.'],\n",
       " ['I sighed.', 'Ho sospirato.'],\n",
       " ['I sighed.', 'Sospirai.'],\n",
       " ['I smiled.', 'Ho sorriso.'],\n",
       " ['I smiled.', 'Io ho sorriso.'],\n",
       " ['I smiled.', 'Sorrisi.'],\n",
       " ['I smiled.', 'Io sorrisi.'],\n",
       " ['I stayed.', 'Sono rimasto.'],\n",
       " ['I stayed.', 'Io sono rimasto.'],\n",
       " ['I stayed.', 'Sono rimasta.'],\n",
       " ['I stayed.', 'Io sono rimasta.'],\n",
       " ['I stayed.', 'Rimasi.'],\n",
       " ['I stayed.', 'Io rimasi.'],\n",
       " ['I stayed.', 'Restai.'],\n",
       " ['I stayed.', 'Io restai.'],\n",
       " ['I stayed.', 'Sono restato.'],\n",
       " ['I stayed.', 'Io sono restato.'],\n",
       " ['I stayed.', 'Sono restata.'],\n",
       " ['I stayed.', 'Io sono restata.'],\n",
       " ['I talked.', 'Ho parlato.'],\n",
       " ['I talked.', 'Parlai.'],\n",
       " ['I use it.', 'Lo uso.'],\n",
       " ['I use it.', 'La uso.'],\n",
       " ['I use it.', 'Io lo uso.'],\n",
       " ['I use it.', 'Io la uso.'],\n",
       " ['I use it.', 'Lo utilizzo.'],\n",
       " ['I use it.', 'Io lo utilizzo.'],\n",
       " ['I use it.', 'La utilizzo.'],\n",
       " ['I use it.', 'Io la utilizzo.'],\n",
       " ['I waited.', 'Ho aspettato.'],\n",
       " ['I waited.', 'Io ho aspettato.'],\n",
       " ['I waited.', 'Aspettai.'],\n",
       " ['I waited.', 'Io aspettai.'],\n",
       " ['I waited.', 'Aspettavo.'],\n",
       " ['I waited.', 'Io aspettavo.'],\n",
       " ['I winked.', \"Ho fatto l'occhiolino.\"],\n",
       " ['I winked.', \"Feci l'occhiolino.\"],\n",
       " ['I winked.', \"Ho strizzato l'occhio.\"],\n",
       " ['I winked.', \"Strizzai l'occhio.\"],\n",
       " ['I yawned.', 'Ho sbadigliato.'],\n",
       " ['I yawned.', 'Sbadigliai.'],\n",
       " [\"I'll die.\", 'Morirò.'],\n",
       " [\"I'll die.\", 'Io morirò.'],\n",
       " [\"I'll pay.\", 'Pago io.'],\n",
       " [\"I'll pay.\", 'Pagherò io.'],\n",
       " [\"I'll win.\", 'Vincerò.'],\n",
       " [\"I'll win.\", 'Io vincerò.'],\n",
       " [\"I'm a DJ.\", 'Sono un DJ.'],\n",
       " [\"I'm a DJ.\", 'Io sono un DJ.'],\n",
       " [\"I'm a DJ.\", 'Sono una DJ.'],\n",
       " [\"I'm a DJ.\", 'Io sono una DJ.'],\n",
       " [\"I'm back.\", 'Sono tornato.'],\n",
       " [\"I'm back.\", 'Sono ritornato.'],\n",
       " [\"I'm back.\", 'Sono tornata.'],\n",
       " [\"I'm bald.\", 'Sono calvo.'],\n",
       " [\"I'm bald.\", 'Io sono calvo.'],\n",
       " [\"I'm bald.\", 'Sono calva.'],\n",
       " [\"I'm bald.\", 'Io sono calva.'],\n",
       " [\"I'm busy.\", 'Sono occupato.'],\n",
       " [\"I'm busy.\", 'Sono impegnato.'],\n",
       " [\"I'm busy.\", 'Io sono impegnato.'],\n",
       " [\"I'm busy.\", 'Sono impegnata.'],\n",
       " [\"I'm busy.\", 'Io sono impegnata.'],\n",
       " [\"I'm busy.\", 'Io sono occupato.'],\n",
       " [\"I'm busy.\", 'Sono occupata.'],\n",
       " [\"I'm busy.\", 'Io sono occupata.'],\n",
       " [\"I'm calm.\", 'Sono calmo.'],\n",
       " [\"I'm calm.\", 'Io sono calmo.'],\n",
       " [\"I'm calm.\", 'Sono calma.'],\n",
       " [\"I'm calm.\", 'Io sono calma.'],\n",
       " [\"I'm cold.\", 'Ho freddo.'],\n",
       " [\"I'm cool.\", 'Sono figo.'],\n",
       " [\"I'm cool.\", 'Io sono figo.'],\n",
       " [\"I'm cool.\", 'Sono alla moda.'],\n",
       " [\"I'm cool.\", 'Io sono alla moda.'],\n",
       " [\"I'm deaf.\", 'Sono sordo.'],\n",
       " [\"I'm deaf.\", 'Io sono sordo.'],\n",
       " [\"I'm deaf.\", 'Sono sorda.'],\n",
       " [\"I'm deaf.\", 'Io sono sorda.'],\n",
       " [\"I'm done.\", 'Ho finito.'],\n",
       " [\"I'm done.\", 'Io ho finito.'],\n",
       " [\"I'm fast.\", 'Sono veloce.'],\n",
       " [\"I'm fast.\", 'Io sono veloce.'],\n",
       " [\"I'm fine.\", 'Sto bene.'],\n",
       " [\"I'm fine.\", 'Mi sento bene.'],\n",
       " [\"I'm free!\", 'Io sono libero!'],\n",
       " [\"I'm free.\", 'Sono libero.'],\n",
       " [\"I'm free.\", 'Sono libera.'],\n",
       " [\"I'm free.\", 'Sono gratuito.'],\n",
       " [\"I'm free.\", 'Sono gratuita.'],\n",
       " [\"I'm full.\", 'Sono pieno.'],\n",
       " [\"I'm full.\", 'Sono piena.'],\n",
       " [\"I'm full.\", 'Io sono pieno.'],\n",
       " [\"I'm full.\", 'Io sono piena.'],\n",
       " [\"I'm glad.\", 'Sono felice.'],\n",
       " [\"I'm glad.\", 'Io sono felice.'],\n",
       " [\"I'm glad.\", 'Io sono contento.'],\n",
       " [\"I'm glad.\", 'Sono contenta.'],\n",
       " [\"I'm glad.\", 'Io sono contenta.'],\n",
       " [\"I'm here.\", 'Sono qua.'],\n",
       " [\"I'm here.\", 'Sono qui.'],\n",
       " [\"I'm here.\", 'Io sono qui.'],\n",
       " [\"I'm here.\", 'Io sono qua.'],\n",
       " [\"I'm home.\", 'Sono a casa.'],\n",
       " [\"I'm home.\", 'Io sono a casa.'],\n",
       " [\"I'm hurt.\", 'Sono ferito.'],\n",
       " [\"I'm hurt.\", 'Io sono ferito.'],\n",
       " [\"I'm hurt.\", 'Sono ferita.'],\n",
       " [\"I'm hurt.\", 'Io sono ferita.'],\n",
       " [\"I'm late.\", 'Sono in ritardo.'],\n",
       " [\"I'm late.\", 'Io sono in ritardo.'],\n",
       " [\"I'm lazy.\", 'Sono pigro.'],\n",
       " [\"I'm lazy.\", 'Io sono pigro.'],\n",
       " [\"I'm lazy.\", 'Sono pigra.'],\n",
       " [\"I'm lazy.\", 'Io sono pigra.'],\n",
       " [\"I'm lost.\", 'Sono perso.'],\n",
       " [\"I'm lost.\", 'Io sono perso.'],\n",
       " [\"I'm lost.\", 'Sono persa.'],\n",
       " [\"I'm lost.\", 'Io sono persa.'],\n",
       " [\"I'm mean.\", 'Sono meschino.'],\n",
       " [\"I'm mean.\", 'Io sono meschino.'],\n",
       " [\"I'm mean.\", 'Sono meschina.'],\n",
       " [\"I'm mean.\", 'Io sono meschina.'],\n",
       " [\"I'm numb.\", 'Sono insensibile.'],\n",
       " [\"I'm numb.\", 'Io sono insensibile.'],\n",
       " [\"I'm numb.\", 'Sono intorpidito.'],\n",
       " [\"I'm numb.\", 'Io sono intorpidito.'],\n",
       " [\"I'm numb.\", 'Sono intorpidita.'],\n",
       " [\"I'm numb.\", 'Io sono intorpidita.'],\n",
       " [\"I'm poor.\", 'Sono povero.'],\n",
       " [\"I'm poor.\", 'Io sono povero.'],\n",
       " [\"I'm poor.\", 'Io sono povera.'],\n",
       " [\"I'm rich.\", 'Sono ricco.'],\n",
       " [\"I'm rich.\", 'Io sono ricco.'],\n",
       " [\"I'm rich.\", 'Sono ricca.'],\n",
       " [\"I'm rich.\", 'Io sono ricca.'],\n",
       " [\"I'm safe.\", 'Sono al sicuro.'],\n",
       " [\"I'm safe.\", 'Io sono al sicuro.'],\n",
       " [\"I'm sick.\", 'Sono malato.'],\n",
       " [\"I'm sick.\", 'Sono malata.'],\n",
       " [\"I'm sick.\", 'Sto male.'],\n",
       " [\"I'm sick.\", 'Sono ammalato.'],\n",
       " [\"I'm slow.\", 'Sono lento.'],\n",
       " [\"I'm slow.\", 'Io sono lento.'],\n",
       " [\"I'm slow.\", 'Sono lenta.'],\n",
       " [\"I'm slow.\", 'Io sono lenta.'],\n",
       " [\"I'm sure.\", 'Io sono positivo.'],\n",
       " [\"I'm thin.\", 'Sono magro.'],\n",
       " [\"I'm thin.\", 'Io sono magro.'],\n",
       " [\"I'm thin.\", 'Sono magra.'],\n",
       " [\"I'm thin.\", 'Io sono magra.'],\n",
       " [\"I'm tidy.\", 'Sono ordinato.'],\n",
       " [\"I'm tidy.\", 'Io sono ordinato.'],\n",
       " [\"I'm tidy.\", 'Sono ordinata.'],\n",
       " [\"I'm tidy.\", 'Io sono ordinata.'],\n",
       " [\"I'm weak.\", 'Sono debole.'],\n",
       " [\"I'm weak.\", 'Io sono debole.'],\n",
       " [\"I'm wise.\", 'Sono saggio.'],\n",
       " [\"I'm wise.\", 'Io sono saggio.'],\n",
       " [\"I'm wise.\", 'Sono saggia.'],\n",
       " [\"I'm wise.\", 'Io sono saggia.'],\n",
       " ['It helps.', 'Aiuta.'],\n",
       " ['It hurts.', 'Fa male.'],\n",
       " ['It works.', 'Questa funziona.'],\n",
       " [\"It's Tom.\", 'È Tom.'],\n",
       " [\"It's fun.\", 'È divertente.'],\n",
       " [\"It's his.\", 'È suo.'],\n",
       " [\"It's his.\", 'È sua.'],\n",
       " [\"It's hot.\", 'Fa caldo.'],\n",
       " [\"It's hot.\", \"C'è caldo.\"],\n",
       " [\"It's new.\", 'È nuovo.'],\n",
       " [\"It's new.\", 'È nuova.'],\n",
       " [\"It's odd.\", 'È strano.'],\n",
       " [\"It's odd.\", 'È strana.'],\n",
       " [\"It's red.\", 'È rosso.'],\n",
       " [\"It's red.\", 'È rossa.'],\n",
       " [\"It's sad.\", 'È triste.'],\n",
       " ['Keep out.', 'Non entrare.'],\n",
       " ['Kill Tom.', 'Uccidi Tom.'],\n",
       " ['Kill Tom.', 'Uccida Tom.'],\n",
       " ['Kill Tom.', 'Uccidete Tom.'],\n",
       " ['Kiss Tom.', 'Bacia Tom.'],\n",
       " ['Kiss Tom.', 'Baciate Tom.'],\n",
       " ['Kiss Tom.', 'Baci Tom.'],\n",
       " ['Leave it.', 'Lascialo.'],\n",
       " ['Leave it.', 'Lasciala.'],\n",
       " ['Leave it.', 'Lo lasci.'],\n",
       " ['Leave it.', 'La lasci.'],\n",
       " ['Leave it.', 'Lasciatelo.'],\n",
       " ['Leave it.', 'Lasciatela.'],\n",
       " ['Leave me.', 'Lasciami.'],\n",
       " ['Leave me.', 'Lasciatemi.'],\n",
       " ['Leave me.', 'Mi lasci.'],\n",
       " ['Leave us.', 'Lasciaci.'],\n",
       " ['Leave us.', 'Lasciateci.'],\n",
       " ['Leave us.', 'Ci lasci.'],\n",
       " [\"Let's go!\", 'Andiamo!'],\n",
       " [\"Let's go.\", 'Andiamo!'],\n",
       " ['Look out!', 'Attenzione!'],\n",
       " ['Look out!', 'Occhio!'],\n",
       " ['Marry me.', 'Sposami.'],\n",
       " ['Marry me.', 'Sposatemi.'],\n",
       " ['Marry me.', 'Mi sposi.'],\n",
       " ['Save Tom.', 'Salva Tom.'],\n",
       " ['Save Tom.', 'Salvate Tom.'],\n",
       " ['Save Tom.', 'Salvi Tom.'],\n",
       " ['She came.', 'È venuta.'],\n",
       " ['She came.', 'Lei è venuta.'],\n",
       " ['She died.', 'È morta.'],\n",
       " ['She died.', 'Lei è morta.'],\n",
       " ['She runs.', 'Corre.'],\n",
       " ['Sit down!', 'Siediti!'],\n",
       " ['Sit down.', 'Siediti.'],\n",
       " ['Sit down.', 'Si sieda.'],\n",
       " ['Sit down.', 'Sedetevi.'],\n",
       " ['Sit here.', 'Siediti qui.'],\n",
       " ['Sit here.', 'Si sieda qui.'],\n",
       " ['Sit here.', 'Sedetevi qui.'],\n",
       " ['Speak up!', 'Parla più forte!'],\n",
       " ['Speak up!', 'Parlate più forte!'],\n",
       " ['Speak up!', 'Parli più forte!'],\n",
       " ['Stand by.', 'Resta in attesa.'],\n",
       " ['Stand by.', 'Restate in attesa.'],\n",
       " ['Stand by.', 'Resti in attesa.'],\n",
       " ['Stand by.', 'Rimani in attesa.'],\n",
       " ['Stand by.', 'Rimanga in attesa.'],\n",
       " ['Stand by.', 'Rimanete in attesa.'],\n",
       " ['Stand up!', 'In piedi!'],\n",
       " ['Stand up!', 'Alzati in piedi!'],\n",
       " ['Stand up!', 'Alzatevi in piedi!'],\n",
       " ['Stand up!', 'Si alzi in piedi!'],\n",
       " ['Stand up.', 'Alzati.'],\n",
       " ['Stand up.', 'Alzatevi.'],\n",
       " ['Stand up.', 'Si alzi.'],\n",
       " ['Stay put.', 'Stai fermo.'],\n",
       " ['Stay put.', 'Stai ferma.'],\n",
       " ['Stay put.', 'Stia fermo.'],\n",
       " ['Stay put.', 'Stia ferma.'],\n",
       " ['Stay put.', 'State fermi.'],\n",
       " ['Stay put.', 'State ferme.'],\n",
       " ['Stay put.', 'Fermo lì.'],\n",
       " ['Stay put.', 'Ferma lì.'],\n",
       " ['Stay put.', 'Fermi lì.'],\n",
       " ['Stay put.', 'Ferme lì.'],\n",
       " ['Stop Tom.', 'Ferma Tom.'],\n",
       " ['Stop Tom.', 'Fermi Tom.'],\n",
       " ['Stop Tom.', 'Fermate Tom.'],\n",
       " ['Take Tom.', 'Prendi Tom.'],\n",
       " ['Take Tom.', 'Prenda Tom.'],\n",
       " ['Take Tom.', 'Prendete Tom.'],\n",
       " ['Tell Tom.', 'Dillo a Tom.'],\n",
       " ['Tell Tom.', 'Ditelo a Tom.'],\n",
       " ['Tell Tom.', 'Lo dica a Tom.'],\n",
       " ['Terrific!', 'Formidabile!'],\n",
       " ['Tom came.', 'Tom è venuto.'],\n",
       " ['Tom came.', 'Tom venne.'],\n",
       " ['Tom came.', 'È venuto Tom.'],\n",
       " ['Tom came.', 'Venne Tom.'],\n",
       " ['Tom died.', 'Tom è morto.'],\n",
       " ['Tom died.', 'Tom morì.'],\n",
       " ['Tom fell.', 'Tom è caduto.'],\n",
       " ['Tom fell.', 'Tom cadde.'],\n",
       " ['Tom fled.', 'Tom è fuggito.'],\n",
       " ['Tom fled.', 'Tom fuggì'],\n",
       " ['Tom knew.', 'Tom lo sapeva.'],\n",
       " ['Tom left.', 'Tom è partito.'],\n",
       " ['Tom left.', \"Tom se n'è andato.\"],\n",
       " ['Tom left.', 'Tom se ne andò.'],\n",
       " ['Tom left.', 'Tom partì.'],\n",
       " ['Tom lied.', 'Tom ha mentito.'],\n",
       " ['Tom lied.', 'Tom mentì.'],\n",
       " ['Tom lies.', 'Tom mente.'],\n",
       " ['Tom lost.', 'Tom ha perso.'],\n",
       " ['Tom lost.', 'Tom perse.'],\n",
       " ['Tom paid.', 'Tom ha pagato.'],\n",
       " ['Tom paid.', 'Tom pagò.'],\n",
       " ['Tom quit.', 'Tom ha rinunciato.'],\n",
       " ['Tom swam.', 'Tom ha nuotato.'],\n",
       " ['Tom swam.', 'Tom nuotò.'],\n",
       " ['Tom wept.', 'Tom piangeva.'],\n",
       " ['Tom wept.', 'Tom pianse.'],\n",
       " ['Tom wept.', 'Tom ha pianto.'],\n",
       " [\"Tom's up.\", 'Tom è alzato.'],\n",
       " ['Too late.', 'Troppo tardi.'],\n",
       " ['Try hard.', 'Prova duramente.'],\n",
       " ['Try hard.', 'Provate duramente.'],\n",
       " ['Try hard.', 'Provi duramente.'],\n",
       " ['Try some.', \"Provane un po'.\"],\n",
       " ['Try some.', \"Provatene un po'.\"],\n",
       " ['Try some.', \"Ne provi un po'.\"],\n",
       " ['Try this.', 'Prova questo.'],\n",
       " ['Try this.', 'Provate questo.'],\n",
       " ['Try this.', 'Provi questo.'],\n",
       " ['Use this.', 'Usa questo.'],\n",
       " ['Use this.', 'Usa questa.'],\n",
       " ['Use this.', 'Utilizza questo.'],\n",
       " ['Use this.', 'Utilizza questa.'],\n",
       " ['Use this.', 'Usate questo.'],\n",
       " ['Use this.', 'Usate questa.'],\n",
       " ['Use this.', 'Utilizzi questo.'],\n",
       " ['Use this.', 'Utilizzi questa.'],\n",
       " ['Use this.', 'Utilizzate questo.'],\n",
       " ['Use this.', 'Utilizzate questa.'],\n",
       " ['Use this.', 'Usi questo.'],\n",
       " ['Use this.', 'Usi questa.'],\n",
       " ['Warn Tom.', 'Avvisa Tom.'],\n",
       " ['Warn Tom.', 'Avvisate Tom.'],\n",
       " ['Warn Tom.', 'Avvisi Tom.'],\n",
       " ['Warn Tom.', 'Avverti Tom.'],\n",
       " ['Warn Tom.', 'Avverta Tom.'],\n",
       " ['Warn Tom.', 'Avvertite Tom.'],\n",
       " ['Watch me.', 'Guardami.'],\n",
       " ['Watch me.', 'Guardatemi.'],\n",
       " ['Watch me.', 'Mi guardi.'],\n",
       " ['Watch us.', 'Guardaci.'],\n",
       " ['Watch us.', 'Ci guardi.'],\n",
       " ['Watch us.', 'Guardateci.'],\n",
       " ['We agree.', \"Siamo d'accordo.\"],\n",
       " ['We agree.', \"Noi siamo d'accordo.\"],\n",
       " ['We tried.', 'Provavamo.'],\n",
       " ['We tried.', 'Noi provavamo.'],\n",
       " [\"We'll go.\", 'Andremo.'],\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng = to_lines(data)\n",
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqi7V6bvII77"
   },
   "source": [
    "The ItalianEng has the data which we just imported and then ran through a funtion is now converted to the array by using the Python's inbuilt function \"array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "7kECyyIaII78",
    "outputId": "63f446bd-c481-4318-9724-6bc2628fb0a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi.', 'Ciao!'],\n",
       "       ['Run!', 'Corri!'],\n",
       "       ['Run!', 'Corra!'],\n",
       "       ...,\n",
       "       ['If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.',\n",
       "        'Se vuoi sembrare un madrelingua, devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato.'],\n",
       "       [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\",\n",
       "        'Se qualcuno che non conosce il tuo background dice che sembri un madrelingua, significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua. In altre parole, non sembri davvero un madrelingua.'],\n",
       "       ['It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.',\n",
       "        'Può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo. Ciononostante, se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando, potremmo essere in grado di minimizzare gli errori.']],\n",
       "      dtype='<U317')"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng = array(ItalianNEng)\n",
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzHntOD3II7_"
   },
   "source": [
    "**As our dataset is too large, we will take first 50000 to train and test the model, If the CPU and GPU permits we can load the whole dataset**\n",
    "\n",
    "\n",
    "Split the whole dataset to check how model works and it would be better to run the model once for shorter dataset and then to increase the dataset if the CPU and GPU of the system permits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2_MWjS2LII8A",
    "outputId": "bf8c5b24-dd77-4c4e-fc15-0edf04dab25e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi.', 'Ciao!'],\n",
       "       ['Run!', 'Corri!'],\n",
       "       ['Run!', 'Corra!'],\n",
       "       ...,\n",
       "       ['Three were killed.', 'Tre sono stati uccisi.'],\n",
       "       ['Three were killed.', 'Tre furono uccisi.'],\n",
       "       ['Throw me the ball.', 'Lanciami la palla.']], dtype='<U317')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng = ItalianNEng[:50000,:]\n",
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZJAh9qxSII8D",
    "outputId": "0580eaf5-4b99-4dd4-a3a5-2d574b1449df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U317')"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZkeWovxII8G"
   },
   "source": [
    "<h3><a id=\"Data_Processing\">&#9997; Data Processing</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuuHZVIFII8H"
   },
   "source": [
    "**remove all the punctuation and then change the case of every word to lower case**\n",
    "\n",
    "\n",
    "we will remove the punctuation in the below code by going through each line of the data and storing it in the same variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcPk9MbFII8I"
   },
   "outputs": [],
   "source": [
    "ItalianNEng[:,0] = [sent.translate(str.maketrans('', '', string.punctuation)) for sent in ItalianNEng[:,0]]\n",
    "ItalianNEng[:,1] = [sent.translate(str.maketrans('', '', string.punctuation)) for sent in ItalianNEng[:,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZYqsytHII8L"
   },
   "source": [
    "Here all the punctuation is removed/cleaned and the data looks like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "_MDgCiY0II8M",
    "outputId": "9f7cf0e5-e04e-475b-d49d-77466b8c7146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi', 'Ciao'],\n",
       "       ['Run', 'Corri'],\n",
       "       ['Run', 'Corra'],\n",
       "       ...,\n",
       "       ['Three were killed', 'Tre sono stati uccisi'],\n",
       "       ['Three were killed', 'Tre furono uccisi'],\n",
       "       ['Throw me the ball', 'Lanciami la palla']], dtype='<U317')"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onunsmZ4II8P"
   },
   "source": [
    "Now we will convert the data into lower case by using python's inbuilt function lower() and save the data to its original variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxM3GbjLII8Q"
   },
   "outputs": [],
   "source": [
    "for i in range(len(ItalianNEng)):\n",
    "    ItalianNEng[i,0] = ItalianNEng[i,0].lower()\n",
    "    \n",
    "    ItalianNEng[i,1] = ItalianNEng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "IhbkrE8LII8S",
    "outputId": "548429fb-5a7d-47d5-8b3e-ea61d1920e38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'ciao'],\n",
       "       ['run', 'corri'],\n",
       "       ['run', 'corra'],\n",
       "       ...,\n",
       "       ['three were killed', 'tre sono stati uccisi'],\n",
       "       ['three were killed', 'tre furono uccisi'],\n",
       "       ['throw me the ball', 'lanciami la palla']], dtype='<U317')"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1UJkawJII8W"
   },
   "source": [
    "**Split the Italian sentence and English sentence and check the length of the words in an sentence and store in the list**\n",
    "\n",
    "To run the seq2Seq model on the data, we need to have both input and output of the same length, so to do that we will create two empty arrays where the length of the input and output is stored for the padding and tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_006x6MGII8X"
   },
   "outputs": [],
   "source": [
    "eng_length = []\n",
    "ita_length = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in ItalianNEng[:,0]:\n",
    "    eng_length.append(len(i.split()))\n",
    "\n",
    "for i in ItalianNEng[:,1]:\n",
    "    ita_length.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "0wWVkdHyII8b",
    "outputId": "cee51e23-debe-413d-841e-2d61a3828f4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "32iNXVcbII8g",
    "outputId": "27d0c7b8-e1f1-48e0-a9af-9c13f65504ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRlYCOsCII8j"
   },
   "source": [
    "The below graph shows the distribution of the length of the data, which will help us to decide on how much length we will be passing to the model and which data will be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "vzuK0OlhII8k",
    "outputId": "3bd29cfb-248a-42ce-a3ff-571a7b4258b6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBV8aEm2TpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_dframe = pd.DataFrame({'Enligh':eng_length, 'Italian':ita_length})\n",
    "length_dframe.hist(bins = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTQ4PWowII8n"
   },
   "source": [
    "<h3><a id=\"Tokenization\">&#9997; Tokenization</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PiMY4hkII8o"
   },
   "source": [
    "**We will create a function which will tokenize our data with the help of the Keras's tokenization function which we have imported at the beginning of this notebook**\n",
    "\n",
    "Below is the function which will help us to vectorize the data by using Keras's tokenization function. Basically Tokenization will convert the text into sentences which will be well read by the model and it will be easier to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ju_J23IiII8o"
   },
   "outputs": [],
   "source": [
    "def tokenization(sentences):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC02SX1sII8q"
   },
   "source": [
    "Here we will tokenize the english word using the funtion we wrote above and then we will check the total English Vocabulary size after tokenizing --> here we are adding the additional one to the size as the tokenizer initiates from zero\n",
    "\n",
    "and the length of the English data is set to 5 because as per the above graph we see that maximum length of English data is 5 and 6 is the maximum length of Italian data, so to make it equal we will take length as **5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SQmio3OQII8r",
    "outputId": "a7f04b70-43d9-4cf3-c707-e3ba7b28e223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size in the dataset is: 4214\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "English_tokenizer = tokenization(ItalianNEng[:, 0])\n",
    "English_Vocab_Size = len(English_tokenizer.word_index) + 1\n",
    "\n",
    "Eng_length = 5\n",
    "print('English Vocabulary Size in the dataset is: %d' % English_Vocab_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5W_IVfDII8v"
   },
   "source": [
    "Here we will tokenize the Italian word using the funtion we wrote above and then we will check the total Italian Vocabulary size after tokenizing --> here we are adding the additional one to the size as the tokenizer initiates from zero\n",
    "\n",
    "and the length of the Italian data is set to 5 because as per the above graph we see that maximum length of English data is 5 and 6 is the maximum length of Italian data, so to make it equal we will take length as **5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "njOMqdOwII8v",
    "outputId": "009e50b9-69ba-4a21-9a2a-06c2e2f91275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian Vocabulary Size in the dataset is: 9614\n"
     ]
    }
   ],
   "source": [
    "Italian_tokenizer = tokenization(ItalianNEng[:, 1])\n",
    "Italian_Vocab_Size = len(Italian_tokenizer.word_index) + 1\n",
    "\n",
    "Ita_length = 5\n",
    "print('Italian Vocabulary Size in the dataset is: %d' % Italian_Vocab_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkdOx3pPII8y"
   },
   "source": [
    "**The values must be encoded from text to sequences of number and padded the sequence using Keras's function**\n",
    "\n",
    "The below function will help to build the sequences and also perform sequence padding to the data if the data length of the sentence is less than the maximum length defined which is **5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QH0ul6J2II8z"
   },
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    sequences = pad_sequences(sequences, maxlen=length, padding='post')\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PR32BneII81"
   },
   "source": [
    "**split the data into training data and test data by using Sklearn.Model_selection's train_test_split function**\n",
    "\n",
    "\n",
    "Split the whole data which we saved in the variable ItalianNEng as 80% to train and 20% to the test. Once the data is split we will run it through the encode sequence which will create sequences of the text and then performs padding sequences on the data whose length is less than the defined maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meHIUmaEII82"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(ItalianNEng, test_size=0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lqLEqNXbII85",
    "outputId": "e90a201f-16e9-409c-f2dc-72c897916850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DAhq4iAXII88",
    "outputId": "8a63aad6-9b5d-4dd7-939e-e5de22d3ab37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVGef0-5II8_"
   },
   "outputs": [],
   "source": [
    "x_train = encode_sequences(Italian_tokenizer, Ita_length, train[:, 1])\n",
    "y_train = encode_sequences(English_tokenizer, Eng_length, train[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V97RR4nzII9B"
   },
   "source": [
    "So every sentence is converted into 5 digit sequence which we will be feeding it to the model and if you see there are 0s added to the array where the length of the data is less than that of the maximum defined length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "wQyUwFrJII9B",
    "outputId": "cbf87db7-caf6-4d03-daa0-8e4111d96938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,   32,   14,    0,    0],\n",
       "       [   1,   24,    2, 2237,    0],\n",
       "       [   2,   10,  864,    0,    0],\n",
       "       ...,\n",
       "       [   1,    2,  896,    0,    0],\n",
       "       [   1,    2, 2192,    0,    0],\n",
       "       [  67,  207,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Xe1sTQMrII9E",
    "outputId": "0222d63d-1c1a-4016-8347-b15e110561e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  49,   99,    3,    0,    0],\n",
       "       [   2,  399,  453,    0,    0],\n",
       "       [  17,    3,   37,    7, 1021],\n",
       "       ...,\n",
       "       [   2,    5,  850,    0,    0],\n",
       "       [   2,   80, 2506,    0,    0],\n",
       "       [ 624,    6,  138,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_F14fe9SII9H"
   },
   "outputs": [],
   "source": [
    "x_test = encode_sequences(Italian_tokenizer, Ita_length, test[:, 1])\n",
    "y_test = encode_sequences(English_tokenizer, Eng_length, test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "cG7fapLyII9J",
    "outputId": "e1764e68-0ff9-4b7e-8e70-60bb7b249111"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  41,   17,   85,  120,    0],\n",
       "       [   4,  283,  204,    0,    0],\n",
       "       [  45,  840,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 489,    6,  943,    0,    0],\n",
       "       [   1,    2, 9091,    0,    0],\n",
       "       [ 404,   19,   50,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "UYhP6mvXII9L",
    "outputId": "169af163-720b-469f-9453-f5f233808019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  26,    8,  159,    0,    0],\n",
       "       [   4,  351,  105,    0,    0],\n",
       "       [   1,  329,    3,    0,    0],\n",
       "       ...,\n",
       "       [ 102, 1042,    0,    0,    0],\n",
       "       [   2,    5, 2137,    0,    0],\n",
       "       [   1,   68,   42,    6,    0]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fUkGnV7II9O"
   },
   "source": [
    "<h3><a id=\"Build_The_Model\">&#9997; Build the Model</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSKsO6a7II9O"
   },
   "source": [
    "## LSTM Model\n",
    "\n",
    "**Build the Seq2Seq LSTM model**\n",
    "\n",
    "Build the model which will take the encoded training dataset and once trained will predict the data which will be in the array format can be decoded back to text format by running it throught the function\n",
    "\n",
    "So the model architecture will be sequential which has encoder and decoder in it, Model would be sequential, embedding with LSTM layer and at the end it will be LSTM followed by the Dense layer which will act as a decoder for thr project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bgyd4_IwII9P"
   },
   "outputs": [],
   "source": [
    "def build_model(ItalianVocabSize, EnglishVocabSize, ItalianLength, EnglishLength, Units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(ItalianVocabSize, Units, input_length=ItalianLength, mask_zero=True))\n",
    "    model.add(LSTM(Units))\n",
    "    model.add(RepeatVector(EnglishLength))\n",
    "    model.add(LSTM(Units, return_sequences=True))\n",
    "    model.add(Dense(EnglishVocabSize, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMYgj7sPII9R"
   },
   "source": [
    "Build the model by passing the parameters and save the model and below is the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "1DG5v1t6II9S",
    "outputId": "04b0cf33-cd9c-42dd-8051-cf29bf8e2148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 512)            4922368   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5, 4214)           2161782   \n",
      "=================================================================\n",
      "Total params: 11,282,550\n",
      "Trainable params: 11,282,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(Italian_Vocab_Size, English_Vocab_Size, Ita_length, Eng_length, 512 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUNNZIlaII9W"
   },
   "source": [
    "We are using RMSProp as the optimizer for the model and sparse_categorical_crossentropy as the loss function.\n",
    "\n",
    "we can play around with these to see which is better suitable for our model, but to train the model we will be going with the above mentoned optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0FdJNuNII9W"
   },
   "outputs": [],
   "source": [
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX3W5kGZII9Y"
   },
   "source": [
    "<h3><a id=\"Model_Train\">&#9997; Train the Model</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-YcjU0rII9a"
   },
   "source": [
    "\n",
    "we will train the model by passing the training data which we split and passed it through the funtion. we will be saving out model on each iteration of the epochs \n",
    "\n",
    "We will be running this model with **35 Epochs**, **512 batch size**\n",
    "\n",
    "we will play around with the batch size and number of epochs to see the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2536
    },
    "colab_type": "code",
    "id": "MahNyebqII9c",
    "outputId": "0e689ded-00ab-4654-b41f-efa36c8599a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/35\n",
      "32000/32000 [==============================] - 7s 217us/step - loss: 4.3635 - val_loss: 3.7206\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.72064, saving model to SavedModel_Checkpoint\n",
      "Epoch 2/35\n",
      "32000/32000 [==============================] - 4s 136us/step - loss: 3.6058 - val_loss: 3.5450\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.72064 to 3.54504, saving model to SavedModel_Checkpoint\n",
      "Epoch 3/35\n",
      "32000/32000 [==============================] - 4s 138us/step - loss: 3.3634 - val_loss: 3.3228\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.54504 to 3.32282, saving model to SavedModel_Checkpoint\n",
      "Epoch 4/35\n",
      "32000/32000 [==============================] - 4s 137us/step - loss: 3.1375 - val_loss: 3.1099\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.32282 to 3.10991, saving model to SavedModel_Checkpoint\n",
      "Epoch 5/35\n",
      "32000/32000 [==============================] - 4s 136us/step - loss: 2.8838 - val_loss: 2.8779\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.10991 to 2.87792, saving model to SavedModel_Checkpoint\n",
      "Epoch 6/35\n",
      "32000/32000 [==============================] - 4s 136us/step - loss: 2.6236 - val_loss: 2.6814\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.87792 to 2.68142, saving model to SavedModel_Checkpoint\n",
      "Epoch 7/35\n",
      "32000/32000 [==============================] - 4s 135us/step - loss: 2.3904 - val_loss: 2.5064\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.68142 to 2.50638, saving model to SavedModel_Checkpoint\n",
      "Epoch 8/35\n",
      "32000/32000 [==============================] - 4s 134us/step - loss: 2.1777 - val_loss: 2.3174\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.50638 to 2.31738, saving model to SavedModel_Checkpoint\n",
      "Epoch 9/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 1.9850 - val_loss: 2.1828\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.31738 to 2.18279, saving model to SavedModel_Checkpoint\n",
      "Epoch 10/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 1.8110 - val_loss: 2.0561\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.18279 to 2.05607, saving model to SavedModel_Checkpoint\n",
      "Epoch 11/35\n",
      "32000/32000 [==============================] - 4s 135us/step - loss: 1.6558 - val_loss: 1.9410\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.05607 to 1.94102, saving model to SavedModel_Checkpoint\n",
      "Epoch 12/35\n",
      "32000/32000 [==============================] - 4s 135us/step - loss: 1.5125 - val_loss: 1.8503\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.94102 to 1.85035, saving model to SavedModel_Checkpoint\n",
      "Epoch 13/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 1.3805 - val_loss: 1.7538\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.85035 to 1.75377, saving model to SavedModel_Checkpoint\n",
      "Epoch 14/35\n",
      "32000/32000 [==============================] - 4s 131us/step - loss: 1.2581 - val_loss: 1.6841\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.75377 to 1.68413, saving model to SavedModel_Checkpoint\n",
      "Epoch 15/35\n",
      "32000/32000 [==============================] - 4s 132us/step - loss: 1.1449 - val_loss: 1.6055\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.68413 to 1.60549, saving model to SavedModel_Checkpoint\n",
      "Epoch 16/35\n",
      "32000/32000 [==============================] - 4s 131us/step - loss: 1.0401 - val_loss: 1.5300\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.60549 to 1.53004, saving model to SavedModel_Checkpoint\n",
      "Epoch 17/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.9448 - val_loss: 1.4609\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.53004 to 1.46092, saving model to SavedModel_Checkpoint\n",
      "Epoch 18/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.8558 - val_loss: 1.4215\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.46092 to 1.42154, saving model to SavedModel_Checkpoint\n",
      "Epoch 19/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.7754 - val_loss: 1.3679\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.42154 to 1.36785, saving model to SavedModel_Checkpoint\n",
      "Epoch 20/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.7028 - val_loss: 1.3101\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.36785 to 1.31009, saving model to SavedModel_Checkpoint\n",
      "Epoch 21/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.6368 - val_loss: 1.2804\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.31009 to 1.28037, saving model to SavedModel_Checkpoint\n",
      "Epoch 22/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.5743 - val_loss: 1.2448\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.28037 to 1.24480, saving model to SavedModel_Checkpoint\n",
      "Epoch 23/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.5208 - val_loss: 1.2081\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.24480 to 1.20807, saving model to SavedModel_Checkpoint\n",
      "Epoch 24/35\n",
      "32000/32000 [==============================] - 4s 134us/step - loss: 0.4702 - val_loss: 1.1964\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.20807 to 1.19635, saving model to SavedModel_Checkpoint\n",
      "Epoch 25/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.4273 - val_loss: 1.1570\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.19635 to 1.15697, saving model to SavedModel_Checkpoint\n",
      "Epoch 26/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.3888 - val_loss: 1.1557\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.15697 to 1.15572, saving model to SavedModel_Checkpoint\n",
      "Epoch 27/35\n",
      "32000/32000 [==============================] - 4s 134us/step - loss: 0.3518 - val_loss: 1.1256\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.15572 to 1.12559, saving model to SavedModel_Checkpoint\n",
      "Epoch 28/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.3226 - val_loss: 1.1128\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.12559 to 1.11277, saving model to SavedModel_Checkpoint\n",
      "Epoch 29/35\n",
      "32000/32000 [==============================] - 4s 134us/step - loss: 0.2941 - val_loss: 1.0961\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.11277 to 1.09609, saving model to SavedModel_Checkpoint\n",
      "Epoch 30/35\n",
      "32000/32000 [==============================] - 4s 137us/step - loss: 0.2695 - val_loss: 1.1046\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.09609\n",
      "Epoch 31/35\n",
      "32000/32000 [==============================] - 4s 136us/step - loss: 0.2478 - val_loss: 1.0787\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.09609 to 1.07869, saving model to SavedModel_Checkpoint\n",
      "Epoch 32/35\n",
      "32000/32000 [==============================] - 4s 134us/step - loss: 0.2288 - val_loss: 1.0722\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.07869 to 1.07217, saving model to SavedModel_Checkpoint\n",
      "Epoch 33/35\n",
      "32000/32000 [==============================] - 4s 132us/step - loss: 0.2126 - val_loss: 1.0755\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.07217\n",
      "Epoch 34/35\n",
      "32000/32000 [==============================] - 4s 133us/step - loss: 0.1983 - val_loss: 1.0766\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.07217\n",
      "Epoch 35/35\n",
      "32000/32000 [==============================] - 4s 132us/step - loss: 0.1853 - val_loss: 1.0852\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.07217\n"
     ]
    }
   ],
   "source": [
    "filename = 'SavedModel_Checkpoint'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(x_train, y_train.reshape(y_train.shape[0], y_train.shape[1], 1), \n",
    "          epochs=35, batch_size=512,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTawSyMVII9e"
   },
   "source": [
    "**plotting the graph** Once the model is built and we also capturing the model loss and the validation loss, we can go ahead and see how the loss is affected by the each iteration\n",
    "to see that we will plot the graph which will show us the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "aJ_5AwULII9f",
    "outputId": "2199088e-2b1b-4886-eb8f-16a3cbae92d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f91ca26d198>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'],'r')\n",
    "plt.plot(history.history['val_loss'],'g')\n",
    "plt.xticks(np.arange(0, 36, 1))\n",
    "plt.rcParams['figure.figsize'] = (32, 18)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train','validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aq2GeQjXII9h"
   },
   "source": [
    "By looking at the graph we can say that the validation loss stopped dropping after **25 epochs** or the loss dropping very slowly after 25 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxKdX03dII9i"
   },
   "source": [
    "<h3><a id=\"Prediction\">&#9997; Prediction</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7qNK4bdwII9j"
   },
   "source": [
    "**Predicting the output**\n",
    "\n",
    "As the model is now built, we will pass the testing data and see how the built model predicts.\n",
    "\n",
    "and as the predicted data will be in the form of array we need to convert the sequence back to word which can be done by the function \"get_word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIVBFmoXII9j"
   },
   "outputs": [],
   "source": [
    "preds = history.model.predict_classes(x_test.reshape((x_test.shape[0],x_test.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-3CP6HCnII9m",
    "outputId": "7eb1db23-1e4e-408f-e06c-925753283cbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26,   8, 159,   0,   0])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "voYVtYO7II9p"
   },
   "source": [
    "The below function will return the text when we pass the tokenized array and as we will store the predicted text into our empty array \"predicted_text\" where we will append all the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWeZ8pNqII9p"
   },
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "      for word, index in tokenizer.word_index.items():\n",
    "          if index == n:\n",
    "              return word\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-V93njQII9q"
   },
   "outputs": [],
   "source": [
    "predicted_text = []\n",
    "for i in preds:\n",
    "       temp = []\n",
    "       for j in range(len(i)):\n",
    "            t = get_word(i[j], English_tokenizer)\n",
    "            if j > 0:\n",
    "                if (t == get_word(i[j-1], English_tokenizer)) or (t == None):\n",
    "                     temp.append('')\n",
    "                else:\n",
    "                     temp.append(t)\n",
    "            else:\n",
    "                   if(t == None):\n",
    "                          temp.append('')\n",
    "                   else:\n",
    "                          temp.append(t) \n",
    "\n",
    "       predicted_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_Gf8cSmII9t"
   },
   "source": [
    "Prediction is done, predicted output is converted back to text format and now we will check how the predicted format looks like, we will pass it through the pandas dataframe  and see the Italian data with Actual data and the predicted data side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHwBp8tKII9v"
   },
   "outputs": [],
   "source": [
    "pred_dframe = pd.DataFrame({'Italian' : test[:,1] ,'actual' : test[:,0], 'predicted' : predicted_text})\n",
    "#pred_df = pd.DataFrame({'Italian' : test[:,1] ,'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9Q2wK9yII9w"
   },
   "source": [
    "We will see how the translation is done, so to check we will check the first 50 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1599
    },
    "colab_type": "code",
    "id": "vUj6PbzVII9w",
    "outputId": "f3e74c86-1d8e-407b-da55-a269d1f76331"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Italian</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ora sei al sicuro</td>\n",
       "      <td>now youre safe</td>\n",
       "      <td>now youre safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sono sempre pronta</td>\n",
       "      <td>im always ready</td>\n",
       "      <td>im always ready</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ti amavo</td>\n",
       "      <td>i loved you</td>\n",
       "      <td>i loved you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tom lha visto</td>\n",
       "      <td>tom saw it</td>\n",
       "      <td>tom saw see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lui non ha detto niente</td>\n",
       "      <td>he said nothing</td>\n",
       "      <td>he said nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qualcuno lha visto</td>\n",
       "      <td>somebody saw you</td>\n",
       "      <td>somebody saw you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tom sta parlando</td>\n",
       "      <td>tom is talking</td>\n",
       "      <td>tom is speaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>state a guardare</td>\n",
       "      <td>stay and watch</td>\n",
       "      <td>stay and watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spegnetelo</td>\n",
       "      <td>switch it off</td>\n",
       "      <td>turn it off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sono sistematico</td>\n",
       "      <td>im methodical</td>\n",
       "      <td>im dead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>è eccitata</td>\n",
       "      <td>are you excited</td>\n",
       "      <td>are you excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ero impegnata</td>\n",
       "      <td>i was busy</td>\n",
       "      <td>i was busy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>io vedo il cane</td>\n",
       "      <td>i see the dog</td>\n",
       "      <td>i see the pen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tom fuggì</td>\n",
       "      <td>did tom escape</td>\n",
       "      <td>tom escaped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mio padre è dentro</td>\n",
       "      <td>my father is in</td>\n",
       "      <td>my father is tall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>io sono di tokyo</td>\n",
       "      <td>im from tokyo</td>\n",
       "      <td>im from tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tom è sporco</td>\n",
       "      <td>toms filthy</td>\n",
       "      <td>tom is dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>porti aiuto</td>\n",
       "      <td>bring help</td>\n",
       "      <td>bring help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tom mi salvò</td>\n",
       "      <td>tom rescued me</td>\n",
       "      <td>tom ate me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tom lha vista</td>\n",
       "      <td>tom saw you</td>\n",
       "      <td>tom saw you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>farei meglio ad andare adesso</td>\n",
       "      <td>id better go now</td>\n",
       "      <td>id better go now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tom è rilassato</td>\n",
       "      <td>tom is relaxed</td>\n",
       "      <td>tom is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stai tranquilla</td>\n",
       "      <td>stay quiet</td>\n",
       "      <td>keep quiet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tom è in preda al panico</td>\n",
       "      <td>toms panicking</td>\n",
       "      <td>toms panicked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>perse un libro</td>\n",
       "      <td>she lost a book</td>\n",
       "      <td>he lost a book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>voi siete generosi</td>\n",
       "      <td>are you generous</td>\n",
       "      <td>youre generous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>chiami lfbi</td>\n",
       "      <td>call the fbi</td>\n",
       "      <td>call the fbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sono orripilato</td>\n",
       "      <td>im horrified</td>\n",
       "      <td>im unusual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tom non fece nulla</td>\n",
       "      <td>tom did nothing</td>\n",
       "      <td>tom did nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tante grazie</td>\n",
       "      <td>many thanks</td>\n",
       "      <td>many thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tom è il suo capo</td>\n",
       "      <td>tom is your boss</td>\n",
       "      <td>tom is your boss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>mi fa male il polso</td>\n",
       "      <td>my wrist hurts</td>\n",
       "      <td>my jaw is sore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lavorerò</td>\n",
       "      <td>i will work</td>\n",
       "      <td>ill going  work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ho una borsa di ghiaccio</td>\n",
       "      <td>i have an ice bag</td>\n",
       "      <td>i have some coat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>smettila di sparare</td>\n",
       "      <td>stop shooting</td>\n",
       "      <td>stop shooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ero fuori</td>\n",
       "      <td>i was outside</td>\n",
       "      <td>i was outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ti rispettavo</td>\n",
       "      <td>i respected you</td>\n",
       "      <td>i respected you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tu mangi troppo</td>\n",
       "      <td>you eat too much</td>\n",
       "      <td>you eat too much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>dobbiamo continuare</td>\n",
       "      <td>we must continue</td>\n",
       "      <td>we must act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>siete rilassate</td>\n",
       "      <td>youre relaxed</td>\n",
       "      <td>youre relaxed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>loro hanno bisogno di questo</td>\n",
       "      <td>they need this</td>\n",
       "      <td>they need this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>lui uscì dai gangheri</td>\n",
       "      <td>he went ballistic</td>\n",
       "      <td>he went ballistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>lui andò via</td>\n",
       "      <td>he walked away</td>\n",
       "      <td>he went away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tom è senza paura</td>\n",
       "      <td>toms fearless</td>\n",
       "      <td>tom is perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>perché studi</td>\n",
       "      <td>why do you study</td>\n",
       "      <td>why do you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>sembri perfetta</td>\n",
       "      <td>you look perfect</td>\n",
       "      <td>you seem perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ama il caffè</td>\n",
       "      <td>she loves coffee</td>\n",
       "      <td>she loves coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>mi spararono</td>\n",
       "      <td>i got shot</td>\n",
       "      <td>i shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>possiamo aiutarvi</td>\n",
       "      <td>can we help you</td>\n",
       "      <td>we can help you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>non parlate con tom</td>\n",
       "      <td>dont talk to tom</td>\n",
       "      <td>dont talk to tom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Italian             actual            predicted\n",
       "0               ora sei al sicuro     now youre safe     now youre safe  \n",
       "1              sono sempre pronta    im always ready    im always ready  \n",
       "2                        ti amavo        i loved you        i loved you  \n",
       "3                   tom lha visto         tom saw it        tom saw see  \n",
       "4         lui non ha detto niente    he said nothing    he said nothing  \n",
       "5              qualcuno lha visto   somebody saw you   somebody saw you  \n",
       "6                tom sta parlando     tom is talking    tom is speaking  \n",
       "7                state a guardare     stay and watch     stay and watch  \n",
       "8                      spegnetelo      switch it off        turn it off  \n",
       "9                sono sistematico      im methodical           im dead   \n",
       "10                     è eccitata    are you excited    are you excited  \n",
       "11                  ero impegnata         i was busy         i was busy  \n",
       "12                io vedo il cane      i see the dog       i see the pen \n",
       "13                      tom fuggì     did tom escape       tom escaped   \n",
       "14             mio padre è dentro    my father is in   my father is tall \n",
       "15               io sono di tokyo      im from tokyo      im from tokyo  \n",
       "16                   tom è sporco        toms filthy       tom is dirty  \n",
       "17                    porti aiuto         bring help        bring help   \n",
       "18                   tom mi salvò     tom rescued me         tom ate me  \n",
       "19                  tom lha vista        tom saw you        tom saw you  \n",
       "20  farei meglio ad andare adesso   id better go now    id better go now \n",
       "21                tom è rilassato     tom is relaxed            tom is   \n",
       "22                stai tranquilla         stay quiet        keep quiet   \n",
       "23       tom è in preda al panico     toms panicking     toms panicked   \n",
       "24                 perse un libro    she lost a book      he lost a book \n",
       "25             voi siete generosi   are you generous    youre generous   \n",
       "26                    chiami lfbi       call the fbi       call the fbi  \n",
       "27                sono orripilato       im horrified        im unusual   \n",
       "28             tom non fece nulla    tom did nothing    tom did nothing  \n",
       "29                   tante grazie        many thanks       many thanks   \n",
       "30              tom è il suo capo   tom is your boss    tom is your boss \n",
       "31            mi fa male il polso     my wrist hurts      my jaw is sore \n",
       "32                       lavorerò        i will work     ill going  work \n",
       "33       ho una borsa di ghiaccio  i have an ice bag    i have some coat \n",
       "34            smettila di sparare      stop shooting     stop shooting   \n",
       "35                      ero fuori      i was outside      i was outside  \n",
       "36                  ti rispettavo    i respected you    i respected you  \n",
       "37                tu mangi troppo   you eat too much    you eat too much \n",
       "38            dobbiamo continuare   we must continue        we must act  \n",
       "39                siete rilassate      youre relaxed     youre relaxed   \n",
       "40   loro hanno bisogno di questo     they need this     they need this  \n",
       "41          lui uscì dai gangheri  he went ballistic  he went ballistic  \n",
       "42                   lui andò via     he walked away       he went away  \n",
       "43              tom è senza paura      toms fearless     tom is perfect  \n",
       "44                   perché studi   why do you study         why do you  \n",
       "45                sembri perfetta   you look perfect   you seem perfect  \n",
       "46                   ama il caffè   she loves coffee   she loves coffee  \n",
       "47                   mi spararono         i got shot            i shot   \n",
       "48              possiamo aiutarvi    can we help you     we can help you \n",
       "49            non parlate con tom   dont talk to tom    dont talk to tom "
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dframe.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1MJ4_1lII9y"
   },
   "source": [
    "<h3><a id=\"Evaluation\">&#9997; Evaluation</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QP5mu5TwII9z"
   },
   "source": [
    "\n",
    "Bilingual Evaluation Understudy Score (BLEU). The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n",
    "\n",
    "we will build the function which will makes a list of the actual output with the predicted output and then evaluates using the corpus_bleu which is a function of the nltk library \"nltk.translate.bleu_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbnEYCrJII9z"
   },
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = get_word(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofr1FST2II91"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, English_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU: %f' % corpus_bleu(actual, predicted, weights=(1, 0, 0, 0)))\n",
    "    #print('BLEU: %f' % sentence_bleu(actual, predicted))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gRck7f1DII92",
    "outputId": "8c011b85-5c25-4460-fb24-1fd4a0e431f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[è per lei], target=[thats for you], predicted=[thats for you]\n",
      "src=[tom si è addormentato], target=[tom fell asleep], predicted=[tom fell asleep]\n",
      "src=[è di fretta], target=[are you in a rush], predicted=[are you in a rush]\n",
      "src=[loro sono miei], target=[theyre mine], predicted=[theyre mine]\n",
      "src=[loro lo amano], target=[they love that], predicted=[they love that]\n",
      "src=[noi vogliamo degli impieghi], target=[we want jobs], predicted=[we want jobs]\n",
      "src=[siete energiche], target=[are you proactive], predicted=[youre energetic]\n",
      "src=[tom lavora qui], target=[tom works here], predicted=[tom works here]\n",
      "src=[mi dia una scelta], target=[give me a choice], predicted=[give me a choice]\n",
      "src=[tom fiutò], target=[tom sniffed], predicted=[tom sniffed]\n",
      "BLEU: 0.116642\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, English_tokenizer, x_train[:50,:], train[:50,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tARGivBwII95"
   },
   "source": [
    "The evalution of the prediction gives us the corpus score of the 0.11 which can be increased by playing aroud with the data size, batch size, optimizers, loss, and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2YzuBKSII96"
   },
   "source": [
    "<h3><a id=\"Conclusion\">&#9997; Conclusion</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sM39h59QII96"
   },
   "source": [
    "The built model was able to translate the Italian words to the English most of the time and by this research I can conclude that GRU with attention was able to translate comparatively better than the other model, but the training time was high for the GRU.\n",
    "\n",
    "![test44](Result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTjnw4Z4II97"
   },
   "source": [
    "<h3><a id=\"Contribution\">&#9997; Contribution</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TlSAe5coII9-"
   },
   "source": [
    "After reviewing a lot of references sites on the machine learning sites namely \"medium\", \"analyticsVidhya\" and various tutoriols on the LSTM and GRU, I have understood how the RNN and its enhanced Gated network works and implemented in this notebook. all the references which I have used to do this research project is all mentioned in the references below and research paper is also included which explains this research in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6LyMZfcII9_"
   },
   "source": [
    "<h3><a id=\"References\">&#9997; References</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HMzuZhMII9_"
   },
   "source": [
    "\n",
    "\n",
    "**o** https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa\n",
    "\n",
    "**o** https://www.analyticsvidhya.com/blog/2018/03/microsofts-claims-language-translation-ai-reached-human-levels-accuracy/\n",
    "\n",
    "**o** https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=nRBnh4qbPHI&vl=en\n",
    "\n",
    "**o** http://www.manythings.org/anki/\n",
    "\n",
    "**o** https://machinelearningmastery.com/introduction-neural-machine-translation/\n",
    "\n",
    "**o** https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=vI2Y3I-JI2Q\n",
    "\n",
    "**o** https://towardsdatascience.com/neural-machine-translator-with-less-than-50-lines-of-code-guide-1fe4fdfe6292\n",
    "\n",
    "**o** https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/\n",
    "\n",
    "**o** https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nP5KqcEzII9_"
   },
   "source": [
    "<h3><a id=\"License\">&#9997; License</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vd4rjo4sII-A"
   },
   "source": [
    "**MIT License**\n",
    "\n",
    "Copyright 2018 Chetan M Jadhav\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yEtNVlNII-A"
   },
   "source": [
    "**The text in the document by Chetan M Jadhav is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/ **"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LSTM_50000, 30 Epochs.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
