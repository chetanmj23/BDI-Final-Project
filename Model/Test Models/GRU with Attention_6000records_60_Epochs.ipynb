{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DCtbqJRKU4Y"
   },
   "source": [
    "# Italian to English Language Translation (NMT) using GRU & Attention\n",
    "\n",
    "**Import All the neccessary Libraries which we will be requiring to run this notebook and the project**\n",
    "\n",
    "This is seq2seq model by using GRU and Attention. let us import all the necessary libraries that is needed to run this notebook\n",
    "\n",
    "the tensorflow GPU version must be updated before running this notebook and eager_execution of the tensorflow should be enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UI8yE-5lKU4Z",
    "outputId": "ea2f3543-b062-4730-97b7-43171302de13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import array, argmax, random, take\n",
    "import time\n",
    "import pandas as pd\n",
    "import string\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xHlI_PMKU4c"
   },
   "source": [
    "Creating a function which will read the file, encode it and save it and then the funtion to_lines wil  split the data into Italian & English part seperately by '\\n' and build it in the form of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26JOk16NKU4d"
   },
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "9UEU4t-VqxOL",
    "outputId": "fdcd5209-4af2-4fca-ef03-ea7ce00340a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-289cfdd7-8a38-4401-afd5-cadcb0700e9f\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-289cfdd7-8a38-4401-afd5-cadcb0700e9f\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ita.txt to ita.txt\n"
     ]
    }
   ],
   "source": [
    "   from google.colab import files\n",
    "\n",
    "   uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NLmiq6rGuC-q",
    "outputId": "8119f11f-d197-4558-af17-27ad9dc35665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User uploaded file \"ita.txt\" with length 19184787 bytes\n"
     ]
    }
   ],
   "source": [
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syMEwtZhKU4f"
   },
   "source": [
    "By using the function we wrote the text file is imported to the jupyter notebook. \n",
    "\n",
    "Once the notebook is opened and read using the function Read_TextFile we will pass it to the to_lines to split and to build the sentences of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcmlctunKU4g"
   },
   "outputs": [],
   "source": [
    "data = read_text(uploaded['ita.txt'])\n",
    "ItalianNEng = to_lines(data)\n",
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fojrJ9MVKU4i"
   },
   "source": [
    "The ItalianEng has the data which we just imported and then ran through a funtion is now converted to the array by using the Python's inbuilt function \"array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCZ9O9r7KU4j"
   },
   "outputs": [],
   "source": [
    "data = read_text(\"ita.txt\")\n",
    "ItalianNEng = to_lines(data)\n",
    "ItalianNEng = array(ItalianNEng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw4Ad-e2KU4m"
   },
   "source": [
    "## PreProcessing the data/ Cleaning the data\n",
    "\n",
    "**remove all the punctuation and then change the case of every word to lower case**\n",
    "\n",
    "\n",
    "we will remove the punctuation in the below code by going through each line of the data and storing it in the same variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_018J7bKU4n"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "ItalianNEng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ItalianNEng[:,0]]\n",
    "ItalianNEng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ItalianNEng[:,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdssLYMlKU4q"
   },
   "source": [
    "Here all the punctuation is removed/cleaned and the data looks like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "t5oQeu4UKU4r",
    "outputId": "15a6e836-30c9-4dca-ef36-2fd6f030d87f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi', 'Ciao'],\n",
       "       ['Run', 'Corri'],\n",
       "       ['Run', 'Corra'],\n",
       "       ...,\n",
       "       ['If you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo',\n",
       "        'Se vuoi sembrare un madrelingua devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato'],\n",
       "       ['If someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker In other words you dont really sound like a native speaker',\n",
       "        'Se qualcuno che non conosce il tuo background dice che sembri un madrelingua significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua In altre parole non sembri davvero un madrelingua'],\n",
       "       ['It may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort However if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
       "        'Può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo Ciononostante se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando potremmo essere in grado di minimizzare gli errori']],\n",
       "      dtype='<U317')"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYpCTTSeKU4v"
   },
   "source": [
    "Now we will convert the data into lower case by using python's inbuilt function lower() and save the data to its original variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKXmS2z8KU4w"
   },
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(ItalianNEng)):\n",
    "    ItalianNEng[i,0] = ItalianNEng[i,0].lower()\n",
    "    \n",
    "    ItalianNEng[i,1] = ItalianNEng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "vkd65bK7KU43",
    "outputId": "85e8d8e0-b3ee-48e9-f4b1-b50aa033b64d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'ciao'],\n",
       "       ['run', 'corri'],\n",
       "       ['run', 'corra'],\n",
       "       ...,\n",
       "       ['if you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo',\n",
       "        'se vuoi sembrare un madrelingua devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato'],\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'se qualcuno che non conosce il tuo background dice che sembri un madrelingua significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua in altre parole non sembri davvero un madrelingua'],\n",
       "       ['it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
       "        'può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo ciononostante se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando potremmo essere in grado di minimizzare gli errori']],\n",
       "      dtype='<U317')"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q53n94cMKU45",
    "outputId": "59d782ef-bc75-4661-ee02-c06b0e6671b0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfFbZ9ADKU48"
   },
   "source": [
    "we convert the data into pandas dataframe and save it in the variable ita_eng_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7TXpS6Q8KU49",
    "outputId": "2c9bb9e2-33c5-4c22-e3a5-6f44f6f0087e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Italian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi</td>\n",
       "      <td>ciao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>corri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run</td>\n",
       "      <td>corra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>correte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who</td>\n",
       "      <td>chi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English  Italian\n",
       "0      hi     ciao\n",
       "1     run    corri\n",
       "2     run    corra\n",
       "3     run  correte\n",
       "4     who      chi"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita_eng_df = pd.DataFrame(ItalianNEng.tolist(), columns = ['English', 'Italian'])\n",
    "ita_eng_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZzQ_oYLKU5A"
   },
   "source": [
    "let us split the data into English and Italian part seperately and save it in their respective variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "l4Tq8lqbKU5B",
    "outputId": "3b161eaf-7b9b-4ad3-c5ce-01c3a1395414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English'], dtype='object')\n",
      "Index(['Italian'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "english_sentences = ita_eng_df.iloc[:,0:1]\n",
    "italian_sentences = ita_eng_df.iloc[:,1:2]\n",
    "print(english_sentences.columns)\n",
    "print(italian_sentences.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SynzEYZKU5E"
   },
   "source": [
    "As the dataset is very large we will sample the model with only 2000 rows, later we can play around the data size and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LplOj2QjKU5F"
   },
   "outputs": [],
   "source": [
    "n = 6000\n",
    "italian_sample = italian_sentences.iloc[:n,:]\n",
    "english_sample = english_sentences.iloc[:n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tiVE9tDKU5I"
   },
   "source": [
    "we will now convert any row which is not of string type, if there is any row with different datatype we will convert it into string so that everything is of same datatype\n",
    "\n",
    "\n",
    "we will do this for both English and Italian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gl_ENDuTKU5J"
   },
   "outputs": [],
   "source": [
    "english_sample.columns\n",
    "english_sample['English']\n",
    "for i in range(len(english_sample['English'].index)):\n",
    "    if type(english_sample['English'][i]) != str:\n",
    "        english_sample['English'][i] = str(english_sample['English'][i])\n",
    "        \n",
    "italian_sample['Italian']\n",
    "for i in range(len(italian_sample['Italian'].index)):\n",
    "    if type(italian_sample['Italian'][i]) != str:\n",
    "        italian_sample['Italian'][i] = str(italian_sample['Italian'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xzk6NFVGKU5K"
   },
   "source": [
    "create a fucntion to add start and end token of each row as a identifier to the model and convert the data into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pK5HyIIbKU5L"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "def cleaning_sentence(data, column):\n",
    "    sentence = [preprocess_sentence(i) for i in data[column]]\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P74XHSbbKU5N"
   },
   "source": [
    "We will continue to create a class which has function which will create word to index and then index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wS_zVM8YKU5O"
   },
   "outputs": [],
   "source": [
    "class LanguageIndex():    \n",
    "    def __init__(self, lang):        \n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "          self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "          self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "          self.idx2word[index] = word   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbnK7PNnKU5Q"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-gBt3ZuKU5T"
   },
   "source": [
    "**Cleaning the data**\n",
    "\n",
    "we will make use of the funciton such as cleaning_sentence, LanguageIndex, and word2idx which we have written to clean, process, Index and padd the data\n",
    "\n",
    "\n",
    "we use Keras's inbuil function which performs sequence padding to the each sentence to the maximum data length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqmQNyi7KU5T"
   },
   "outputs": [],
   "source": [
    "#Cleaning Sentences\n",
    "italian_sent = cleaning_sentence(italian_sample, 'Italian')\n",
    "english_sent = cleaning_sentence(english_sample, 'English')\n",
    "\n",
    "# index language using the class defined above\n",
    "inp_lang = LanguageIndex(it for it in italian_sent)\n",
    "targ_lang = LanguageIndex(en for en in english_sent)\n",
    "\n",
    "#Italian Sentences which will be indexed\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in it.split(' ')] for it in italian_sent]\n",
    "#English sentences which will be indexed\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en in english_sent]\n",
    "\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "# Padding the input and output tensor to the maximum length\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')\n",
    "\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                              maxlen=max_length_tar, \n",
    "                                                              padding='post')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cbch8w3KU5W"
   },
   "source": [
    "**split the data into training data and test data by using Sklearn.Model_selection's train_test_split function**\n",
    "\n",
    "\n",
    "Split the whole data which we saved in the variable ItalianNEng as 80% to train and 20% to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye3xgSgzKU5W"
   },
   "outputs": [],
   "source": [
    "ax_train, ay_train, ax_test, ay_test = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKVnvNWYKU5Y"
   },
   "source": [
    "after we split the training data and the testing data we will check the length and the data in it, if we see the below data its vectorized which we did using the word2idx and Keras's inbuilt function sequence_padding and is in the array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "9VUII8zPKU5Z",
    "outputId": "945196a6-5682-4175-bd9c-55ed25f5b827"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  10, 2338, 2538, ...,    0,    0,    0],\n",
       "       [  10, 1270, 1047, ...,    0,    0,    0],\n",
       "       [  10, 2338, 2538, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  10, 1358, 2060, ...,    0,    0,    0],\n",
       "       [  10, 2099, 2336, ...,    0,    0,    0],\n",
       "       [  10, 1440, 1259, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7KYxn5OrKU5b",
    "outputId": "8cfad82d-da9d-42ef-c770-e6a730fb30bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 1200, 4800, 1200)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show length\n",
    "len(ax_train), len(ay_train), len(ax_test), len(ay_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uAzQ5wPKU5e"
   },
   "source": [
    "Create the parameters where we can modify and pass later to tune the model and we will create the tensorflow dataset using the tf's function tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "MQVmNcQDKU5e",
    "outputId": "a013c3fd-31ba-440d-efc4-c4a904277334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "64\n",
      "75\n",
      "2539\n",
      "973\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(ax_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "\n",
    "print(BUFFER_SIZE)\n",
    "print(BATCH_SIZE)\n",
    "print(N_BATCH)\n",
    "print(vocab_inp_size)\n",
    "print(vocab_tar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SpEMRrYYKU5h",
    "outputId": "fbac5e25-2977-4fa6-87f4-98980711194e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 9), (64, 6)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((ax_train, ax_test)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FMXLifsKU5k"
   },
   "source": [
    "We will build the encoder and decoder model by implementing \"attention equation\"\n",
    "\n",
    "attention overcomes the limitation in the encode-decoder architecture by allowing the network to learn where to pay attention to the input for each item in the output sequence.\n",
    "\n",
    "this approach has been used across different types sequence prediction problems include text translation, speech recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lEXhULxKU5l"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                           return_sequences=True, \n",
    "                           return_state=True, \n",
    "                           recurrent_activation='sigmoid', \n",
    "                           recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkpXCBuOKU5n"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmM86Kp1KU5p"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU2MUnA1KU5q"
   },
   "outputs": [],
   "source": [
    "def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drtfo7UYKU5s"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6thdVy0kKU5t"
   },
   "source": [
    "**Optimizer and Loss Function**\n",
    "\n",
    "We are using \"Adam\" as the optimizer for the model and sparse_softmax_cross_entropy as the loss function.\n",
    "\n",
    "we can play around with these to see which is better suitable for our model, but to train the model we will be going with the above mentoned optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrkGCX1tKU5u"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lebrg1MoKU5v"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "we will train the model by passing the training data which we split and passed it through the funtion. we will be saving out model on each iteration of the epochs \n",
    "\n",
    "We will be running this model with **30 Epochs**\n",
    "\n",
    "we will play around with the number of epochs to see what best works for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4168
    },
    "colab_type": "code",
    "id": "wknvJ6F2KU5w",
    "outputId": "e4921d9c-2560-433e-fa6e-56f3650cc6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1 Batch 0 Loss 3.8523\n",
      "Epoch 1 Loss 2.5304\n",
      "Time taken for 1 epoch 16.319268703460693 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.9032\n",
      "Epoch 2 Loss 1.9296\n",
      "Time taken for 1 epoch 12.716121912002563 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7170\n",
      "Epoch 3 Loss 1.6937\n",
      "Time taken for 1 epoch 12.823138236999512 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5961\n",
      "Epoch 4 Loss 1.4607\n",
      "Time taken for 1 epoch 13.502289533615112 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.3418\n",
      "Epoch 5 Loss 1.2593\n",
      "Time taken for 1 epoch 13.110735416412354 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.1195\n",
      "Epoch 6 Loss 1.1130\n",
      "Time taken for 1 epoch 12.813982248306274 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9626\n",
      "Epoch 7 Loss 0.9812\n",
      "Time taken for 1 epoch 12.70730185508728 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.8077\n",
      "Epoch 8 Loss 0.8698\n",
      "Time taken for 1 epoch 12.866644382476807 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7665\n",
      "Epoch 9 Loss 0.7642\n",
      "Time taken for 1 epoch 12.769709587097168 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6343\n",
      "Epoch 10 Loss 0.6706\n",
      "Time taken for 1 epoch 13.11710810661316 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.5929\n",
      "Epoch 11 Loss 0.5763\n",
      "Time taken for 1 epoch 13.56724500656128 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.4342\n",
      "Epoch 12 Loss 0.4855\n",
      "Time taken for 1 epoch 12.835062980651855 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.3658\n",
      "Epoch 13 Loss 0.4059\n",
      "Time taken for 1 epoch 12.94894003868103 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.3461\n",
      "Epoch 14 Loss 0.3407\n",
      "Time taken for 1 epoch 12.741496324539185 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.2450\n",
      "Epoch 15 Loss 0.2770\n",
      "Time taken for 1 epoch 12.776547193527222 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.1894\n",
      "Epoch 16 Loss 0.2226\n",
      "Time taken for 1 epoch 12.805064916610718 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.2073\n",
      "Epoch 17 Loss 0.1785\n",
      "Time taken for 1 epoch 13.870991230010986 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.1508\n",
      "Epoch 18 Loss 0.1525\n",
      "Time taken for 1 epoch 12.838823318481445 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.1182\n",
      "Epoch 19 Loss 0.1333\n",
      "Time taken for 1 epoch 12.723740577697754 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0739\n",
      "Epoch 20 Loss 0.1097\n",
      "Time taken for 1 epoch 12.796648263931274 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0655\n",
      "Epoch 21 Loss 0.1042\n",
      "Time taken for 1 epoch 12.695494413375854 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0678\n",
      "Epoch 22 Loss 0.0915\n",
      "Time taken for 1 epoch 12.751342535018921 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0818\n",
      "Epoch 23 Loss 0.0823\n",
      "Time taken for 1 epoch 13.90829348564148 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0684\n",
      "Epoch 24 Loss 0.0815\n",
      "Time taken for 1 epoch 13.444160223007202 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0676\n",
      "Epoch 25 Loss 0.0728\n",
      "Time taken for 1 epoch 14.447543621063232 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0650\n",
      "Epoch 26 Loss 0.0728\n",
      "Time taken for 1 epoch 12.672031879425049 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0768\n",
      "Epoch 27 Loss 0.0747\n",
      "Time taken for 1 epoch 12.84520697593689 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0854\n",
      "Epoch 28 Loss 0.0711\n",
      "Time taken for 1 epoch 12.696802139282227 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0595\n",
      "Epoch 29 Loss 0.0701\n",
      "Time taken for 1 epoch 13.853675127029419 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0672\n",
      "Epoch 30 Loss 0.0632\n",
      "Time taken for 1 epoch 12.768047571182251 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0608\n",
      "Epoch 31 Loss 0.0680\n",
      "Time taken for 1 epoch 12.618232250213623 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0377\n",
      "Epoch 32 Loss 0.0637\n",
      "Time taken for 1 epoch 12.76252269744873 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0504\n",
      "Epoch 33 Loss 0.0611\n",
      "Time taken for 1 epoch 12.609782934188843 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0553\n",
      "Epoch 34 Loss 0.0614\n",
      "Time taken for 1 epoch 12.725339889526367 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0354\n",
      "Epoch 35 Loss 0.0598\n",
      "Time taken for 1 epoch 13.665526628494263 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0712\n",
      "Epoch 36 Loss 0.0606\n",
      "Time taken for 1 epoch 12.892224311828613 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0262\n",
      "Epoch 37 Loss 0.0584\n",
      "Time taken for 1 epoch 12.805133819580078 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0450\n",
      "Epoch 38 Loss 0.0594\n",
      "Time taken for 1 epoch 12.720723152160645 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0549\n",
      "Epoch 39 Loss 0.0598\n",
      "Time taken for 1 epoch 12.765357494354248 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0417\n",
      "Epoch 40 Loss 0.0615\n",
      "Time taken for 1 epoch 12.68313717842102 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0359\n",
      "Epoch 41 Loss 0.0584\n",
      "Time taken for 1 epoch 13.213427543640137 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0492\n",
      "Epoch 42 Loss 0.0590\n",
      "Time taken for 1 epoch 13.412222385406494 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0426\n",
      "Epoch 43 Loss 0.0607\n",
      "Time taken for 1 epoch 12.63398790359497 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0511\n",
      "Epoch 44 Loss 0.0614\n",
      "Time taken for 1 epoch 12.735651969909668 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0515\n",
      "Epoch 45 Loss 0.0653\n",
      "Time taken for 1 epoch 12.673831462860107 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0739\n",
      "Epoch 46 Loss 0.0700\n",
      "Time taken for 1 epoch 12.795674800872803 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0324\n",
      "Epoch 47 Loss 0.0737\n",
      "Time taken for 1 epoch 12.858952522277832 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0637\n",
      "Epoch 48 Loss 0.0711\n",
      "Time taken for 1 epoch 13.824191331863403 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0378\n",
      "Epoch 49 Loss 0.0663\n",
      "Time taken for 1 epoch 14.441565036773682 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0794\n",
      "Epoch 50 Loss 0.0641\n",
      "Time taken for 1 epoch 12.694092035293579 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0376\n",
      "Epoch 51 Loss 0.0605\n",
      "Time taken for 1 epoch 12.720125198364258 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0282\n",
      "Epoch 52 Loss 0.0556\n",
      "Time taken for 1 epoch 12.571800470352173 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0470\n",
      "Epoch 53 Loss 0.0495\n",
      "Time taken for 1 epoch 12.716988325119019 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0131\n",
      "Epoch 54 Loss 0.0495\n",
      "Time taken for 1 epoch 13.97572660446167 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0426\n",
      "Epoch 55 Loss 0.0474\n",
      "Time taken for 1 epoch 12.694826364517212 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0569\n",
      "Epoch 56 Loss 0.0472\n",
      "Time taken for 1 epoch 12.760908603668213 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0478\n",
      "Epoch 57 Loss 0.0465\n",
      "Time taken for 1 epoch 12.679811477661133 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0069\n",
      "Epoch 58 Loss 0.0462\n",
      "Time taken for 1 epoch 12.787583589553833 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0513\n",
      "Epoch 59 Loss 0.0458\n",
      "Time taken for 1 epoch 12.699692010879517 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0296\n",
      "Epoch 60 Loss 0.0460\n",
      "Time taken for 1 epoch 13.917403221130371 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 60\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))   \n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igSWtzupKU5x"
   },
   "source": [
    "## Prediction\n",
    "Once the model is trained we can translate the Italian sentences by passing it to the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqDEAweHKU5y"
   },
   "outputs": [],
   "source": [
    "def Prediction_eval(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7DwyJJkKU51"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = Prediction_eval(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "udESsGOpKU53",
    "outputId": "4e94c739-874a-4012-ec99-18d77751423c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sample.iloc[:1,:].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "X-MPUkeVKU55",
    "outputId": "22b2121a-fd2c-4595-bb63-717d56e1d19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ciao <end>\n",
      "Predicted translation: hi <end> \n"
     ]
    }
   ],
   "source": [
    "translate(italian_sample.iloc[:1,:].values[0][0], encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "j1RGsyUIKU58",
    "outputId": "f337d76a-37ff-46ca-c8d2-fed1e8c75217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> corri <end>\n",
      "Predicted translation: you run <end> \n"
     ]
    }
   ],
   "source": [
    "test = 'corri'\n",
    "translate(sentence=test, encoder=encoder, decoder=decoder, inp_lang= inp_lang, targ_lang= targ_lang, max_length_inp= max_length_inp, max_length_targ= max_length_tar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oJ1K5aYKU6H"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "**o** https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/\n",
    "\n",
    "**o** https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa\n",
    "\n",
    "**o** https://www.analyticsvidhya.com/blog/2018/03/microsofts-claims-language-translation-ai-reached-human-levels-accuracy/\n",
    "\n",
    "**o** https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=nRBnh4qbPHI&vl=en\n",
    "\n",
    "**o** http://www.manythings.org/anki/\n",
    "\n",
    "**o** https://machinelearningmastery.com/introduction-neural-machine-translation/\n",
    "\n",
    "**o** https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=vI2Y3I-JI2Q\n",
    "\n",
    "**o** https://towardsdatascience.com/neural-machine-translator-with-less-than-50-lines-of-code-guide-1fe4fdfe6292\n",
    "\n",
    "**o** https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/\n",
    "\n",
    "**o** https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnVG9BT0KU6I"
   },
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buv-bbhUKU6I"
   },
   "source": [
    "**MIT License**\n",
    "\n",
    "Copyright 2018 Chetan M Jadhav\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUbVc5x8KU6J"
   },
   "source": [
    "**The text in the document by Chetan M Jadhav is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/ **"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9oJ1K5aYKU6H",
    "OnVG9BT0KU6I"
   ],
   "name": "Attention_6000records_60_Epochs.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
