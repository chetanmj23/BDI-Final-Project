{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DCtbqJRKU4Y"
   },
   "source": [
    "# Italian to English Language Translation (NMT) using GRU & Attention\n",
    "\n",
    "**Import All the neccessary Libraries which we will be requiring to run this notebook and the project**\n",
    "\n",
    "This is seq2seq model by using GRU and Attention. let us import all the necessary libraries that is needed to run this notebook\n",
    "\n",
    "the tensorflow GPU version must be updated before running this notebook and eager_execution of the tensorflow should be enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UI8yE-5lKU4Z",
    "outputId": "1486968e-253e-48cb-9ac3-621c06681ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import array, argmax, random, take\n",
    "import time\n",
    "import pandas as pd\n",
    "import string\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xHlI_PMKU4c"
   },
   "source": [
    "Creating a function which will read the file, encode it and save it and then the funtion to_lines wil  split the data into Italian & English part seperately by '\\n' and build it in the form of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26JOk16NKU4d"
   },
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "9UEU4t-VqxOL",
    "outputId": "4c6a9cce-347d-43d1-ef68-a865231f210a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-5e5f3b03-1c71-468f-b15b-f4fd80ac7f4d\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-5e5f3b03-1c71-468f-b15b-f4fd80ac7f4d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ita.txt to ita.txt\n"
     ]
    }
   ],
   "source": [
    "   from google.colab import files\n",
    "\n",
    "   uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NLmiq6rGuC-q",
    "outputId": "b0a17cdb-1a30-45d5-9651-2a04069791c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User uploaded file \"ita.txt\" with length 19184787 bytes\n"
     ]
    }
   ],
   "source": [
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syMEwtZhKU4f"
   },
   "source": [
    "By using the function we wrote the text file is imported to the jupyter notebook. \n",
    "\n",
    "Once the notebook is opened and read using the function Read_TextFile we will pass it to the to_lines to split and to build the sentences of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcmlctunKU4g"
   },
   "outputs": [],
   "source": [
    "data = read_text(uploaded['ita.txt'])\n",
    "ItalianNEng = to_lines(data)\n",
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fojrJ9MVKU4i"
   },
   "source": [
    "The ItalianEng has the data which we just imported and then ran through a funtion is now converted to the array by using the Python's inbuilt function \"array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCZ9O9r7KU4j"
   },
   "outputs": [],
   "source": [
    "data = read_text(\"ita.txt\")\n",
    "ItalianNEng = to_lines(data)\n",
    "ItalianNEng = array(ItalianNEng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw4Ad-e2KU4m"
   },
   "source": [
    "## PreProcessing the data/ Cleaning the data\n",
    "\n",
    "**remove all the punctuation and then change the case of every word to lower case**\n",
    "\n",
    "\n",
    "we will remove the punctuation in the below code by going through each line of the data and storing it in the same variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_018J7bKU4n"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "ItalianNEng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ItalianNEng[:,0]]\n",
    "ItalianNEng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ItalianNEng[:,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdssLYMlKU4q"
   },
   "source": [
    "Here all the punctuation is removed/cleaned and the data looks like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "t5oQeu4UKU4r",
    "outputId": "8c3f188f-0bd4-4ed2-ab38-e61242abc5c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi', 'Ciao'],\n",
       "       ['Run', 'Corri'],\n",
       "       ['Run', 'Corra'],\n",
       "       ...,\n",
       "       ['If you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo',\n",
       "        'Se vuoi sembrare un madrelingua devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato'],\n",
       "       ['If someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker In other words you dont really sound like a native speaker',\n",
       "        'Se qualcuno che non conosce il tuo background dice che sembri un madrelingua significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua In altre parole non sembri davvero un madrelingua'],\n",
       "       ['It may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort However if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
       "        'Può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo Ciononostante se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando potremmo essere in grado di minimizzare gli errori']],\n",
       "      dtype='<U317')"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYpCTTSeKU4v"
   },
   "source": [
    "Now we will convert the data into lower case by using python's inbuilt function lower() and save the data to its original variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKXmS2z8KU4w"
   },
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(ItalianNEng)):\n",
    "    ItalianNEng[i,0] = ItalianNEng[i,0].lower()\n",
    "    \n",
    "    ItalianNEng[i,1] = ItalianNEng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "vkd65bK7KU43",
    "outputId": "a9d61036-3bf8-4606-ef52-da6574f11ed7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'ciao'],\n",
       "       ['run', 'corri'],\n",
       "       ['run', 'corra'],\n",
       "       ...,\n",
       "       ['if you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo',\n",
       "        'se vuoi sembrare un madrelingua devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato'],\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'se qualcuno che non conosce il tuo background dice che sembri un madrelingua significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua in altre parole non sembri davvero un madrelingua'],\n",
       "       ['it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
       "        'può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo ciononostante se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando potremmo essere in grado di minimizzare gli errori']],\n",
       "      dtype='<U317')"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q53n94cMKU45",
    "outputId": "9338e4ab-b215-4d8b-a4c1-d0335400955c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfFbZ9ADKU48"
   },
   "source": [
    "we convert the data into pandas dataframe and save it in the variable ita_eng_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7TXpS6Q8KU49",
    "outputId": "df0222f1-2396-40bf-fb93-589e61a24216"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Italian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi</td>\n",
       "      <td>ciao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>corri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run</td>\n",
       "      <td>corra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>correte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who</td>\n",
       "      <td>chi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English  Italian\n",
       "0      hi     ciao\n",
       "1     run    corri\n",
       "2     run    corra\n",
       "3     run  correte\n",
       "4     who      chi"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita_eng_df = pd.DataFrame(ItalianNEng.tolist(), columns = ['English', 'Italian'])\n",
    "ita_eng_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZzQ_oYLKU5A"
   },
   "source": [
    "let us split the data into English and Italian part seperately and save it in their respective variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "l4Tq8lqbKU5B",
    "outputId": "4b3c3d2e-8e81-423b-ea78-7cd9702634ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English'], dtype='object')\n",
      "Index(['Italian'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "english_sentences = ita_eng_df.iloc[:,0:1]\n",
    "italian_sentences = ita_eng_df.iloc[:,1:2]\n",
    "print(english_sentences.columns)\n",
    "print(italian_sentences.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SynzEYZKU5E"
   },
   "source": [
    "As the dataset is very large we will sample the model with only 2000 rows, later we can play around the data size and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LplOj2QjKU5F"
   },
   "outputs": [],
   "source": [
    "n = 4000\n",
    "italian_sample = italian_sentences.iloc[:n,:]\n",
    "english_sample = english_sentences.iloc[:n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tiVE9tDKU5I"
   },
   "source": [
    "we will now convert any row which is not of string type, if there is any row with different datatype we will convert it into string so that everything is of same datatype\n",
    "\n",
    "\n",
    "we will do this for both English and Italian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gl_ENDuTKU5J"
   },
   "outputs": [],
   "source": [
    "english_sample.columns\n",
    "english_sample['English']\n",
    "for i in range(len(english_sample['English'].index)):\n",
    "    if type(english_sample['English'][i]) != str:\n",
    "        english_sample['English'][i] = str(english_sample['English'][i])\n",
    "        \n",
    "italian_sample['Italian']\n",
    "for i in range(len(italian_sample['Italian'].index)):\n",
    "    if type(italian_sample['Italian'][i]) != str:\n",
    "        italian_sample['Italian'][i] = str(italian_sample['Italian'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xzk6NFVGKU5K"
   },
   "source": [
    "create a fucntion to add start and end token of each row as a identifier to the model and convert the data into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pK5HyIIbKU5L"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "def cleaning_sentence(data, column):\n",
    "    sentence = [preprocess_sentence(i) for i in data[column]]\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P74XHSbbKU5N"
   },
   "source": [
    "We will continue to create a class which has function which will create word to index and then index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wS_zVM8YKU5O"
   },
   "outputs": [],
   "source": [
    "class LanguageIndex():    \n",
    "    def __init__(self, lang):        \n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "          self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "          self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "          self.idx2word[index] = word   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbnK7PNnKU5Q"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-gBt3ZuKU5T"
   },
   "source": [
    "**Cleaning the data**\n",
    "\n",
    "we will make use of the funciton such as cleaning_sentence, LanguageIndex, and word2idx which we have written to clean, process, Index and padd the data\n",
    "\n",
    "\n",
    "we use Keras's inbuil function which performs sequence padding to the each sentence to the maximum data length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqmQNyi7KU5T"
   },
   "outputs": [],
   "source": [
    "#Cleaning Sentences\n",
    "italian_sent = cleaning_sentence(italian_sample, 'Italian')\n",
    "english_sent = cleaning_sentence(english_sample, 'English')\n",
    "\n",
    "# index language using the class defined above\n",
    "inp_lang = LanguageIndex(it for it in italian_sent)\n",
    "targ_lang = LanguageIndex(en for en in english_sent)\n",
    "\n",
    "#Italian Sentences which will be indexed\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in it.split(' ')] for it in italian_sent]\n",
    "#English sentences which will be indexed\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en in english_sent]\n",
    "\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "# Padding the input and output tensor to the maximum length\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')\n",
    "\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                              maxlen=max_length_tar, \n",
    "                                                              padding='post')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cbch8w3KU5W"
   },
   "source": [
    "**split the data into training data and test data by using Sklearn.Model_selection's train_test_split function**\n",
    "\n",
    "\n",
    "Split the whole data which we saved in the variable ItalianNEng as 80% to train and 20% to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye3xgSgzKU5W"
   },
   "outputs": [],
   "source": [
    "ax_train, ay_train, ax_test, ay_test = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKVnvNWYKU5Y"
   },
   "source": [
    "after we split the training data and the testing data we will check the length and the data in it, if we see the below data its vectorized which we did using the word2idx and Keras's inbuilt function sequence_padding and is in the array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "9VUII8zPKU5Z",
    "outputId": "2b805d8f-5e5e-4621-ddac-e2faf039732c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6,  844, 1554, ...,    0,    0,    0],\n",
       "       [   6, 1885, 1142, ...,    0,    0,    0],\n",
       "       [   6,  178,    5, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   6,  498,    5, ...,    0,    0,    0],\n",
       "       [   6,  844,  988, ...,    0,    0,    0],\n",
       "       [   6, 1492, 1130, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7KYxn5OrKU5b",
    "outputId": "6e74512f-6fa3-4660-ea04-e78f10b40483"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 800, 3200, 800)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show length\n",
    "len(ax_train), len(ay_train), len(ax_test), len(ay_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uAzQ5wPKU5e"
   },
   "source": [
    "Create the parameters where we can modify and pass later to tune the model and we will create the tensorflow dataset using the tf's function tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "MQVmNcQDKU5e",
    "outputId": "ff8b42b1-39df-4cb5-d54e-25b5c7447323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "64\n",
      "50\n",
      "1886\n",
      "693\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(ax_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "\n",
    "print(BUFFER_SIZE)\n",
    "print(BATCH_SIZE)\n",
    "print(N_BATCH)\n",
    "print(vocab_inp_size)\n",
    "print(vocab_tar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SpEMRrYYKU5h",
    "outputId": "a18dfa7a-b15d-4e3e-a8f3-1ccb2545207b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 9), (64, 6)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((ax_train, ax_test)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FMXLifsKU5k"
   },
   "source": [
    "We will build the encoder and decoder model by implementing \"attention equation\"\n",
    "\n",
    "attention overcomes the limitation in the encode-decoder architecture by allowing the network to learn where to pay attention to the input for each item in the output sequence.\n",
    "\n",
    "this approach has been used across different types sequence prediction problems include text translation, speech recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lEXhULxKU5l"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                           return_sequences=True, \n",
    "                           return_state=True, \n",
    "                           recurrent_activation='sigmoid', \n",
    "                           recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkpXCBuOKU5n"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmM86Kp1KU5p"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU2MUnA1KU5q"
   },
   "outputs": [],
   "source": [
    "def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drtfo7UYKU5s"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6thdVy0kKU5t"
   },
   "source": [
    "**Optimizer and Loss Function**\n",
    "\n",
    "We are using \"Adam\" as the optimizer for the model and sparse_softmax_cross_entropy as the loss function.\n",
    "\n",
    "we can play around with these to see which is better suitable for our model, but to train the model we will be going with the above mentoned optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrkGCX1tKU5u"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lebrg1MoKU5v"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "we will train the model by passing the training data which we split and passed it through the funtion. we will be saving out model on each iteration of the epochs \n",
    "\n",
    "We will be running this model with **30 Epochs**\n",
    "\n",
    "we will play around with the number of epochs to see what best works for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2808
    },
    "colab_type": "code",
    "id": "wknvJ6F2KU5w",
    "outputId": "6654d8c9-707b-4956-c8a1-f85ce1cfb04d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1 Batch 0 Loss 3.6454\n",
      "Epoch 1 Loss 2.5360\n",
      "Time taken for 1 epoch 9.944748163223267 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.9223\n",
      "Epoch 2 Loss 1.8720\n",
      "Time taken for 1 epoch 8.805859565734863 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7307\n",
      "Epoch 3 Loss 1.7082\n",
      "Time taken for 1 epoch 9.48662543296814 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5072\n",
      "Epoch 4 Loss 1.5026\n",
      "Time taken for 1 epoch 8.954044580459595 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4262\n",
      "Epoch 5 Loss 1.2997\n",
      "Time taken for 1 epoch 8.833491802215576 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.1589\n",
      "Epoch 6 Loss 1.1484\n",
      "Time taken for 1 epoch 8.69847846031189 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.0229\n",
      "Epoch 7 Loss 1.0363\n",
      "Time taken for 1 epoch 8.853046417236328 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.9543\n",
      "Epoch 8 Loss 0.9238\n",
      "Time taken for 1 epoch 8.750823974609375 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7996\n",
      "Epoch 9 Loss 0.8197\n",
      "Time taken for 1 epoch 8.902754068374634 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.7619\n",
      "Epoch 10 Loss 0.7445\n",
      "Time taken for 1 epoch 8.692206859588623 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.6503\n",
      "Epoch 11 Loss 0.6723\n",
      "Time taken for 1 epoch 8.646382093429565 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.5428\n",
      "Epoch 12 Loss 0.6044\n",
      "Time taken for 1 epoch 9.473032712936401 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.5566\n",
      "Epoch 13 Loss 0.5469\n",
      "Time taken for 1 epoch 8.906400442123413 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.4571\n",
      "Epoch 14 Loss 0.4824\n",
      "Time taken for 1 epoch 8.779704809188843 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.4145\n",
      "Epoch 15 Loss 0.4292\n",
      "Time taken for 1 epoch 8.7562735080719 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.3186\n",
      "Epoch 16 Loss 0.3694\n",
      "Time taken for 1 epoch 9.722482442855835 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.2717\n",
      "Epoch 17 Loss 0.3141\n",
      "Time taken for 1 epoch 9.086331129074097 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.2053\n",
      "Epoch 18 Loss 0.2676\n",
      "Time taken for 1 epoch 8.70476484298706 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.2077\n",
      "Epoch 19 Loss 0.2372\n",
      "Time taken for 1 epoch 8.663342237472534 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.2257\n",
      "Epoch 20 Loss 0.1914\n",
      "Time taken for 1 epoch 8.741352319717407 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.1347\n",
      "Epoch 21 Loss 0.1585\n",
      "Time taken for 1 epoch 9.414800882339478 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.1262\n",
      "Epoch 22 Loss 0.1400\n",
      "Time taken for 1 epoch 8.973430871963501 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0934\n",
      "Epoch 23 Loss 0.1284\n",
      "Time taken for 1 epoch 8.72501254081726 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.1024\n",
      "Epoch 24 Loss 0.1261\n",
      "Time taken for 1 epoch 8.555520296096802 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.1167\n",
      "Epoch 25 Loss 0.1142\n",
      "Time taken for 1 epoch 8.650434017181396 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0614\n",
      "Epoch 26 Loss 0.1074\n",
      "Time taken for 1 epoch 8.618392705917358 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0791\n",
      "Epoch 27 Loss 0.0931\n",
      "Time taken for 1 epoch 8.639946460723877 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0854\n",
      "Epoch 28 Loss 0.0784\n",
      "Time taken for 1 epoch 8.617584466934204 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0498\n",
      "Epoch 29 Loss 0.0723\n",
      "Time taken for 1 epoch 8.559330224990845 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0810\n",
      "Epoch 30 Loss 0.0688\n",
      "Time taken for 1 epoch 9.288172245025635 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0751\n",
      "Epoch 31 Loss 0.0665\n",
      "Time taken for 1 epoch 9.042842388153076 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0479\n",
      "Epoch 32 Loss 0.0616\n",
      "Time taken for 1 epoch 8.523797512054443 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0442\n",
      "Epoch 33 Loss 0.0621\n",
      "Time taken for 1 epoch 9.351551055908203 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0479\n",
      "Epoch 34 Loss 0.0609\n",
      "Time taken for 1 epoch 8.494425773620605 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0912\n",
      "Epoch 35 Loss 0.0610\n",
      "Time taken for 1 epoch 8.608481407165527 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0423\n",
      "Epoch 36 Loss 0.0576\n",
      "Time taken for 1 epoch 8.638583660125732 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0692\n",
      "Epoch 37 Loss 0.0570\n",
      "Time taken for 1 epoch 8.421333312988281 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0477\n",
      "Epoch 38 Loss 0.0552\n",
      "Time taken for 1 epoch 8.54841160774231 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0571\n",
      "Epoch 39 Loss 0.0553\n",
      "Time taken for 1 epoch 8.925658226013184 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0605\n",
      "Epoch 40 Loss 0.0551\n",
      "Time taken for 1 epoch 9.23789644241333 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))   \n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igSWtzupKU5x"
   },
   "source": [
    "## Prediction\n",
    "Once the model is trained we can translate the Italian sentences by passing it to the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqDEAweHKU5y"
   },
   "outputs": [],
   "source": [
    "def Prediction_eval(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7DwyJJkKU51"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = Prediction_eval(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "udESsGOpKU53",
    "outputId": "1a85c2dd-f284-4a1b-dbdf-e8ff6eafac13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sample.iloc[:1,:].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "X-MPUkeVKU55",
    "outputId": "c92fbb02-7b3e-495f-c0d3-314097a7e194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ciao <end>\n",
      "Predicted translation: we walked <end> \n"
     ]
    }
   ],
   "source": [
    "translate(italian_sample.iloc[:1,:].values[0][0], encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "j1RGsyUIKU58",
    "outputId": "f209774c-b938-483b-b6a7-abbe972f3c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> venire <end>\n",
      "Predicted translation: come over <end> \n"
     ]
    }
   ],
   "source": [
    "test = 'venire'\n",
    "translate(sentence=test, encoder=encoder, decoder=decoder, inp_lang= inp_lang, targ_lang= targ_lang, max_length_inp= max_length_inp, max_length_targ= max_length_tar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oJ1K5aYKU6H"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "**o** https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/\n",
    "\n",
    "**o** https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa\n",
    "\n",
    "**o** https://www.analyticsvidhya.com/blog/2018/03/microsofts-claims-language-translation-ai-reached-human-levels-accuracy/\n",
    "\n",
    "**o** https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=nRBnh4qbPHI&vl=en\n",
    "\n",
    "**o** http://www.manythings.org/anki/\n",
    "\n",
    "**o** https://machinelearningmastery.com/introduction-neural-machine-translation/\n",
    "\n",
    "**o** https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=vI2Y3I-JI2Q\n",
    "\n",
    "**o** https://towardsdatascience.com/neural-machine-translator-with-less-than-50-lines-of-code-guide-1fe4fdfe6292\n",
    "\n",
    "**o** https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/\n",
    "\n",
    "**o** https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnVG9BT0KU6I"
   },
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buv-bbhUKU6I"
   },
   "source": [
    "**MIT License**\n",
    "\n",
    "Copyright 2018 Chetan M Jadhav\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUbVc5x8KU6J"
   },
   "source": [
    "**The text in the document by Chetan M Jadhav is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/ **"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9oJ1K5aYKU6H",
    "OnVG9BT0KU6I"
   ],
   "name": "Attention_4,000_records_40_Epochs.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
