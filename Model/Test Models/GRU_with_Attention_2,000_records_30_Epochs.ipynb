{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DCtbqJRKU4Y"
   },
   "source": [
    "# Italian to English Language Translation (NMT) using GRU & Attention\n",
    "\n",
    "**Import All the neccessary Libraries which we will be requiring to run this notebook and the project**\n",
    "\n",
    "This is seq2seq model by using GRU and Attention. let us import all the necessary libraries that is needed to run this notebook\n",
    "\n",
    "the tensorflow GPU version must be updated before running this notebook and eager_execution of the tensorflow should be enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UI8yE-5lKU4Z",
    "outputId": "713e95af-3312-492e-c2ac-8760b17eb510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import array, argmax, random, take\n",
    "import time\n",
    "import pandas as pd\n",
    "import string\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xHlI_PMKU4c"
   },
   "source": [
    "Creating a function which will read the file, encode it and save it and then the funtion to_lines wil  split the data into Italian & English part seperately by '\\n' and build it in the form of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26JOk16NKU4d"
   },
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syMEwtZhKU4f"
   },
   "source": [
    "By using the function we wrote the text file is imported to the jupyter notebook. \n",
    "\n",
    "Once the notebook is opened and read using the function Read_TextFile we will pass it to the to_lines to split and to build the sentences of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "vcmlctunKU4g",
    "outputId": "45eb15ad-16c8-4527-ab3a-47d53c6938a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Ciao!'],\n",
       " ['Run!', 'Corri!'],\n",
       " ['Run!', 'Corra!'],\n",
       " ['Run!', 'Correte!'],\n",
       " ['Who?', 'Chi?'],\n",
       " ['Wow!', 'Wow!'],\n",
       " ['Jump!', 'Salta!'],\n",
       " ['Jump!', 'Salti!'],\n",
       " ['Jump!', 'Saltate!'],\n",
       " ['Jump.', 'Salta.'],\n",
       " ['Jump.', 'Salti.'],\n",
       " ['Jump.', 'Saltate.'],\n",
       " ['Stop!', 'Fermati!'],\n",
       " ['Stop!', 'Fermatevi!'],\n",
       " ['Stop!', 'Stop!'],\n",
       " ['Stop!', 'Si fermi!'],\n",
       " ['Wait!', 'Aspetta!'],\n",
       " ['Wait!', 'Aspettate!'],\n",
       " ['Wait!', 'Aspetti!'],\n",
       " ['Wait.', 'Aspetta.'],\n",
       " ['Wait.', 'Aspetti.'],\n",
       " ['Wait.', 'Aspettate.'],\n",
       " ['Do it.', 'Fallo.'],\n",
       " ['Do it.', 'Falla.'],\n",
       " ['Do it.', 'Lo faccia.'],\n",
       " ['Do it.', 'La faccia.'],\n",
       " ['Do it.', 'Fatelo.'],\n",
       " ['Do it.', 'Fatela.'],\n",
       " ['Go on.', 'Vai avanti.'],\n",
       " ['Go on.', 'Continua.'],\n",
       " ['Go on.', 'Continui.'],\n",
       " ['Go on.', 'Continuate.'],\n",
       " ['Go on.', 'Vada avanti.'],\n",
       " ['Go on.', 'Andate avanti.'],\n",
       " ['Hello!', 'Buongiorno!'],\n",
       " ['Hello!', 'Ciao!'],\n",
       " ['Hello!', 'Salve.'],\n",
       " ['I ran.', 'Ho corso.'],\n",
       " ['I ran.', 'Corsi.'],\n",
       " ['I see.', 'Capisco.'],\n",
       " ['I see.', 'Io capisco.'],\n",
       " ['I try.', 'Provo.'],\n",
       " ['I try.', 'Io provo.'],\n",
       " ['I won!', 'Ho vinto!'],\n",
       " ['I won.', 'Vinsi.'],\n",
       " ['Oh no!', 'Oh no!'],\n",
       " ['Relax.', 'Rilassati.'],\n",
       " ['Relax.', 'Si rilassi.'],\n",
       " ['Relax.', 'Rilassatevi.'],\n",
       " ['Shoot!', 'Spara!'],\n",
       " ['Shoot!', 'Spari!'],\n",
       " ['Shoot!', 'Sparate!'],\n",
       " ['Smile.', 'Sorridi.'],\n",
       " ['Smile.', 'Sorrida.'],\n",
       " ['Smile.', 'Sorridete.'],\n",
       " ['Attack!', 'Attacca!'],\n",
       " ['Attack!', 'Attaccate!'],\n",
       " ['Attack!', 'Attacchi!'],\n",
       " ['Cheers!', 'Alla vostra!'],\n",
       " ['Cheers!', 'Salute!'],\n",
       " ['Cheers!', 'Alla tua!'],\n",
       " ['Cheers!', 'Alla sua!'],\n",
       " ['Eat it.', 'Mangialo.'],\n",
       " ['Eat it.', 'Mangiala.'],\n",
       " ['Eat it.', 'Lo mangi.'],\n",
       " ['Eat it.', 'La mangi.'],\n",
       " ['Eat it.', 'Mangiatelo.'],\n",
       " ['Eat it.', 'Mangiatela.'],\n",
       " ['Eat up.', 'Finisci di mangiare.'],\n",
       " ['Eat up.', 'Finisca di mangiare.'],\n",
       " ['Eat up.', 'Finite di mangiare.'],\n",
       " ['Freeze!', 'Fermo!'],\n",
       " ['Freeze!', 'Ferma!'],\n",
       " ['Freeze!', 'Fermi!'],\n",
       " ['Freeze!', 'Ferme!'],\n",
       " ['Freeze!', 'Che nessuno si muova!'],\n",
       " ['Get up.', 'Alzati.'],\n",
       " ['Go now.', 'Vai ora.'],\n",
       " ['Go now.', 'Vai adesso.'],\n",
       " ['Go now.', 'Vada ora.'],\n",
       " ['Go now.', 'Vada adesso.'],\n",
       " ['Go now.', 'Andate ora.'],\n",
       " ['Go now.', 'Andate adesso.'],\n",
       " ['Got it!', 'Capito!'],\n",
       " ['Got it!', 'Capita!'],\n",
       " ['He ran.', 'Corse.'],\n",
       " ['Hop in.', 'Salta su.'],\n",
       " ['Hop in.', 'Saltate su.'],\n",
       " ['Hop in.', 'Salti su.'],\n",
       " ['Hug me.', 'Abbracciami.'],\n",
       " ['Hug me.', 'Mi abbracci.'],\n",
       " ['Hug me.', 'Abbracciatemi.'],\n",
       " ['I fell.', 'Sono caduta.'],\n",
       " ['I fell.', 'Sono caduto.'],\n",
       " ['I fell.', 'Io sono caduto.'],\n",
       " ['I fell.', 'Io sono caduta.'],\n",
       " ['I fell.', 'Caddi.'],\n",
       " ['I fell.', 'Io caddi.'],\n",
       " ['I knit.', 'Lavoro a maglia.'],\n",
       " ['I knit.', 'Io lavoro a maglia.'],\n",
       " ['I knit.', 'Lavoro ai ferri.'],\n",
       " ['I knit.', 'Io lavoro ai ferri.'],\n",
       " ['I know.', 'Lo so.'],\n",
       " ['I know.', 'Io lo so.'],\n",
       " ['I left.', 'Sono partito.'],\n",
       " ['I left.', 'Io sono partito.'],\n",
       " ['I left.', 'Sono partita.'],\n",
       " ['I left.', 'Io sono partita.'],\n",
       " ['I left.', 'Partii.'],\n",
       " ['I left.', 'Io partii.'],\n",
       " ['I left.', 'Me ne sono andato.'],\n",
       " ['I left.', 'Io me ne sono andato.'],\n",
       " ['I left.', 'Me ne sono andata.'],\n",
       " ['I left.', 'Io me ne sono andata.'],\n",
       " ['I left.', 'Me ne andai.'],\n",
       " ['I left.', 'Io me ne andai.'],\n",
       " ['I lied.', 'Ho mentito.'],\n",
       " ['I lied.', 'Io ho mentito.'],\n",
       " ['I lied.', 'Mentii.'],\n",
       " ['I lied.', 'Io mentii.'],\n",
       " ['I lost.', 'Ho perso.'],\n",
       " ['I lost.', 'Io ho perso.'],\n",
       " ['I lost.', 'Persi.'],\n",
       " ['I lost.', 'Io persi.'],\n",
       " ['I paid.', 'Ho pagato.'],\n",
       " ['I paid.', 'Pagai.'],\n",
       " ['I quit.', 'Mi licenzio.'],\n",
       " ['I swim.', 'Nuoto.'],\n",
       " ['I work.', 'Io lavoro.'],\n",
       " ['I work.', 'Lavoro.'],\n",
       " [\"I'm 19.\", 'Ho diciannove anni.'],\n",
       " [\"I'm 19.\", 'Ho 19 anni.'],\n",
       " [\"I'm OK.\", 'Sto bene.'],\n",
       " ['Listen.', 'Ascolta!'],\n",
       " ['Listen.', 'Ascoltate!'],\n",
       " ['Listen.', 'Ascolti!'],\n",
       " ['No way!', 'È impossibile.'],\n",
       " ['Really?', 'Veramente?'],\n",
       " ['Really?', 'Davvero?'],\n",
       " ['Thanks.', 'Grazie!'],\n",
       " ['Try it.', 'Provalo.'],\n",
       " ['Try it.', 'Provala.'],\n",
       " ['Try it.', 'Lo provi.'],\n",
       " ['Try it.', 'La provi.'],\n",
       " ['Try it.', 'Provatelo.'],\n",
       " ['Try it.', 'Provatela.'],\n",
       " ['We ate.', 'Abbiamo mangiato.'],\n",
       " ['We ate.', 'Noi abbiamo mangiato.'],\n",
       " ['We ate.', 'Mangiammo.'],\n",
       " ['We ate.', 'Noi mangiammo.'],\n",
       " ['We try.', 'Proviamo.'],\n",
       " ['We try.', 'Ci proviamo.'],\n",
       " ['We try.', 'Noi proviamo.'],\n",
       " ['We won.', 'Abbiamo vinto.'],\n",
       " ['We won.', 'Noi abbiamo vinto.'],\n",
       " ['We won.', 'Vincemmo.'],\n",
       " ['We won.', 'Noi vincemmo.'],\n",
       " ['Why me?', 'Perché io?'],\n",
       " ['Ask Tom.', 'Chiedi a Tom.'],\n",
       " ['Ask Tom.', 'Chiedete a Tom.'],\n",
       " ['Ask Tom.', 'Chieda a Tom.'],\n",
       " ['Ask Tom.', 'Chiedilo a Tom.'],\n",
       " ['Ask Tom.', 'Chiedetelo a Tom.'],\n",
       " ['Ask Tom.', 'Lo chieda a Tom.'],\n",
       " ['Awesome!', 'Fantastico!'],\n",
       " ['Awesome!', 'Spettacolare!'],\n",
       " ['Be calm.', 'Sii calmo.'],\n",
       " ['Be calm.', 'Sii calma.'],\n",
       " ['Be calm.', 'Sia calmo.'],\n",
       " ['Be calm.', 'Sia calma.'],\n",
       " ['Be calm.', 'Siate calmi.'],\n",
       " ['Be calm.', 'Siate calme.'],\n",
       " ['Be cool.', 'Atteggiati.'],\n",
       " ['Be fair.', 'Sii imparziale.'],\n",
       " ['Be fair.', 'Sia imparziale.'],\n",
       " ['Be fair.', 'Siate imparziali.'],\n",
       " ['Be kind.', 'Sii gentile.'],\n",
       " ['Be kind.', 'Sia gentile.'],\n",
       " ['Be kind.', 'Siate gentili.'],\n",
       " ['Be nice.', 'Sii gentile.'],\n",
       " ['Be nice.', 'Sia gentile.'],\n",
       " ['Be nice.', 'Siate gentili.'],\n",
       " ['Beat it.', 'Sparisci.'],\n",
       " ['Burn it.', 'Brucialo.'],\n",
       " ['Burn it.', 'Bruciala.'],\n",
       " ['Burn it.', 'Lo bruci.'],\n",
       " ['Burn it.', 'La bruci.'],\n",
       " ['Burn it.', 'Bruciatelo.'],\n",
       " ['Burn it.', 'Bruciatela.'],\n",
       " ['Call me.', 'Chiamami.'],\n",
       " ['Call me.', 'Chiamatemi.'],\n",
       " ['Call me.', 'Mi chiami.'],\n",
       " ['Call us.', 'Chiamaci.'],\n",
       " ['Call us.', 'Ci chiami.'],\n",
       " ['Call us.', 'Chiamateci.'],\n",
       " ['Come in.', 'Entrate!'],\n",
       " ['Come in.', 'Entra!'],\n",
       " ['Come in.', 'Entri!'],\n",
       " ['Come in.', 'Vieni dentro.'],\n",
       " ['Come in.', 'Venite dentro.'],\n",
       " ['Come in.', 'Venga dentro.'],\n",
       " ['Come in.', 'Entrate.'],\n",
       " ['Come on!', 'Forza!'],\n",
       " ['Come on!', 'Vada!'],\n",
       " ['Come on!', 'Andate!'],\n",
       " ['Drop it!', 'Mollalo!'],\n",
       " ['Drop it!', 'Mollala!'],\n",
       " ['Drop it!', 'Lo molli!'],\n",
       " ['Drop it!', 'La molli!'],\n",
       " ['Drop it!', 'Mollatelo!'],\n",
       " ['Drop it!', 'Mollatela!'],\n",
       " ['Get Tom.', 'Prendi Tom.'],\n",
       " ['Get Tom.', 'Prenda Tom.'],\n",
       " ['Get Tom.', 'Prendete Tom.'],\n",
       " ['Get out!', 'Vattene!'],\n",
       " ['Get out!', 'Andatevene!'],\n",
       " ['Get out!', 'Esci!'],\n",
       " ['Get out!', 'Se ne vada!'],\n",
       " ['Get out!', 'Vattene fuori!'],\n",
       " ['Get out!', 'Se ne vada fuori!'],\n",
       " ['Get out!', 'Andatevene fuori!'],\n",
       " ['Get out!', 'Uscite!'],\n",
       " ['Get out!', 'Esca!'],\n",
       " ['Get out.', 'Esci.'],\n",
       " ['Get out.', 'Vattene.'],\n",
       " ['Get out.', 'Uscite.'],\n",
       " ['Get out.', 'Esca.'],\n",
       " ['Go away!', 'Vattene!'],\n",
       " ['Go away!', 'Andatevene!'],\n",
       " ['Go away!', 'Vai via!'],\n",
       " ['Go away!', 'Se ne vada!'],\n",
       " ['Go away!', 'Vada via!'],\n",
       " ['Go away!', 'Andate via!'],\n",
       " ['Go away.', 'Vattene.'],\n",
       " ['Go away.', 'Se ne vada.'],\n",
       " ['Go away.', 'Andatevene.'],\n",
       " ['Go away.', 'Vai via.'],\n",
       " ['Go away.', 'Andate via.'],\n",
       " ['Go away.', 'Vada via.'],\n",
       " ['Go home.', 'Vai a casa.'],\n",
       " ['Go home.', 'Vada a casa.'],\n",
       " ['Go home.', 'Andate a casa.'],\n",
       " ['Go slow.', 'Vai lentamente.'],\n",
       " ['Go slow.', 'Vada lentamente.'],\n",
       " ['Go slow.', 'Andate lentamente.'],\n",
       " ['Goodbye!', 'Arrivederci.'],\n",
       " ['Goodbye!', 'Ciao!'],\n",
       " ['Hang on!', 'Aspetta!'],\n",
       " ['Hang on!', 'Aspettate!'],\n",
       " ['Hang on!', 'Aspetti!'],\n",
       " ['Hang on!', 'Attendi!'],\n",
       " ['Hang on!', 'Attenda!'],\n",
       " ['Hang on!', 'Attendete!'],\n",
       " ['Hang on.', 'Aspetta!'],\n",
       " ['Hang on.', 'Aspettate!'],\n",
       " ['Hang on.', 'Aspetti!'],\n",
       " ['Hang on.', 'Attendi!'],\n",
       " ['Hang on.', 'Attenda!'],\n",
       " ['Hang on.', 'Attendete!'],\n",
       " ['He came.', 'È venuto.'],\n",
       " ['He came.', 'Lui è venuto.'],\n",
       " ['He quit.', 'Ha rinunciato.'],\n",
       " ['He quit.', 'Lui ha rinunciato.'],\n",
       " ['He quit.', 'Rinunciò.'],\n",
       " ['He quit.', 'Lui rinunciò.'],\n",
       " ['He runs.', 'Corre.'],\n",
       " ['He runs.', 'Lui corre.'],\n",
       " ['Help me!', 'Aiutatemi!'],\n",
       " ['Help me!', 'Mi aiuti!'],\n",
       " ['Help me.', 'Aiutami.'],\n",
       " ['Help me.', 'Mi aiuti.'],\n",
       " ['Help me.', 'Aiutatemi.'],\n",
       " ['Help us.', 'Aiutateci.'],\n",
       " ['Help us.', 'Aiutaci.'],\n",
       " ['Help us.', 'Ci aiuti.'],\n",
       " ['Hit Tom.', 'Colpisci Tom.'],\n",
       " ['Hit Tom.', 'Colpisca Tom.'],\n",
       " ['Hit Tom.', 'Colpite Tom.'],\n",
       " ['Hold it!', 'Aspetta!'],\n",
       " ['Hold it!', 'Aspettate!'],\n",
       " ['Hold it!', 'Aspetti!'],\n",
       " ['Hold it!', 'Aspetta un attimo!'],\n",
       " ['Hold it!', 'Aspetti un attimo!'],\n",
       " ['Hold it!', 'Aspettate un attimo!'],\n",
       " ['Hold it!', 'Un attimo!'],\n",
       " ['Hug Tom.', 'Abbraccia Tom.'],\n",
       " ['Hug Tom.', 'Abbracci Tom.'],\n",
       " ['Hug Tom.', 'Abbracciate Tom.'],\n",
       " ['I agree.', \"Io sono d'accordo.\"],\n",
       " ['I bowed.', 'Mi sono inchinato.'],\n",
       " ['I bowed.', 'Mi sono inchinata.'],\n",
       " ['I bowed.', 'Mi inchinai.'],\n",
       " ['I cried.', 'Ho pianto.'],\n",
       " ['I cried.', 'Piansi.'],\n",
       " ['I dozed.', 'Ho dormicchiato.'],\n",
       " ['I dozed.', 'Dormicchiai.'],\n",
       " ['I dozed.', 'Ho sonnecchiato.'],\n",
       " ['I dozed.', 'Sonnecchiai.'],\n",
       " ['I drive.', 'Guido.'],\n",
       " ['I drive.', 'Io guido.'],\n",
       " ['I drove.', 'Ho guidato.'],\n",
       " ['I drove.', 'Guidai.'],\n",
       " ['I moved.', 'Mi sono mosso.'],\n",
       " ['I moved.', 'Mi sono mossa.'],\n",
       " ['I moved.', 'Mi sono trasferito.'],\n",
       " ['I moved.', 'Mi sono trasferita.'],\n",
       " ['I moved.', 'Mi mossi.'],\n",
       " ['I moved.', 'Mi trasferii.'],\n",
       " ['I slept.', 'Ho dormito.'],\n",
       " ['I slept.', 'Io ho dormito.'],\n",
       " ['I slept.', 'Dormii.'],\n",
       " ['I slept.', 'Io dormii.'],\n",
       " ['I smoke.', 'Io fumo.'],\n",
       " ['I smoke.', 'Fumo.'],\n",
       " ['I snore.', 'Russo.'],\n",
       " ['I snore.', 'Io russo.'],\n",
       " ['I stink.', 'Puzzo.'],\n",
       " ['I stink.', 'Io puzzo.'],\n",
       " ['I swore.', 'Ho giurato.'],\n",
       " ['I swore.', 'Giurai.'],\n",
       " [\"I'll go.\", 'Andrò.'],\n",
       " [\"I'll go.\", 'Io andrò.'],\n",
       " [\"I'm Tom.\", 'Sono Tom.'],\n",
       " [\"I'm Tom.\", 'Io sono Tom.'],\n",
       " [\"I'm fat.\", 'Sono grasso.'],\n",
       " [\"I'm fat.\", 'Io sono grasso.'],\n",
       " [\"I'm fat.\", 'Sono grassa.'],\n",
       " [\"I'm fat.\", 'Io sono grassa.'],\n",
       " [\"I'm fit.\", 'Sono in forma.'],\n",
       " [\"I'm fit.\", 'Io sono in forma.'],\n",
       " [\"I'm hit!\", 'Sono stato colpito!'],\n",
       " [\"I'm hit!\", 'Sono stata colpita!'],\n",
       " [\"I'm hot.\", 'Ho caldo.'],\n",
       " [\"I'm hot.\", 'Io ho caldo.'],\n",
       " [\"I'm ill.\", 'Sono malato.'],\n",
       " [\"I'm ill.\", 'Sono malata.'],\n",
       " [\"I'm ill.\", 'Sto male.'],\n",
       " [\"I'm ill.\", 'Sono ammalato.'],\n",
       " [\"I'm old.\", 'Sono vecchio.'],\n",
       " [\"I'm old.\", 'Sono vecchia.'],\n",
       " [\"I'm old.\", 'Io sono vecchio.'],\n",
       " [\"I'm old.\", 'Io sono vecchia.'],\n",
       " [\"I'm sad.\", 'Sono triste.'],\n",
       " [\"I'm sad.\", 'Io sono triste.'],\n",
       " [\"I'm shy.\", 'Sono timido.'],\n",
       " [\"I'm shy.\", 'Io sono timido.'],\n",
       " [\"I'm shy.\", 'Sono timida.'],\n",
       " [\"I'm shy.\", 'Io sono timida.'],\n",
       " [\"It's OK.\", 'Va bene.'],\n",
       " ['Join us.', 'Unisciti a noi.'],\n",
       " ['Keep it.', 'Tienilo.'],\n",
       " ['Keep it.', 'Lo tenga.'],\n",
       " ['Keep it.', 'Tenetelo.'],\n",
       " ['Keep it.', 'Tienila.'],\n",
       " ['Keep it.', 'La tenga.'],\n",
       " ['Keep it.', 'Tenetela.'],\n",
       " ['Kick it.', 'Calcialo.'],\n",
       " ['Kick it.', 'Calciala.'],\n",
       " ['Kick it.', 'Lo calci.'],\n",
       " ['Kick it.', 'La calci.'],\n",
       " ['Kick it.', 'Calciatelo.'],\n",
       " ['Kick it.', 'Calciatela.'],\n",
       " ['Kiss me.', 'Baciami.'],\n",
       " ['Kiss me.', 'Baciatemi.'],\n",
       " ['Kiss me.', 'Mi baci.'],\n",
       " ['Lock it.', 'Chiudilo a chiave.'],\n",
       " ['Lock it.', 'Chiudila a chiave.'],\n",
       " ['Lock it.', 'Lo chiuda a chiave.'],\n",
       " ['Lock it.', 'Chiudetelo a chiave.'],\n",
       " ['Lock it.', 'La chiuda a chiave.'],\n",
       " ['Lock it.', 'Chiudetela a chiave.'],\n",
       " ['Me, too.', \"Anch'io.\"],\n",
       " ['Me, too.', 'Anche io.'],\n",
       " ['Me, too.', 'Pure io.'],\n",
       " ['Open up.', 'Apriti.'],\n",
       " ['Open up.', 'Apritevi.'],\n",
       " ['Open up.', 'Si apra.'],\n",
       " ['Open up.', 'Apri.'],\n",
       " ['Open up.', 'Apra.'],\n",
       " ['Open up.', 'Aprite.'],\n",
       " ['Open up.', 'Confidati.'],\n",
       " ['Open up.', 'Si confidi.'],\n",
       " ['Open up.', 'Confidatevi.'],\n",
       " ['Perfect!', 'Perfetto!'],\n",
       " ['Pull it.', 'Tiralo.'],\n",
       " ['Pull it.', 'Tirala.'],\n",
       " ['Pull it.', 'Lo tiri.'],\n",
       " ['Pull it.', 'La tiri.'],\n",
       " ['Pull it.', 'Tiratelo.'],\n",
       " ['Pull it.', 'Tiratela.'],\n",
       " ['Push it.', 'Spingilo.'],\n",
       " ['Push it.', 'Spingila.'],\n",
       " ['Push it.', 'Lo spinga.'],\n",
       " ['Push it.', 'La spinga.'],\n",
       " ['Push it.', 'Spingetelo.'],\n",
       " ['Push it.', 'Spingetela.'],\n",
       " ['See you!', 'Arrivederci.'],\n",
       " ['See you!', 'Ci si vede!'],\n",
       " ['See you.', 'Arrivederci.'],\n",
       " ['See you.', 'Ci si vede!'],\n",
       " ['See you.', 'Ci si vede.'],\n",
       " ['See you.', 'Ci vediamo.'],\n",
       " ['Show me.', 'Fammi vedere.'],\n",
       " ['Show me.', 'Mostrami.'],\n",
       " ['Show me.', 'Fatemi vedere.'],\n",
       " ['Show me.', 'Mi faccia vedere.'],\n",
       " ['Show me.', 'Mostratemi.'],\n",
       " ['Show me.', 'Mi mostri.'],\n",
       " ['Shut up!', 'Taci!'],\n",
       " ['Shut up!', 'Stai zitto!'],\n",
       " ['Shut up!', 'Stai zitta!'],\n",
       " ['Shut up!', 'Stia zitto!'],\n",
       " ['Shut up!', 'Stia zitta!'],\n",
       " ['Shut up!', 'State zitti!'],\n",
       " ['Shut up!', 'State zitte!'],\n",
       " ['Shut up!', 'Silenzio!'],\n",
       " ['Shut up!', 'Tacete!'],\n",
       " ['Shut up!', 'Taccia!'],\n",
       " ['Shut up!', \"Sta' zitto!\"],\n",
       " ['Skip it.', 'Saltalo.'],\n",
       " ['Skip it.', 'Saltala.'],\n",
       " ['Skip it.', 'Lo salti.'],\n",
       " ['Skip it.', 'La salti.'],\n",
       " ['Skip it.', 'Saltatelo.'],\n",
       " ['Skip it.', 'Saltatela.'],\n",
       " ['So long.', 'A tra poco!'],\n",
       " ['Stop it.', 'Smettila!'],\n",
       " ['Stop it.', 'La smetta!'],\n",
       " ['Stop it.', 'Smettetela!'],\n",
       " ['Tom ate.', 'Tom ha mangiato.'],\n",
       " ['Tom ate.', 'Tom mangiò.'],\n",
       " ['Tom ran.', 'Tom ha corso.'],\n",
       " ['Tom ran.', 'Tom corse.'],\n",
       " ['Tom won.', 'Tom ha vinto.'],\n",
       " ['Tom won.', 'Tom vinse.'],\n",
       " ['Tom won.', 'Ha vinto Tom.'],\n",
       " ['Tom won.', 'Vinse Tom.'],\n",
       " ['Wait up.', 'Aspetta.'],\n",
       " ['Wait up.', 'Aspetti.'],\n",
       " ['Wait up.', 'Aspettate.'],\n",
       " ['Wake up!', 'Sveglia!'],\n",
       " ['Wake up!', 'Svegliati!'],\n",
       " ['Wake up!', 'Svegliatevi!'],\n",
       " ['Wake up!', 'Si svegli!'],\n",
       " ['Wake up.', 'Alzati.'],\n",
       " ['Wash up.', 'Lavati.'],\n",
       " ['Wash up.', 'Lavatevi.'],\n",
       " ['Wash up.', 'Si lavi.'],\n",
       " ['We care.', 'A noi importa.'],\n",
       " ['We know.', 'Lo sappiamo.'],\n",
       " ['We know.', 'Noi lo sappiamo.'],\n",
       " ['We know.', 'Sappiamo.'],\n",
       " ['We know.', 'Noi sappiamo.'],\n",
       " ['We lost.', 'Abbiamo perso.'],\n",
       " ['We lost.', 'Noi abbiamo perso.'],\n",
       " ['We lost.', 'Perdemmo.'],\n",
       " ['We lost.', 'Noi perdemmo.'],\n",
       " ['Welcome.', 'Benvenuto!'],\n",
       " ['Welcome.', 'Benvenuta!'],\n",
       " ['Welcome.', 'Benvenuti!'],\n",
       " ['Welcome.', 'Benvenute!'],\n",
       " ['Welcome.', 'Ben arrivato!'],\n",
       " ['Who ate?', 'Chi ha mangiato?'],\n",
       " ['Who ran?', 'Chi ha corso?'],\n",
       " ['Who won?', 'Chi ha vinto?'],\n",
       " ['Why not?', 'Perché no?'],\n",
       " ['You run.', 'Corri.'],\n",
       " ['You run.', 'Correte.'],\n",
       " ['You run.', 'Corra.'],\n",
       " ['You won.', 'Hai vinto.'],\n",
       " ['You won.', 'Ha vinto.'],\n",
       " ['You won.', 'Avete vinto.'],\n",
       " ['Am I fat?', 'Sono grasso?'],\n",
       " ['Am I fat?', 'Io sono grasso?'],\n",
       " ['Am I fat?', 'Sono grassa?'],\n",
       " ['Am I fat?', 'Io sono grassa?'],\n",
       " ['Ask them.', 'Chiedilo a loro.'],\n",
       " ['Ask them.', 'Lo chieda a loro.'],\n",
       " ['Ask them.', 'Chiedetelo a loro.'],\n",
       " ['Back off!', 'Stai indietro!'],\n",
       " ['Back off!', 'Stia indietro!'],\n",
       " ['Back off!', 'State indietro!'],\n",
       " ['Back off.', 'Indietreggia.'],\n",
       " ['Back off.', 'Indietreggi.'],\n",
       " ['Back off.', 'Indietreggiate.'],\n",
       " ['Back off.', 'Tirati indietro.'],\n",
       " ['Back off.', 'Si tiri indietro.'],\n",
       " ['Back off.', 'Tiratevi indietro.'],\n",
       " ['Be brave.', 'Sii coraggioso.'],\n",
       " ['Be brave.', 'Sii coraggiosa.'],\n",
       " ['Be brave.', 'Sia coraggioso.'],\n",
       " ['Be brave.', 'Sia coraggiosa.'],\n",
       " ['Be brave.', 'Siate coraggiosi.'],\n",
       " ['Be brave.', 'Siate coraggiose.'],\n",
       " ['Be brief.', 'Siate brevi.'],\n",
       " ['Be brief.', 'Sii breve.'],\n",
       " ['Be brief.', 'Sia breve.'],\n",
       " ['Be still.', 'Stai fermo.'],\n",
       " ['Be still.', 'Stai ferma.'],\n",
       " ['Be still.', 'Stia fermo.'],\n",
       " ['Be still.', 'Stia ferma.'],\n",
       " ['Be still.', 'State fermi.'],\n",
       " ['Be still.', 'State ferme.'],\n",
       " ['Buzz off.', 'Levati dai piedi.'],\n",
       " ['Buzz off.', 'Si levi dai piedi.'],\n",
       " ['Buzz off.', 'Levatevi dai piedi.'],\n",
       " ['Bye, Tom.', 'Arrivederci, Tom.'],\n",
       " ['Bye, Tom.', 'Ci vediamo, Tom.'],\n",
       " ['Bye, Tom.', 'Ci si vede, Tom.'],\n",
       " ['Call Tom.', 'Chiama Tom.'],\n",
       " ['Call Tom.', 'Chiami Tom.'],\n",
       " ['Call Tom.', 'Chiamate Tom.'],\n",
       " ['Can I go?', 'Posso andare?'],\n",
       " ['Cheer up!', 'Coraggio!'],\n",
       " ['Cheer up!', 'Su col morale!'],\n",
       " ['Cheer up!', 'Su con la vita!'],\n",
       " ['Cool off!', 'Calmati!'],\n",
       " ['Cool off!', 'Rilassati!'],\n",
       " ['Cool off!', 'Rilassatevi!'],\n",
       " ['Cool off!', 'Si rilassi!'],\n",
       " ['Cool off!', 'Si calmi!'],\n",
       " ['Cool off!', 'Calmatevi!'],\n",
       " ['Cuff him.', 'Ammanettalo.'],\n",
       " ['Cuff him.', 'Lo ammanetti.'],\n",
       " ['Cuff him.', 'Ammanettatelo.'],\n",
       " [\"Don't go.\", 'Non andare.'],\n",
       " [\"Don't go.\", 'Non andate.'],\n",
       " [\"Don't go.\", 'Non vada.'],\n",
       " ['Drive on.', 'Continua a guidare.'],\n",
       " ['Drive on.', 'Continui a guidare.'],\n",
       " ['Drive on.', 'Continuate a guidare.'],\n",
       " ['Find Tom.', 'Trova Tom.'],\n",
       " ['Find Tom.', 'Trovate Tom.'],\n",
       " ['Find Tom.', 'Trovi Tom.'],\n",
       " ['Fix this.', 'Ripara questo.'],\n",
       " ['Fix this.', 'Riparate questo.'],\n",
       " ['Fix this.', 'Ripari questo.'],\n",
       " ['Fix this.', 'Fissa questo.'],\n",
       " ['Fix this.', 'Fissate questo.'],\n",
       " ['Fix this.', 'Fissi questo.'],\n",
       " ['Get away!', 'Vattene!'],\n",
       " ['Get away!', 'Andatevene!'],\n",
       " ['Get away!', 'Vai via!'],\n",
       " ['Get away!', 'Vattene.'],\n",
       " ['Get away!', 'Se ne vada.'],\n",
       " ['Get away!', 'Andatevene.'],\n",
       " ['Get away!', 'Vai via.'],\n",
       " ['Get away!', 'Andate via.'],\n",
       " ['Get away!', 'Vada via.'],\n",
       " ['Get away!', 'Se ne vada!'],\n",
       " ['Get away!', 'Vada via!'],\n",
       " ['Get away!', 'Andate via!'],\n",
       " ['Get down!', 'Vieni giù!'],\n",
       " ['Get down!', 'Venite giù!'],\n",
       " ['Get down!', 'Venga giù!'],\n",
       " ['Get lost!', 'Smamma!'],\n",
       " ['Get lost!', 'Smammate!'],\n",
       " ['Get lost!', 'Smammi!'],\n",
       " ['Get lost.', 'Smamma.'],\n",
       " ['Get lost.', 'Smammi.'],\n",
       " ['Get lost.', 'Smammate.'],\n",
       " ['Get lost.', 'Sgomma.'],\n",
       " ['Get lost.', 'Sgommate.'],\n",
       " ['Get lost.', 'Sgommi.'],\n",
       " ['Get lost.', 'Vai al diavolo.'],\n",
       " ['Get lost.', 'Vada al diavolo.'],\n",
       " ['Get lost.', 'Andate al diavolo.'],\n",
       " ['Get real.', 'Sii realista.'],\n",
       " ['Get real.', 'Sia realista.'],\n",
       " ['Get real.', 'Siate realisti.'],\n",
       " ['Get real.', 'Siate realiste.'],\n",
       " ['Go ahead!', 'Vai pure!'],\n",
       " ['Go ahead!', 'Vada pure!'],\n",
       " ['Go ahead!', 'Andate pure!'],\n",
       " ['Go ahead.', 'Vai pure.'],\n",
       " ['Go ahead.', 'Vai avanti.'],\n",
       " ['Good job!', 'Buon lavoro!'],\n",
       " ['Grab Tom.', 'Agguanta Tom.'],\n",
       " ['Grab Tom.', 'Agguantate Tom.'],\n",
       " ['Grab Tom.', 'Agguanti Tom.'],\n",
       " ['Grab him.', 'Afferralo.'],\n",
       " ['Grab him.', 'Afferratelo.'],\n",
       " ['Grab him.', 'Lo afferri.'],\n",
       " ['Have fun.', 'Divertiti!'],\n",
       " ['Have fun.', 'Divertitevi!'],\n",
       " ['Have fun.', 'Si diverta!'],\n",
       " ['He spoke.', 'Ha parlato.'],\n",
       " ['He spoke.', 'Lui ha parlato.'],\n",
       " ['He spoke.', 'Parlò.'],\n",
       " ['He spoke.', 'Lui parlò.'],\n",
       " ['He tried.', 'Provò.'],\n",
       " ['He tried.', 'Lui provò.'],\n",
       " ['He tries.', 'Prova.'],\n",
       " ['He tries.', 'Lui prova.'],\n",
       " [\"He's wet.\", 'È bagnato.'],\n",
       " [\"He's wet.\", 'Lui è bagnato.'],\n",
       " ['Help Tom.', 'Aiuta Tom.'],\n",
       " ['Help Tom.', 'Aiutate Tom.'],\n",
       " ['Help Tom.', 'Aiuti Tom.'],\n",
       " ['Help him.', 'Aiutatelo.'],\n",
       " ['Help him.', 'Aiutalo.'],\n",
       " ['Help him.', 'Lo aiuti.'],\n",
       " ['How cute!', 'Che carino!'],\n",
       " ['How cute!', 'Che carina!'],\n",
       " ['How cute!', 'Che carini!'],\n",
       " ['How cute!', 'Che carine!'],\n",
       " ['How deep?', 'Quanto profondo?'],\n",
       " ['How deep?', 'Quanto profonda?'],\n",
       " ['How deep?', 'Quanto profondi?'],\n",
       " ['How deep?', 'Quanto profonde?'],\n",
       " ['How nice!', 'Che bella!'],\n",
       " ['How nice!', 'Che belli!'],\n",
       " ['How nice!', 'Che belle!'],\n",
       " ['Humor me.', 'Assecondami.'],\n",
       " ['Humor me.', 'Mi assecondi.'],\n",
       " ['Humor me.', 'Assecondatemi.'],\n",
       " ['Humor me.', 'Fammi ridere.'],\n",
       " ['Humor me.', 'Mi faccia ridere.'],\n",
       " ['Humor me.', 'Fatemi ridere.'],\n",
       " ['Hurry up.', 'Svelto!'],\n",
       " ['Hurry up.', 'Sbrigati!'],\n",
       " ['Hurry up.', 'Sbrigatevi!'],\n",
       " ['Hurry up.', 'Si sbrighi!'],\n",
       " ['I agreed.', \"Ero d'accordo.\"],\n",
       " ['I agreed.', \"Io ero d'accordo.\"],\n",
       " ['I am fat.', 'Sono grasso.'],\n",
       " ['I am fat.', 'Io sono grasso.'],\n",
       " ['I am fat.', 'Sono grassa.'],\n",
       " ['I am fat.', 'Io sono grassa.'],\n",
       " ['I am hot.', 'Ho caldo.'],\n",
       " ['I am hot.', 'Io ho caldo.'],\n",
       " ['I am old.', 'Sono vecchio.'],\n",
       " ['I am old.', 'Sono vecchia.'],\n",
       " ['I ate it.', \"L'ho mangiato.\"],\n",
       " ['I ate it.', \"L'ho mangiata.\"],\n",
       " ['I burped.', 'Ruttai.'],\n",
       " ['I danced.', 'Ballai.'],\n",
       " ['I danced.', 'Danzai.'],\n",
       " ['I failed.', 'Ho fallito.'],\n",
       " ['I gasped.', 'Ho rantolato.'],\n",
       " ['I gasped.', 'Rantolai.'],\n",
       " ['I gasped.', 'Ho ansimato.'],\n",
       " ['I gasped.', 'Ansimai.'],\n",
       " ['I got it.', 'Ho capito.'],\n",
       " ['I got it.', 'Ho compreso.'],\n",
       " ['I got it.', 'Io ho capito.'],\n",
       " ['I got it.', 'Io ho compreso.'],\n",
       " ['I helped.', 'Aiutai.'],\n",
       " ['I jumped.', 'Ho saltato.'],\n",
       " ['I jumped.', 'Saltai.'],\n",
       " ['I looked.', 'Ho guardato.'],\n",
       " ['I looked.', 'Guardai.'],\n",
       " ['I moaned.', 'Ho gemuto.'],\n",
       " ['I moaned.', 'Gemetti.'],\n",
       " ['I moaned.', 'Ho frignato.'],\n",
       " ['I moaned.', 'Frignai.'],\n",
       " ['I moaned.', 'Gemettei.'],\n",
       " ['I nodded.', 'Ho annuito.'],\n",
       " ['I nodded.', 'Annuii.'],\n",
       " ['I obeyed.', 'Ho ubbidito.'],\n",
       " ['I obeyed.', 'Ho obbedito.'],\n",
       " ['I paused.', 'Mi sono fermato.'],\n",
       " ['I paused.', 'Mi sono fermata.'],\n",
       " ['I paused.', 'Mi fermai.'],\n",
       " ['I paused.', 'Mi sono interrotto.'],\n",
       " ['I paused.', 'Mi sono interrotta.'],\n",
       " ['I phoned.', 'Ho telefonato.'],\n",
       " ['I phoned.', 'Io ho telefonato.'],\n",
       " ['I phoned.', 'Telefonai.'],\n",
       " ['I phoned.', 'Io telefonai.'],\n",
       " ['I prayed.', 'Pregai.'],\n",
       " ['I refuse.', 'Mi rifiuto.'],\n",
       " ['I refuse.', 'Io mi rifiuto.'],\n",
       " ['I rested.', 'Mi sono riposato.'],\n",
       " ['I rested.', 'Mi sono riposata.'],\n",
       " ['I rested.', 'Mi riposai.'],\n",
       " ['I shaved.', 'Mi sono rasato.'],\n",
       " ['I shaved.', 'Mi sono rasata.'],\n",
       " ['I shaved.', 'Mi rasai.'],\n",
       " ['I sighed.', 'Ho sospirato.'],\n",
       " ['I sighed.', 'Sospirai.'],\n",
       " ['I smiled.', 'Ho sorriso.'],\n",
       " ['I smiled.', 'Io ho sorriso.'],\n",
       " ['I smiled.', 'Sorrisi.'],\n",
       " ['I smiled.', 'Io sorrisi.'],\n",
       " ['I stayed.', 'Sono rimasto.'],\n",
       " ['I stayed.', 'Io sono rimasto.'],\n",
       " ['I stayed.', 'Sono rimasta.'],\n",
       " ['I stayed.', 'Io sono rimasta.'],\n",
       " ['I stayed.', 'Rimasi.'],\n",
       " ['I stayed.', 'Io rimasi.'],\n",
       " ['I stayed.', 'Restai.'],\n",
       " ['I stayed.', 'Io restai.'],\n",
       " ['I stayed.', 'Sono restato.'],\n",
       " ['I stayed.', 'Io sono restato.'],\n",
       " ['I stayed.', 'Sono restata.'],\n",
       " ['I stayed.', 'Io sono restata.'],\n",
       " ['I talked.', 'Ho parlato.'],\n",
       " ['I talked.', 'Parlai.'],\n",
       " ['I use it.', 'Lo uso.'],\n",
       " ['I use it.', 'La uso.'],\n",
       " ['I use it.', 'Io lo uso.'],\n",
       " ['I use it.', 'Io la uso.'],\n",
       " ['I use it.', 'Lo utilizzo.'],\n",
       " ['I use it.', 'Io lo utilizzo.'],\n",
       " ['I use it.', 'La utilizzo.'],\n",
       " ['I use it.', 'Io la utilizzo.'],\n",
       " ['I waited.', 'Ho aspettato.'],\n",
       " ['I waited.', 'Io ho aspettato.'],\n",
       " ['I waited.', 'Aspettai.'],\n",
       " ['I waited.', 'Io aspettai.'],\n",
       " ['I waited.', 'Aspettavo.'],\n",
       " ['I waited.', 'Io aspettavo.'],\n",
       " ['I winked.', \"Ho fatto l'occhiolino.\"],\n",
       " ['I winked.', \"Feci l'occhiolino.\"],\n",
       " ['I winked.', \"Ho strizzato l'occhio.\"],\n",
       " ['I winked.', \"Strizzai l'occhio.\"],\n",
       " ['I yawned.', 'Ho sbadigliato.'],\n",
       " ['I yawned.', 'Sbadigliai.'],\n",
       " [\"I'll die.\", 'Morirò.'],\n",
       " [\"I'll die.\", 'Io morirò.'],\n",
       " [\"I'll pay.\", 'Pago io.'],\n",
       " [\"I'll pay.\", 'Pagherò io.'],\n",
       " [\"I'll win.\", 'Vincerò.'],\n",
       " [\"I'll win.\", 'Io vincerò.'],\n",
       " [\"I'm a DJ.\", 'Sono un DJ.'],\n",
       " [\"I'm a DJ.\", 'Io sono un DJ.'],\n",
       " [\"I'm a DJ.\", 'Sono una DJ.'],\n",
       " [\"I'm a DJ.\", 'Io sono una DJ.'],\n",
       " [\"I'm back.\", 'Sono tornato.'],\n",
       " [\"I'm back.\", 'Sono ritornato.'],\n",
       " [\"I'm back.\", 'Sono tornata.'],\n",
       " [\"I'm bald.\", 'Sono calvo.'],\n",
       " [\"I'm bald.\", 'Io sono calvo.'],\n",
       " [\"I'm bald.\", 'Sono calva.'],\n",
       " [\"I'm bald.\", 'Io sono calva.'],\n",
       " [\"I'm busy.\", 'Sono occupato.'],\n",
       " [\"I'm busy.\", 'Sono impegnato.'],\n",
       " [\"I'm busy.\", 'Io sono impegnato.'],\n",
       " [\"I'm busy.\", 'Sono impegnata.'],\n",
       " [\"I'm busy.\", 'Io sono impegnata.'],\n",
       " [\"I'm busy.\", 'Io sono occupato.'],\n",
       " [\"I'm busy.\", 'Sono occupata.'],\n",
       " [\"I'm busy.\", 'Io sono occupata.'],\n",
       " [\"I'm calm.\", 'Sono calmo.'],\n",
       " [\"I'm calm.\", 'Io sono calmo.'],\n",
       " [\"I'm calm.\", 'Sono calma.'],\n",
       " [\"I'm calm.\", 'Io sono calma.'],\n",
       " [\"I'm cold.\", 'Ho freddo.'],\n",
       " [\"I'm cool.\", 'Sono figo.'],\n",
       " [\"I'm cool.\", 'Io sono figo.'],\n",
       " [\"I'm cool.\", 'Sono alla moda.'],\n",
       " [\"I'm cool.\", 'Io sono alla moda.'],\n",
       " [\"I'm deaf.\", 'Sono sordo.'],\n",
       " [\"I'm deaf.\", 'Io sono sordo.'],\n",
       " [\"I'm deaf.\", 'Sono sorda.'],\n",
       " [\"I'm deaf.\", 'Io sono sorda.'],\n",
       " [\"I'm done.\", 'Ho finito.'],\n",
       " [\"I'm done.\", 'Io ho finito.'],\n",
       " [\"I'm fast.\", 'Sono veloce.'],\n",
       " [\"I'm fast.\", 'Io sono veloce.'],\n",
       " [\"I'm fine.\", 'Sto bene.'],\n",
       " [\"I'm fine.\", 'Mi sento bene.'],\n",
       " [\"I'm free!\", 'Io sono libero!'],\n",
       " [\"I'm free.\", 'Sono libero.'],\n",
       " [\"I'm free.\", 'Sono libera.'],\n",
       " [\"I'm free.\", 'Sono gratuito.'],\n",
       " [\"I'm free.\", 'Sono gratuita.'],\n",
       " [\"I'm full.\", 'Sono pieno.'],\n",
       " [\"I'm full.\", 'Sono piena.'],\n",
       " [\"I'm full.\", 'Io sono pieno.'],\n",
       " [\"I'm full.\", 'Io sono piena.'],\n",
       " [\"I'm glad.\", 'Sono felice.'],\n",
       " [\"I'm glad.\", 'Io sono felice.'],\n",
       " [\"I'm glad.\", 'Io sono contento.'],\n",
       " [\"I'm glad.\", 'Sono contenta.'],\n",
       " [\"I'm glad.\", 'Io sono contenta.'],\n",
       " [\"I'm here.\", 'Sono qua.'],\n",
       " [\"I'm here.\", 'Sono qui.'],\n",
       " [\"I'm here.\", 'Io sono qui.'],\n",
       " [\"I'm here.\", 'Io sono qua.'],\n",
       " [\"I'm home.\", 'Sono a casa.'],\n",
       " [\"I'm home.\", 'Io sono a casa.'],\n",
       " [\"I'm hurt.\", 'Sono ferito.'],\n",
       " [\"I'm hurt.\", 'Io sono ferito.'],\n",
       " [\"I'm hurt.\", 'Sono ferita.'],\n",
       " [\"I'm hurt.\", 'Io sono ferita.'],\n",
       " [\"I'm late.\", 'Sono in ritardo.'],\n",
       " [\"I'm late.\", 'Io sono in ritardo.'],\n",
       " [\"I'm lazy.\", 'Sono pigro.'],\n",
       " [\"I'm lazy.\", 'Io sono pigro.'],\n",
       " [\"I'm lazy.\", 'Sono pigra.'],\n",
       " [\"I'm lazy.\", 'Io sono pigra.'],\n",
       " [\"I'm lost.\", 'Sono perso.'],\n",
       " [\"I'm lost.\", 'Io sono perso.'],\n",
       " [\"I'm lost.\", 'Sono persa.'],\n",
       " [\"I'm lost.\", 'Io sono persa.'],\n",
       " [\"I'm mean.\", 'Sono meschino.'],\n",
       " [\"I'm mean.\", 'Io sono meschino.'],\n",
       " [\"I'm mean.\", 'Sono meschina.'],\n",
       " [\"I'm mean.\", 'Io sono meschina.'],\n",
       " [\"I'm numb.\", 'Sono insensibile.'],\n",
       " [\"I'm numb.\", 'Io sono insensibile.'],\n",
       " [\"I'm numb.\", 'Sono intorpidito.'],\n",
       " [\"I'm numb.\", 'Io sono intorpidito.'],\n",
       " [\"I'm numb.\", 'Sono intorpidita.'],\n",
       " [\"I'm numb.\", 'Io sono intorpidita.'],\n",
       " [\"I'm poor.\", 'Sono povero.'],\n",
       " [\"I'm poor.\", 'Io sono povero.'],\n",
       " [\"I'm poor.\", 'Io sono povera.'],\n",
       " [\"I'm rich.\", 'Sono ricco.'],\n",
       " [\"I'm rich.\", 'Io sono ricco.'],\n",
       " [\"I'm rich.\", 'Sono ricca.'],\n",
       " [\"I'm rich.\", 'Io sono ricca.'],\n",
       " [\"I'm safe.\", 'Sono al sicuro.'],\n",
       " [\"I'm safe.\", 'Io sono al sicuro.'],\n",
       " [\"I'm sick.\", 'Sono malato.'],\n",
       " [\"I'm sick.\", 'Sono malata.'],\n",
       " [\"I'm sick.\", 'Sto male.'],\n",
       " [\"I'm sick.\", 'Sono ammalato.'],\n",
       " [\"I'm slow.\", 'Sono lento.'],\n",
       " [\"I'm slow.\", 'Io sono lento.'],\n",
       " [\"I'm slow.\", 'Sono lenta.'],\n",
       " [\"I'm slow.\", 'Io sono lenta.'],\n",
       " [\"I'm sure.\", 'Io sono positivo.'],\n",
       " [\"I'm thin.\", 'Sono magro.'],\n",
       " [\"I'm thin.\", 'Io sono magro.'],\n",
       " [\"I'm thin.\", 'Sono magra.'],\n",
       " [\"I'm thin.\", 'Io sono magra.'],\n",
       " [\"I'm tidy.\", 'Sono ordinato.'],\n",
       " [\"I'm tidy.\", 'Io sono ordinato.'],\n",
       " [\"I'm tidy.\", 'Sono ordinata.'],\n",
       " [\"I'm tidy.\", 'Io sono ordinata.'],\n",
       " [\"I'm weak.\", 'Sono debole.'],\n",
       " [\"I'm weak.\", 'Io sono debole.'],\n",
       " [\"I'm wise.\", 'Sono saggio.'],\n",
       " [\"I'm wise.\", 'Io sono saggio.'],\n",
       " [\"I'm wise.\", 'Sono saggia.'],\n",
       " [\"I'm wise.\", 'Io sono saggia.'],\n",
       " ['It helps.', 'Aiuta.'],\n",
       " ['It hurts.', 'Fa male.'],\n",
       " ['It works.', 'Questa funziona.'],\n",
       " [\"It's Tom.\", 'È Tom.'],\n",
       " [\"It's fun.\", 'È divertente.'],\n",
       " [\"It's his.\", 'È suo.'],\n",
       " [\"It's his.\", 'È sua.'],\n",
       " [\"It's hot.\", 'Fa caldo.'],\n",
       " [\"It's hot.\", \"C'è caldo.\"],\n",
       " [\"It's new.\", 'È nuovo.'],\n",
       " [\"It's new.\", 'È nuova.'],\n",
       " [\"It's odd.\", 'È strano.'],\n",
       " [\"It's odd.\", 'È strana.'],\n",
       " [\"It's red.\", 'È rosso.'],\n",
       " [\"It's red.\", 'È rossa.'],\n",
       " [\"It's sad.\", 'È triste.'],\n",
       " ['Keep out.', 'Non entrare.'],\n",
       " ['Kill Tom.', 'Uccidi Tom.'],\n",
       " ['Kill Tom.', 'Uccida Tom.'],\n",
       " ['Kill Tom.', 'Uccidete Tom.'],\n",
       " ['Kiss Tom.', 'Bacia Tom.'],\n",
       " ['Kiss Tom.', 'Baciate Tom.'],\n",
       " ['Kiss Tom.', 'Baci Tom.'],\n",
       " ['Leave it.', 'Lascialo.'],\n",
       " ['Leave it.', 'Lasciala.'],\n",
       " ['Leave it.', 'Lo lasci.'],\n",
       " ['Leave it.', 'La lasci.'],\n",
       " ['Leave it.', 'Lasciatelo.'],\n",
       " ['Leave it.', 'Lasciatela.'],\n",
       " ['Leave me.', 'Lasciami.'],\n",
       " ['Leave me.', 'Lasciatemi.'],\n",
       " ['Leave me.', 'Mi lasci.'],\n",
       " ['Leave us.', 'Lasciaci.'],\n",
       " ['Leave us.', 'Lasciateci.'],\n",
       " ['Leave us.', 'Ci lasci.'],\n",
       " [\"Let's go!\", 'Andiamo!'],\n",
       " [\"Let's go.\", 'Andiamo!'],\n",
       " ['Look out!', 'Attenzione!'],\n",
       " ['Look out!', 'Occhio!'],\n",
       " ['Marry me.', 'Sposami.'],\n",
       " ['Marry me.', 'Sposatemi.'],\n",
       " ['Marry me.', 'Mi sposi.'],\n",
       " ['Save Tom.', 'Salva Tom.'],\n",
       " ['Save Tom.', 'Salvate Tom.'],\n",
       " ['Save Tom.', 'Salvi Tom.'],\n",
       " ['She came.', 'È venuta.'],\n",
       " ['She came.', 'Lei è venuta.'],\n",
       " ['She died.', 'È morta.'],\n",
       " ['She died.', 'Lei è morta.'],\n",
       " ['She runs.', 'Corre.'],\n",
       " ['Sit down!', 'Siediti!'],\n",
       " ['Sit down.', 'Siediti.'],\n",
       " ['Sit down.', 'Si sieda.'],\n",
       " ['Sit down.', 'Sedetevi.'],\n",
       " ['Sit here.', 'Siediti qui.'],\n",
       " ['Sit here.', 'Si sieda qui.'],\n",
       " ['Sit here.', 'Sedetevi qui.'],\n",
       " ['Speak up!', 'Parla più forte!'],\n",
       " ['Speak up!', 'Parlate più forte!'],\n",
       " ['Speak up!', 'Parli più forte!'],\n",
       " ['Stand by.', 'Resta in attesa.'],\n",
       " ['Stand by.', 'Restate in attesa.'],\n",
       " ['Stand by.', 'Resti in attesa.'],\n",
       " ['Stand by.', 'Rimani in attesa.'],\n",
       " ['Stand by.', 'Rimanga in attesa.'],\n",
       " ['Stand by.', 'Rimanete in attesa.'],\n",
       " ['Stand up!', 'In piedi!'],\n",
       " ['Stand up!', 'Alzati in piedi!'],\n",
       " ['Stand up!', 'Alzatevi in piedi!'],\n",
       " ['Stand up!', 'Si alzi in piedi!'],\n",
       " ['Stand up.', 'Alzati.'],\n",
       " ['Stand up.', 'Alzatevi.'],\n",
       " ['Stand up.', 'Si alzi.'],\n",
       " ['Stay put.', 'Stai fermo.'],\n",
       " ['Stay put.', 'Stai ferma.'],\n",
       " ['Stay put.', 'Stia fermo.'],\n",
       " ['Stay put.', 'Stia ferma.'],\n",
       " ['Stay put.', 'State fermi.'],\n",
       " ['Stay put.', 'State ferme.'],\n",
       " ['Stay put.', 'Fermo lì.'],\n",
       " ['Stay put.', 'Ferma lì.'],\n",
       " ['Stay put.', 'Fermi lì.'],\n",
       " ['Stay put.', 'Ferme lì.'],\n",
       " ['Stop Tom.', 'Ferma Tom.'],\n",
       " ['Stop Tom.', 'Fermi Tom.'],\n",
       " ['Stop Tom.', 'Fermate Tom.'],\n",
       " ['Take Tom.', 'Prendi Tom.'],\n",
       " ['Take Tom.', 'Prenda Tom.'],\n",
       " ['Take Tom.', 'Prendete Tom.'],\n",
       " ['Tell Tom.', 'Dillo a Tom.'],\n",
       " ['Tell Tom.', 'Ditelo a Tom.'],\n",
       " ['Tell Tom.', 'Lo dica a Tom.'],\n",
       " ['Terrific!', 'Formidabile!'],\n",
       " ['Tom came.', 'Tom è venuto.'],\n",
       " ['Tom came.', 'Tom venne.'],\n",
       " ['Tom came.', 'È venuto Tom.'],\n",
       " ['Tom came.', 'Venne Tom.'],\n",
       " ['Tom died.', 'Tom è morto.'],\n",
       " ['Tom died.', 'Tom morì.'],\n",
       " ['Tom fell.', 'Tom è caduto.'],\n",
       " ['Tom fell.', 'Tom cadde.'],\n",
       " ['Tom fled.', 'Tom è fuggito.'],\n",
       " ['Tom fled.', 'Tom fuggì'],\n",
       " ['Tom knew.', 'Tom lo sapeva.'],\n",
       " ['Tom left.', 'Tom è partito.'],\n",
       " ['Tom left.', \"Tom se n'è andato.\"],\n",
       " ['Tom left.', 'Tom se ne andò.'],\n",
       " ['Tom left.', 'Tom partì.'],\n",
       " ['Tom lied.', 'Tom ha mentito.'],\n",
       " ['Tom lied.', 'Tom mentì.'],\n",
       " ['Tom lies.', 'Tom mente.'],\n",
       " ['Tom lost.', 'Tom ha perso.'],\n",
       " ['Tom lost.', 'Tom perse.'],\n",
       " ['Tom paid.', 'Tom ha pagato.'],\n",
       " ['Tom paid.', 'Tom pagò.'],\n",
       " ['Tom quit.', 'Tom ha rinunciato.'],\n",
       " ['Tom swam.', 'Tom ha nuotato.'],\n",
       " ['Tom swam.', 'Tom nuotò.'],\n",
       " ['Tom wept.', 'Tom piangeva.'],\n",
       " ['Tom wept.', 'Tom pianse.'],\n",
       " ['Tom wept.', 'Tom ha pianto.'],\n",
       " [\"Tom's up.\", 'Tom è alzato.'],\n",
       " ['Too late.', 'Troppo tardi.'],\n",
       " ['Try hard.', 'Prova duramente.'],\n",
       " ['Try hard.', 'Provate duramente.'],\n",
       " ['Try hard.', 'Provi duramente.'],\n",
       " ['Try some.', \"Provane un po'.\"],\n",
       " ['Try some.', \"Provatene un po'.\"],\n",
       " ['Try some.', \"Ne provi un po'.\"],\n",
       " ['Try this.', 'Prova questo.'],\n",
       " ['Try this.', 'Provate questo.'],\n",
       " ['Try this.', 'Provi questo.'],\n",
       " ['Use this.', 'Usa questo.'],\n",
       " ['Use this.', 'Usa questa.'],\n",
       " ['Use this.', 'Utilizza questo.'],\n",
       " ['Use this.', 'Utilizza questa.'],\n",
       " ['Use this.', 'Usate questo.'],\n",
       " ['Use this.', 'Usate questa.'],\n",
       " ['Use this.', 'Utilizzi questo.'],\n",
       " ['Use this.', 'Utilizzi questa.'],\n",
       " ['Use this.', 'Utilizzate questo.'],\n",
       " ['Use this.', 'Utilizzate questa.'],\n",
       " ['Use this.', 'Usi questo.'],\n",
       " ['Use this.', 'Usi questa.'],\n",
       " ['Warn Tom.', 'Avvisa Tom.'],\n",
       " ['Warn Tom.', 'Avvisate Tom.'],\n",
       " ['Warn Tom.', 'Avvisi Tom.'],\n",
       " ['Warn Tom.', 'Avverti Tom.'],\n",
       " ['Warn Tom.', 'Avverta Tom.'],\n",
       " ['Warn Tom.', 'Avvertite Tom.'],\n",
       " ['Watch me.', 'Guardami.'],\n",
       " ['Watch me.', 'Guardatemi.'],\n",
       " ['Watch me.', 'Mi guardi.'],\n",
       " ['Watch us.', 'Guardaci.'],\n",
       " ['Watch us.', 'Ci guardi.'],\n",
       " ['Watch us.', 'Guardateci.'],\n",
       " ['We agree.', \"Siamo d'accordo.\"],\n",
       " ['We agree.', \"Noi siamo d'accordo.\"],\n",
       " ['We tried.', 'Provavamo.'],\n",
       " ['We tried.', 'Noi provavamo.'],\n",
       " [\"We'll go.\", 'Andremo.'],\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_text(\"ita.txt\")\n",
    "ItalianNEng = to_lines(data)\n",
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fojrJ9MVKU4i"
   },
   "source": [
    "The ItalianEng has the data which we just imported and then ran through a funtion is now converted to the array by using the Python's inbuilt function \"array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCZ9O9r7KU4j"
   },
   "outputs": [],
   "source": [
    "data = read_text(\"ita.txt\")\n",
    "ItalianNEng = to_lines(data)\n",
    "ItalianNEng = array(ItalianNEng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw4Ad-e2KU4m"
   },
   "source": [
    "## PreProcessing the data/ Cleaning the data\n",
    "\n",
    "**remove all the punctuation and then change the case of every word to lower case**\n",
    "\n",
    "\n",
    "we will remove the punctuation in the below code by going through each line of the data and storing it in the same variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_018J7bKU4n"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "ItalianNEng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ItalianNEng[:,0]]\n",
    "ItalianNEng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ItalianNEng[:,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdssLYMlKU4q"
   },
   "source": [
    "Here all the punctuation is removed/cleaned and the data looks like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "t5oQeu4UKU4r",
    "outputId": "cc047626-5cdb-4734-da85-a741c41012f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi', 'Ciao'],\n",
       "       ['Run', 'Corri'],\n",
       "       ['Run', 'Corra'],\n",
       "       ...,\n",
       "       ['If you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo',\n",
       "        'Se vuoi sembrare un madrelingua devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato'],\n",
       "       ['If someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker In other words you dont really sound like a native speaker',\n",
       "        'Se qualcuno che non conosce il tuo background dice che sembri un madrelingua significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua In altre parole non sembri davvero un madrelingua'],\n",
       "       ['It may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort However if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
       "        'Può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo Ciononostante se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando potremmo essere in grado di minimizzare gli errori']],\n",
       "      dtype='<U317')"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYpCTTSeKU4v"
   },
   "source": [
    "Now we will convert the data into lower case by using python's inbuilt function lower() and save the data to its original variable ItalianEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKXmS2z8KU4w"
   },
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(ItalianNEng)):\n",
    "    ItalianNEng[i,0] = ItalianNEng[i,0].lower()\n",
    "    \n",
    "    ItalianNEng[i,1] = ItalianNEng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "vkd65bK7KU43",
    "outputId": "f48a7bcf-ddd6-4d5b-9bca-47c014131e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'ciao'],\n",
       "       ['run', 'corri'],\n",
       "       ['run', 'corra'],\n",
       "       ...,\n",
       "       ['if you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo',\n",
       "        'se vuoi sembrare un madrelingua devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato'],\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'se qualcuno che non conosce il tuo background dice che sembri un madrelingua significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua in altre parole non sembri davvero un madrelingua'],\n",
       "       ['it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
       "        'può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo ciononostante se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando potremmo essere in grado di minimizzare gli errori']],\n",
       "      dtype='<U317')"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q53n94cMKU45",
    "outputId": "18a5f164-58fd-41fb-966e-468a6e7a145b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItalianNEng[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfFbZ9ADKU48"
   },
   "source": [
    "we convert the data into pandas dataframe and save it in the variable ita_eng_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7TXpS6Q8KU49",
    "outputId": "5e05e68b-8db8-4902-9128-2cec639e97be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Italian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi</td>\n",
       "      <td>ciao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>corri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run</td>\n",
       "      <td>corra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>correte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who</td>\n",
       "      <td>chi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English  Italian\n",
       "0      hi     ciao\n",
       "1     run    corri\n",
       "2     run    corra\n",
       "3     run  correte\n",
       "4     who      chi"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita_eng_df = pd.DataFrame(ItalianNEng.tolist(), columns = ['English', 'Italian'])\n",
    "ita_eng_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZzQ_oYLKU5A"
   },
   "source": [
    "let us split the data into English and Italian part seperately and save it in their respective variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "l4Tq8lqbKU5B",
    "outputId": "df27643c-1e51-4bf0-a31c-d0dfaf093d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English'], dtype='object')\n",
      "Index(['Italian'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "english_sentences = ita_eng_df.iloc[:,0:1]\n",
    "italian_sentences = ita_eng_df.iloc[:,1:2]\n",
    "print(english_sentences.columns)\n",
    "print(italian_sentences.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SynzEYZKU5E"
   },
   "source": [
    "As the dataset is very large we will sample the model with only 2000 rows, later we can play around the data size and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LplOj2QjKU5F"
   },
   "outputs": [],
   "source": [
    "n = 2000\n",
    "italian_sample = italian_sentences.iloc[:n,:]\n",
    "english_sample = english_sentences.iloc[:n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tiVE9tDKU5I"
   },
   "source": [
    "we will now convert any row which is not of string type, if there is any row with different datatype we will convert it into string so that everything is of same datatype\n",
    "\n",
    "\n",
    "we will do this for both English and Italian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gl_ENDuTKU5J"
   },
   "outputs": [],
   "source": [
    "english_sample.columns\n",
    "english_sample['English']\n",
    "for i in range(len(english_sample['English'].index)):\n",
    "    if type(english_sample['English'][i]) != str:\n",
    "        english_sample['English'][i] = str(english_sample['English'][i])\n",
    "        \n",
    "italian_sample['Italian']\n",
    "for i in range(len(italian_sample['Italian'].index)):\n",
    "    if type(italian_sample['Italian'][i]) != str:\n",
    "        italian_sample['Italian'][i] = str(italian_sample['Italian'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xzk6NFVGKU5K"
   },
   "source": [
    "create a fucntion to add start and end token of each row as a identifier to the model and convert the data into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pK5HyIIbKU5L"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "def cleaning_sentence(data, column):\n",
    "    sentence = [preprocess_sentence(i) for i in data[column]]\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P74XHSbbKU5N"
   },
   "source": [
    "We will continue to create a class which has function which will create word to index and then index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wS_zVM8YKU5O"
   },
   "outputs": [],
   "source": [
    "class LanguageIndex():    \n",
    "    def __init__(self, lang):        \n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "          self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "          self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "          self.idx2word[index] = word   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbnK7PNnKU5Q"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-gBt3ZuKU5T"
   },
   "source": [
    "**Cleaning the data**\n",
    "\n",
    "we will make use of the funciton such as cleaning_sentence, LanguageIndex, and word2idx which we have written to clean, process, Index and padd the data\n",
    "\n",
    "\n",
    "we use Keras's inbuil function which performs sequence padding to the each sentence to the maximum data length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqmQNyi7KU5T"
   },
   "outputs": [],
   "source": [
    "#Cleaning Sentences\n",
    "italian_sent = cleaning_sentence(italian_sample, 'Italian')\n",
    "english_sent = cleaning_sentence(english_sample, 'English')\n",
    "\n",
    "# index language using the class defined above\n",
    "inp_lang = LanguageIndex(it for it in italian_sent)\n",
    "targ_lang = LanguageIndex(en for en in english_sent)\n",
    "\n",
    "#Italian Sentences which will be indexed\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in it.split(' ')] for it in italian_sent]\n",
    "#English sentences which will be indexed\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en in english_sent]\n",
    "\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "# Padding the input and output tensor to the maximum length\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')\n",
    "\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                              maxlen=max_length_tar, \n",
    "                                                              padding='post')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cbch8w3KU5W"
   },
   "source": [
    "**split the data into training data and test data by using Sklearn.Model_selection's train_test_split function**\n",
    "\n",
    "\n",
    "Split the whole data which we saved in the variable ItalianNEng as 80% to train and 20% to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye3xgSgzKU5W"
   },
   "outputs": [],
   "source": [
    "ax_train, ay_train, ax_test, ay_test = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKVnvNWYKU5Y"
   },
   "source": [
    "after we split the training data and the testing data we will check the length and the data in it, if we see the below data its vectorized which we did using the word2idx and Keras's inbuilt function sequence_padding and is in the array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "9VUII8zPKU5Z",
    "outputId": "26935667-d9c9-4b54-a4f7-e8a6b48824a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,  844, 1002, ...,    0,    0,    0],\n",
       "       [   5,  285,    4, ...,    0,    0,    0],\n",
       "       [   5,  932,  168, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   5,  932,  194, ...,    0,    0,    0],\n",
       "       [   5, 1061,  476, ...,    4,    0,    0],\n",
       "       [   5,   75, 1132, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7KYxn5OrKU5b",
    "outputId": "9e7fc1b1-aa47-4f33-def5-6255adfe019f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 400, 1600, 400)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show length\n",
    "len(ax_train), len(ay_train), len(ax_test), len(ay_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uAzQ5wPKU5e"
   },
   "source": [
    "Create the parameters where we can modify and pass later to tune the model and we will create the tensorflow dataset using the tf's function tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "MQVmNcQDKU5e",
    "outputId": "393e5c0c-15c7-4bd7-8eb5-308e691d9d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "64\n",
      "25\n",
      "1159\n",
      "438\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(ax_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "\n",
    "print(BUFFER_SIZE)\n",
    "print(BATCH_SIZE)\n",
    "print(N_BATCH)\n",
    "print(vocab_inp_size)\n",
    "print(vocab_tar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SpEMRrYYKU5h",
    "outputId": "65e6be27-2749-44f5-e9cc-150f612396e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 7), (64, 5)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((ax_train, ax_test)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FMXLifsKU5k"
   },
   "source": [
    "We will build the encoder and decoder model by implementing \"attention equation\"\n",
    "\n",
    "attention overcomes the limitation in the encode-decoder architecture by allowing the network to learn where to pay attention to the input for each item in the output sequence.\n",
    "\n",
    "this approach has been used across different types sequence prediction problems include text translation, speech recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lEXhULxKU5l"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                           return_sequences=True, \n",
    "                           return_state=True, \n",
    "                           recurrent_activation='sigmoid', \n",
    "                           recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkpXCBuOKU5n"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmM86Kp1KU5p"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU2MUnA1KU5q"
   },
   "outputs": [],
   "source": [
    "def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drtfo7UYKU5s"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6thdVy0kKU5t"
   },
   "source": [
    "**Optimizer and Loss Function**\n",
    "\n",
    "We are using \"Adam\" as the optimizer for the model and sparse_softmax_cross_entropy as the loss function.\n",
    "\n",
    "we can play around with these to see which is better suitable for our model, but to train the model we will be going with the above mentoned optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrkGCX1tKU5u"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lebrg1MoKU5v"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "we will train the model by passing the training data which we split and passed it through the funtion. we will be saving out model on each iteration of the epochs \n",
    "\n",
    "We will be running this model with **30 Epochs**\n",
    "\n",
    "we will play around with the number of epochs to see what best works for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2128
    },
    "colab_type": "code",
    "id": "wknvJ6F2KU5w",
    "outputId": "99ba580c-2ffb-452d-ae33-9652d20559b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1 Batch 0 Loss 3.6870\n",
      "Epoch 1 Loss 2.8127\n",
      "Time taken for 1 epoch 3.9979658126831055 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.2180\n",
      "Epoch 2 Loss 2.1255\n",
      "Time taken for 1 epoch 3.422673463821411 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.9123\n",
      "Epoch 3 Loss 1.8474\n",
      "Time taken for 1 epoch 3.5521862506866455 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5828\n",
      "Epoch 4 Loss 1.6300\n",
      "Time taken for 1 epoch 3.3715474605560303 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.5532\n",
      "Epoch 5 Loss 1.4790\n",
      "Time taken for 1 epoch 3.3691043853759766 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.3462\n",
      "Epoch 6 Loss 1.3482\n",
      "Time taken for 1 epoch 3.3520689010620117 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.3363\n",
      "Epoch 7 Loss 1.2417\n",
      "Time taken for 1 epoch 3.37825870513916 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.0950\n",
      "Epoch 8 Loss 1.1400\n",
      "Time taken for 1 epoch 3.5299947261810303 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0548\n",
      "Epoch 9 Loss 1.0624\n",
      "Time taken for 1 epoch 3.3363037109375 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.0538\n",
      "Epoch 10 Loss 0.9912\n",
      "Time taken for 1 epoch 3.3699395656585693 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.9385\n",
      "Epoch 11 Loss 0.9083\n",
      "Time taken for 1 epoch 3.3596200942993164 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.8627\n",
      "Epoch 12 Loss 0.8623\n",
      "Time taken for 1 epoch 3.3558349609375 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.7966\n",
      "Epoch 13 Loss 0.8052\n",
      "Time taken for 1 epoch 3.3981566429138184 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.7173\n",
      "Epoch 14 Loss 0.7290\n",
      "Time taken for 1 epoch 3.504643440246582 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.6200\n",
      "Epoch 15 Loss 0.6785\n",
      "Time taken for 1 epoch 3.3223588466644287 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.5711\n",
      "Epoch 16 Loss 0.6286\n",
      "Time taken for 1 epoch 3.3759407997131348 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5154\n",
      "Epoch 17 Loss 0.6001\n",
      "Time taken for 1 epoch 3.3162126541137695 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.5034\n",
      "Epoch 18 Loss 0.5328\n",
      "Time taken for 1 epoch 3.3221631050109863 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.4151\n",
      "Epoch 19 Loss 0.4823\n",
      "Time taken for 1 epoch 3.8211886882781982 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.4448\n",
      "Epoch 20 Loss 0.4420\n",
      "Time taken for 1 epoch 3.728731155395508 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.3618\n",
      "Epoch 21 Loss 0.4075\n",
      "Time taken for 1 epoch 3.6832656860351562 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.3418\n",
      "Epoch 22 Loss 0.3701\n",
      "Time taken for 1 epoch 3.3606433868408203 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.2997\n",
      "Epoch 23 Loss 0.3318\n",
      "Time taken for 1 epoch 3.3265371322631836 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.2770\n",
      "Epoch 24 Loss 0.3026\n",
      "Time taken for 1 epoch 3.530639171600342 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.2901\n",
      "Epoch 25 Loss 0.2764\n",
      "Time taken for 1 epoch 3.3358941078186035 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.1945\n",
      "Epoch 26 Loss 0.2528\n",
      "Time taken for 1 epoch 3.3008012771606445 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.1729\n",
      "Epoch 27 Loss 0.2348\n",
      "Time taken for 1 epoch 3.302164077758789 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.1369\n",
      "Epoch 28 Loss 0.2259\n",
      "Time taken for 1 epoch 3.336372137069702 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.1524\n",
      "Epoch 29 Loss 0.1983\n",
      "Time taken for 1 epoch 3.5109617710113525 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.2086\n",
      "Epoch 30 Loss 0.1649\n",
      "Time taken for 1 epoch 3.3250839710235596 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))   \n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igSWtzupKU5x"
   },
   "source": [
    "## Prediction\n",
    "Once the model is trained we can translate the Italian sentences by passing it to the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqDEAweHKU5y"
   },
   "outputs": [],
   "source": [
    "def Prediction_eval(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7DwyJJkKU51"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = Prediction_eval(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "udESsGOpKU53",
    "outputId": "36ddaaaa-edb0-4b8e-fd82-6712896cc6aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sample.iloc[:1,:].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "X-MPUkeVKU55",
    "outputId": "1ee059e7-0b18-44c6-8072-8b9c961a1f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ciao <end>\n",
      "Predicted translation: hello <end> \n"
     ]
    }
   ],
   "source": [
    "translate(italian_sample.iloc[:1,:].values[0][0], encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "j1RGsyUIKU58",
    "outputId": "82bc1523-cefe-4188-a146-2aace581cca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> corri <end>\n",
      "Predicted translation: run <end> \n"
     ]
    }
   ],
   "source": [
    "test = 'corri'\n",
    "translate(sentence=test, encoder=encoder, decoder=decoder, inp_lang= inp_lang, targ_lang= targ_lang, max_length_inp= max_length_inp, max_length_targ= max_length_tar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oJ1K5aYKU6H"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "**o** https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/\n",
    "\n",
    "**o** https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa\n",
    "\n",
    "**o** https://www.analyticsvidhya.com/blog/2018/03/microsofts-claims-language-translation-ai-reached-human-levels-accuracy/\n",
    "\n",
    "**o** https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=nRBnh4qbPHI&vl=en\n",
    "\n",
    "**o** http://www.manythings.org/anki/\n",
    "\n",
    "**o** https://machinelearningmastery.com/introduction-neural-machine-translation/\n",
    "\n",
    "**o** https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "\n",
    "**o** https://www.youtube.com/watch?v=vI2Y3I-JI2Q\n",
    "\n",
    "**o** https://towardsdatascience.com/neural-machine-translator-with-less-than-50-lines-of-code-guide-1fe4fdfe6292\n",
    "\n",
    "**o** https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/\n",
    "\n",
    "**o** https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnVG9BT0KU6I"
   },
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buv-bbhUKU6I"
   },
   "source": [
    "**MIT License**\n",
    "\n",
    "Copyright 2018 Chetan M Jadhav\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUbVc5x8KU6J"
   },
   "source": [
    "**The text in the document by Chetan M Jadhav is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/ **"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "GRU with Attention_2,000_records_30_Epochs.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
